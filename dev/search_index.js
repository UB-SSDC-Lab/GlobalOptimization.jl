var documenterSearchIndex = {"docs":
[{"location":"dev/contributing/#Developing-Documentation","page":"Contributing","title":"Developing Documentation","text":"","category":"section"},{"location":"dev/contributing/","page":"Contributing","title":"Contributing","text":"When developing documentation locally, it is suggested to use servedocs() provided by LiveServer.jl to tests the documentation  build process while viewing updates to them dynamically as they are made. This can be  done by running the following command in your terminal while at the base level of your local instance of GlobalOptimization.jl:","category":"page"},{"location":"dev/contributing/","page":"Contributing","title":"Contributing","text":"julia --project=docs -ie 'using GlobalOptimization, LiveServer; servedocs(include_dirs=[\"src/\"])'","category":"page"},{"location":"dev/contributing/","page":"Contributing","title":"Contributing","text":"If the documentation build is successful, this will print a link to a spawned local server that you can open in any browser.","category":"page"},{"location":"algs/pso/","page":"-","title":"-","text":"To be continued...","category":"page"},{"location":"lib/internal/temp/#Internals","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"lib/internal/temp/","page":"Internals","title":"Internals","text":"Listing all non-exported types and functions here for now, but split off categories onto separate pages in the future!","category":"page"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractAdaptationStrategy","page":"Internals","title":"GlobalOptimization.AbstractAdaptationStrategy","text":"AbstractAdaptationStrategy\n\nSubtypes of this type should be used in conjunction with subtypes of AbstractMutationParameters and AbstractCrossoverParameters to control how the parameters are adapted in their respective adapt! methods.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractAlgorithmSpecificOptions","page":"Internals","title":"GlobalOptimization.AbstractAlgorithmSpecificOptions","text":"AbstractAlgorithmSpecificOptions\n\nAbstract type for algorithm specific options\n\nAll subtypes must define the following fields:\n\ngeneral: The general options for an optimizer.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractBinomialCrossoverParameters","page":"Internals","title":"GlobalOptimization.AbstractBinomialCrossoverParameters","text":"AbstractBinomialCrossoverParameters\n\nAn abstract type representing the parameters for a binomial crossover strategy in Differential Evolution (DE). The crossover! method is provided for subtypes of this abstract type, however, the get_parameter, initialize!, and adapt! methods are must still be defined.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractCandidate","page":"Internals","title":"GlobalOptimization.AbstractCandidate","text":"AbstractCandidate\n\nAbstract type for a candidate\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractCrossoverParameters","page":"Internals","title":"GlobalOptimization.AbstractCrossoverParameters","text":"AbstractCrossoverParameters\n\nAn abstract type representing the parameters for a crossover strategy in Differential Evolution (DE).\n\nSubtypes of this abstract type should define the following methods:\n\nget_parameter(params::AbstractCrossoverParameters, i): Returns the crossover parameter for the i-th candidate.\ninitialize!(params::AbstractCrossoverParameters, num_dims, population_size): Initializes the crossover parameters.\nadapt!(params::AbstractCrossoverParameters, improved, global_best_improved): Adapts the crossover parameters based on the improvement status of the candidates.\ncrossover!(population::DEPopulation, crossover_params, search_space): Performs the crossover operation on the population using the specified crossover parameters.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractCrossoverTransformation","page":"Internals","title":"GlobalOptimization.AbstractCrossoverTransformation","text":"AbstractCrossoverTransformation\n\nAn abstract type representing a transformation applied to a candidate prior to applying the crossover operator.\n\nSubtypes of this abstract type should define the following methods:\n\ninitialize!(transformation::AbstractCrossoverTransformation, population_size): Initializes the transformation with the population\nupdate_transformation!(transformation::AbstractCrossoverTransformation, population): Updates the transformation based on the current population\nto_transformed(transformation::AbstractCrossoverTransformation, c, m): Transforms the candidate c and mutant m to an   alternative representation, returning the transformed candidate, transformed mutant, and a boolean indicating whether the transformation was applied.\nfrom_transformed!(transformation::AbstractCrossoverTransformation, mt, m): Transforms the mutant mt back to the original representation m.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractEvaluator","page":"Internals","title":"GlobalOptimization.AbstractEvaluator","text":"AbstractEvaluator\n\nAbstract type for an evaluator. An evaluator is responsible for evaluating the fitness of a population or candidate.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractFunctionEvaluationMethod","page":"Internals","title":"GlobalOptimization.AbstractFunctionEvaluationMethod","text":"AbstractFunctionEvaluationMethod\n\nA function evaluation method is a strategy for evaluating the fitness/objective, as well as     possibly other algorithm specific things.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractMBHDistribution","page":"Internals","title":"GlobalOptimization.AbstractMBHDistribution","text":"AbstractMBHDistribution{T}\n\nAbstract type for MBH distributions.\n\nAll subtypes of AbstractMBHDistribution must implement the following methods:\n\ninitialize!(dist::AbstractMBHDistribution, ss::ContinuousRectangularSearchSpace):   Initializes the distribution with the search space.\ndraw_step!(step::AbstractVector{T}, dist::AbstractMBHDistribution{T}): Draws a step from the distribution   and stores it in step.\nget_show_trace_elements(dist::AbstractMBHDistribution{T}, trace_mode::Union{Val{:detailed}, Val{:all}}):   Returns the trace elements for the distribution to be shown in the terminal terminal terminal terminal terminal terminal terminal terminal terminal.\nget_save_trace_elements(dist::AbstractMBHDistribution{T}, trace_mode::Union{Val{:detailed}, Val{:all}}):   Returns the trace elements for the distribution to be saved to a file.\n\nNote: The AbstractMBHDistribution interface is tightly coupled with the AbstractHopperSet interface and could use some refactoring to decouple the two. If an additional adaptive distribution is added, a new abstract type, such as AbstractAdaptiveMBHDistribution, should be added, and MBHAdaptiveDistribution should be re-named to something more specific.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractMutationOperator","page":"Internals","title":"GlobalOptimization.AbstractMutationOperator","text":"AbstractMutationOperator\n\nThe abstract type for all DE mutation strategies. This type is used to define the mutation strategies available in the DE algorithm.\n\nSubtypes of this type should implement the following method:\n\nget_parameters(strategy::AbstractMutationOperator, dist): Returns the mutation   parameters for the given strategy and distribution. The parameters should be returned   as a SVector{4, Float64} containing the F₁, F₂, F₃, and F₄ weights for the unified   mutation strategy.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractMutationParameters","page":"Internals","title":"GlobalOptimization.AbstractMutationParameters","text":"AbstractMutationParameters\n\nThe abstract type for all DE mutation parameters. This type is used to define the mutation parameters available in the DE algorithm.\n\nSubtypes of this type should implement the following methods:\n\nget_parameters(params::AbstractMutationParameters, i): Returns the mutation   parameters for the given mutation parameters object and index i. The parameters should   be returned as a tuple of four Float64 values representing the F₁, F₂, F₃, and F₄   weights for the unified mutation strategy.\ninitialize!(params::AbstractMutationParameters, population_size): Initializes the   mutation parameters for the given population size.\nadapt!(params::AbstractMutationParameters, improved, global_best_improved): Adapts   the mutation parameters based on whether the population has improved and whether the   global best candidate has improved.\nmutate!(population::DEPopulation, F::AbstractMutationParameters): Mutates the   population using the mutation parameters.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractNonlinearEquationProblem","page":"Internals","title":"GlobalOptimization.AbstractNonlinearEquationProblem","text":"AbstractNonlinearEquationProblem\n\nAbstract type for problems involving a set of nonlinear equations to solve.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractOptimizationProblem","page":"Internals","title":"GlobalOptimization.AbstractOptimizationProblem","text":"AbstractOptimizationProblem\n\nAbstract type for optimization problems.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractOptimizer","page":"Internals","title":"GlobalOptimization.AbstractOptimizer","text":"AbstractOptimizer\n\nAbstract type of all optimization algorithms.\n\nAll subtypes must have the following fields:\n\noptions<:AbstractAlgorithmSpecificOptions: The options for the optimizer. See the   AbstractAlgorithmSpecificOptions interface documentation for details.\ncache<:AbstractOptimizerCache: The cache for the optimizer. See the   AbstractOptimizerCache interface documentation for details.\n\nAll subtypes must define the following methods:\n\ninitialize!(opt<:AbstractOptimizer): Initialize the optimizer.\nstep!(opt<:AbstractOptimizer): Perform a single step/iteration with the optimizer.\nget_best_fitness(opt<:AbstractOptimizer): Get the best fitness of the optimizer.\nget_best_candidate(opt<:AbstractOptimizer): Get the best candidate of the optimizer.\nget_show_trace_elements(opt<:AbstractOptimizer, trace_mode::Union{Val{:detailed}, Val{:all}}):   Returns a Tuple of TraceElements and, if necessary, Vector{TraceElement}s, of data   to be printed to the terminal. Note that separate methods should be defined for   Val{:detailed} and Val{:all} if applicable.\nget_save_trace_elements(opt<:AbstractOptimizer, trace_mode::Union{Val{:detailed}, Val{:all}}):   Returns a Tuple of TraceElements and, if necessary, Vector{TraceElement}s, of data   to be saved to the trace file. Note that separate methods should be defined for   Val{:detailed} and Val{:all} if applicable.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractOptimizerCache","page":"Internals","title":"GlobalOptimization.AbstractOptimizerCache","text":"AbstractOptimizerCache\n\nAbstract type of all optimization algorithm caches.\n\nAll subtypes must have the following fields:\n\niteration: The current iteration number of the optimizer.\nstart_time: The start time of the optimization.\nstall_start_time: THe start time of the stall.\nstall_iteration: The iteration number of the stall.\nstall_value: The objective function value at start of stall.\n\nThese fields must be initialized in the initialize!(opt) method of the optimizer.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractOptions","page":"Internals","title":"GlobalOptimization.AbstractOptions","text":"AbstractOptions\n\nAbstract type for multiple options\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractPopulation","page":"Internals","title":"GlobalOptimization.AbstractPopulation","text":"AbstractPopulation\n\nAbstract type for a population of candidates.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractPopulationBasedOptimizer","page":"Internals","title":"GlobalOptimization.AbstractPopulationBasedOptimizer","text":"AbstractPopulationBasedOptimizer\n\nAbstract type of all population based optimization algorithms.\n\nAll subtypes must have the following fields:\n\noptions<:AbstractAlgorithmSpecificOptions: The options for the optimizer. See the   AbstractAlgorithmSpecificOptions interface documentation for details.\ncache<:AbstractPopulationBasedOptimizerCache: The cache for the optimizer. See the   AbstractPopulationBasedOptimizerCache interface documentation for details.\n\nAll subtypes must define the following methods:\n\ninitialize!(<:AbstractPopulationBasedOptimizer): Initialize the optimizer.\nstep!(<:AbstractPopulationBasedOptimizer): Perform a single step/iteration with the   optimizer.\nget_population(opt<:AbstractPopulationBasedOptimizer): Get the population of the   optimizer. This should return a subtype of AbstractPopulation.\nget_show_trace_elements(opt<:AbstractOptimizer, trace_mode::Union{Val{:detailed}, Val{:all}}):   Returns a Tuple of TraceElements and, if necessary, Vector{TraceElement}s, of data   to be printed to the terminal. Note that separate methods should be defined for   Val{:detailed} and Val{:all} if applicable.\nget_save_trace_elements(opt<:AbstractOptimizer, trace_mode::Union{Val{:detailed}, Val{:all}}):   Returns a Tuple of TraceElements and, if necessary, Vector{TraceElement}s, of data   to be saved to the trace file. Note that separate methods should be defined for   Val{:detailed} and Val{:all} if applicable.\n\nNote that the get_best_fitness and get_best_candidate methods required by the AbstractOptimizer interface are provided for subtypes of AbstractPopulationBasedOptimizer.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractPopulationBasedOptimizerCache","page":"Internals","title":"GlobalOptimization.AbstractPopulationBasedOptimizerCache","text":"AbstractPopulationBasedOptimizerCache\n\nAbstract type of all population based optimization algorithm caches.\n\nAll subtypes must have the following fields:\n\nAll fields of AbstractOptimizerCache.\nglobal_best_candidate: The global best candidate of the population.\nglobal_best_fitness: The global best fitness of the population.\n\nThese fields must be initialized in the initialize!(opt) method of the optimizer. Additionally, the global_best_candidate and global_best_fitness fields must be updated in each iteration when appropriate (i.e., when the global best candidate improves) in iterate!(opt).\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractProblem","page":"Internals","title":"GlobalOptimization.AbstractProblem","text":"AbstractProblem\n\nAbstract type for all solvable problems.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractRandomNeighborhoodVelocityUpdateScheme","page":"Internals","title":"GlobalOptimization.AbstractRandomNeighborhoodVelocityUpdateScheme","text":"AbstractRandomNeighborhoodVelocityUpdateScheme\n\nAbstract type for a velocity update scheme that randomly selects the particles in the neighborhood of a given particle.\n\nSubtypes of this abstract type have an implementation of the update_velocity! method provided, but must still define the initialize! and adapt! methods.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractSelector","page":"Internals","title":"GlobalOptimization.AbstractSelector","text":"AbstractSelector\n\nSubtypes of this type should be used to select candidates from the population and must implement the following methods:\n\ninitialize!(s::AbstractSelector, population_size::Int): Initializes the selector with the population size.\nselect(s::AbstractSelector, target::Int, pop_size::Int): Selects candidates based on the target index and population size.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractVelocityUpdateScheme","page":"Internals","title":"GlobalOptimization.AbstractVelocityUpdateScheme","text":"AbstractVelocityUpdateScheme\n\nAbstract type for velocity update schemes in Particle Swarm Optimization (PSO).\n\nAll subtypes much define the following methods:\n\ninitialize!(vu::AbstractVelocityUpdateScheme, swarm_size::Int): Initialize the velocity   update scheme for a given swarm size.\nupdate_velocity!(swarm::Swarm, vu::AbstractVelocityUpdateScheme): Update the velocity of   each candidate in the swarm using the specified velocity update scheme.\nadapt!(vu::AbstractVelocityUpdateScheme, improved::Bool, stall_iteration::Int): Adapt   the velocity update scheme based on the improvement status of the swarm and the stall iteration counter.\nget_show_trace_elements(vu::AbstractVelocityUpdateScheme, trace_mode::Union{Val{:detailed},Val{:all}}):   Get the elements to show in the trace for the velocity update scheme.\nget_save_trace_elements(vu::AbstractVelocityUpdateScheme, trace_mode::Union{Val{:detailed},Val{:all}}):   Get the elements to save in the trace for the velocity update scheme.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AsyncEvaluator","page":"Internals","title":"GlobalOptimization.AsyncEvaluator","text":"AsyncEvaluator\n\nAbstract type for an evaluator that evaluates the fitness of a single candidate asyncronously.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.BatchEvaluator","page":"Internals","title":"GlobalOptimization.BatchEvaluator","text":"BatchEvaluator\n\nAbstract type for an evaluator that evaluates the fitness of an entire population.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.BatchJobEvaluator","page":"Internals","title":"GlobalOptimization.BatchJobEvaluator","text":"BatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEBasePopulation","page":"Internals","title":"GlobalOptimization.DEBasePopulation","text":"DEBasePopulation{T <: AbstractFloat} <: AbstractPopulation{T}\n\nThe base representation of some population for the DE algorithms.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEOptions","page":"Internals","title":"GlobalOptimization.DEOptions","text":"DEOptions\n\nOptions for the Differential Evolution (DE) algorithms.\n\nFields:\n\ngeneral<:GeneralOptions: The general options.\npop_init_method<:AbstractPopulationInitialization: The population initialization method.\nmutation_params<:AbstractMutationParameters: The mutation strategy parameters.\ncrossover_params<:AbstractCrossoverParameters: The crossover strategy parameters.\ninitial_space<:Union{Nothing,ContinuousRectangularSearchSpace}: The initial space to initialize the population.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEPopulation","page":"Internals","title":"GlobalOptimization.DEPopulation","text":"DEPopulation{T <: AbstractFloat} <: AbstractPopulation{T}\n\nThe full population representation for the DE algorithms, including both the candidates and the mutants.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEPopulation-Tuple{Integer, Integer}","page":"Internals","title":"GlobalOptimization.DEPopulation","text":"DEPopulation(num_candidates::Integer, num_dims::Integer)\n\nConstructs a DEPopulation with num_candidates candidates in num_dims dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.FeasibilityHandlingEvaluator","page":"Internals","title":"GlobalOptimization.FeasibilityHandlingEvaluator","text":"FeasibilityHandlingEvaluator\n\nAn evaluator that handled a functions returned infeasibility penalty\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.FixedDimensionSearchSpace","page":"Internals","title":"GlobalOptimization.FixedDimensionSearchSpace","text":"FixedDimensionSearchSpace\n\nThe base abstract type for a search space with a fixed finite number of dimensions. Applicable to the vast majority of optimization problems.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.GeneralOptions","page":"Internals","title":"GlobalOptimization.GeneralOptions","text":"GeneralOptions\n\nGeneral options for all optimizers\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.GlobalOptimizationTrace","page":"Internals","title":"GlobalOptimization.GlobalOptimizationTrace","text":"GlobalOptimizationTrace{SHT, SAT, TM}\n\nA structure to hold the global optimization trace settings.\n\nFields:\n\nshow_trace::SHT: A flag indicating whether to show the trace in the console.\nsave_trace::SAT: A flag indicating whether to save the trace to a file.\nsave_file::String: The file path where the trace will be saved.\ntrace_level::TraceLevel{TM}: The trace level settings, which can be TraceMinimal, TraceDetailed, or TraceAll.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.LinearOperatorCrossoverTransformation","page":"Internals","title":"GlobalOptimization.LinearOperatorCrossoverTransformation","text":"LinearOperatorCrossoverTransformation\n\nAn abstract type representing a transformation applied to a candidate prior to applying the crossover operator which uses a linear operator.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.MBHOptions","page":"Internals","title":"GlobalOptimization.MBHOptions","text":"MBHOptions <: AbstractAlgorithmSpecificOptions\n\nOptions for the Monotonic Basin Hopping (MBH) algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.MBHStepMemory","page":"Internals","title":"GlobalOptimization.MBHStepMemory","text":"MBHStepMemory{T}\n\nMemory about MBH accepted steps.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.MinimalOptimizerCache","page":"Internals","title":"GlobalOptimization.MinimalOptimizerCache","text":"MinimalOptimizerCache{T}\n\nA minimal implementation of the AbstractOptimizerCache type.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.MinimalPopulationBasedOptimizerCache","page":"Internals","title":"GlobalOptimization.MinimalPopulationBasedOptimizerCache","text":"MinimalPopulationBasedOptimizerCache{T}\n\nA minimal implementation of the AbstractPopulationBasedOptimizerCache type.\n\nAlgorithms employing this cache must initialize all fields in initialize!(opt) and must update the iteration field each iteration in iterate!(opt). Additionally, the global_best_candidate and global_best_fitness fields must be updated in each iteration when appropriate (i.e., when the global best candidate improves) in iterate!(opt).\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.NoAdaptation","page":"Internals","title":"GlobalOptimization.NoAdaptation","text":"NoAdaptation\n\nParameters are not adapted at all.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.NoTransformation","page":"Internals","title":"GlobalOptimization.NoTransformation","text":"NoTransformation\n\nA transformation that does not apply any transformation to the candidate or mutant.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.PSOOptions","page":"Internals","title":"GlobalOptimization.PSOOptions","text":"PSOOptions <: AbstractAlgorithmSpecificOptions\n\nOptions for the PSO algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.PolyesterBatchEvaluator","page":"Internals","title":"GlobalOptimization.PolyesterBatchEvaluator","text":"PolyesterBatchEvaluator\n\nAn evaluator that evaluates the fitness of a population in parallel using multi-threading using Polyester.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.PolyesterBatchJobEvaluator","page":"Internals","title":"GlobalOptimization.PolyesterBatchJobEvaluator","text":"PolyesterBatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs in parallel using Polyester.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.RandomAdaptation","page":"Internals","title":"GlobalOptimization.RandomAdaptation","text":"RandomAdaptation\n\nParameters are randomly adapted.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.RectangularSearchSpace","page":"Internals","title":"GlobalOptimization.RectangularSearchSpace","text":"RectangularSearchSpace\n\nA FixedDimensionSearchSpace with N dimensional rectangle as the set of feasible points.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.Results","page":"Internals","title":"GlobalOptimization.Results","text":"Results{T}\n\nA simple struct for returning results.\n\nFields\n\nfbest::T: The best function value found.\nxbest::Vector{T}: The best candidate found.\niters::Int: The number of iterations performed.\ntime::Float64: The time taken to perform the optimization in seconds.\nexitFlag::Int: The exit flag of the optimization.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.Results-Union{Tuple{T}, Tuple{T, AbstractVector{T}, Any, Any, GlobalOptimization.Status}} where T","page":"Internals","title":"GlobalOptimization.Results","text":"Results(fbest::T, xbest::AbstractVector{T}, iters, time, exitFlag)\n\nConstructs a new Results struct.\n\nArguments\n\nfbest::T: The best function value found.\nxbest::AbstractVector{T}: The best candidate found.\niters::Int: The number of iterations performed.\ntime::AbstractFloat: The time taken to perform the optimization in seconds.\nexitFlag::Int: The exit flag of the optimization.\n\nReturns\n\nResults{T}\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.SearchSpace","page":"Internals","title":"GlobalOptimization.SearchSpace","text":"SearchSpace\n\nThe base abstract type for a Problem search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.SerialBatchEvaluator","page":"Internals","title":"GlobalOptimization.SerialBatchEvaluator","text":"SerialBatchEvaluator\n\nAn evaluator that evaluates the fitness of a population in serial.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.SerialBatchJobEvaluator","page":"Internals","title":"GlobalOptimization.SerialBatchJobEvaluator","text":"SerialBatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs in serial.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.SingleEvaluator","page":"Internals","title":"GlobalOptimization.SingleEvaluator","text":"SingleEvaluator\n\nAbstract type for an evaluator that evaluates the fitness of a single candidate\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.Swarm","page":"Internals","title":"GlobalOptimization.Swarm","text":"Swarm{T <: AbstractFloat} <: AbstractPopulation\n\nA population of particles for the PSO algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.ThreadedBatchEvaluator","page":"Internals","title":"GlobalOptimization.ThreadedBatchEvaluator","text":"ThreadedBatchEvaluator\n\nAn evaluator that evaluates the fitness of a population in parallel using multi-threading.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.ThreadedBatchJobEvaluator","page":"Internals","title":"GlobalOptimization.ThreadedBatchJobEvaluator","text":"ThreadedBatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs in parallel using Threads.jl and ChunkSplitters.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.TraceElement","page":"Internals","title":"GlobalOptimization.TraceElement","text":"TraceElement{T}\n\nA structure to hold a single trace element, which includes a label, flag, length, precision, and value.\n\nAn AbstractOptimizer should return a tuple of TraceElement objects when the get_show_trace_elements or get_save_trace_elements functions are called.\n\nFields:\n\nlabel::String: The label for the trace element.\nflag::Char: A character indicating the format of the value (e.g., 'd' for integer, 'f' for float).\nlength::Int: The length of the formatted string representation of the value.\nprecision::Int: The precision for floating-point values.\nvalue::T: The value of the trace element, which can be of any type T.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.TraceLevel","page":"Internals","title":"GlobalOptimization.TraceLevel","text":"TraceLevel{TM}\n\nA structure to with information about the tracing of the global optimization process.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.UniformInitialization","page":"Internals","title":"GlobalOptimization.UniformInitialization","text":"UniformInitialization\n\nInitializes a population from a uniform distribution in the search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#Base.eachindex-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"Base.eachindex","text":"eachindex(pop::AbstractPopulation)\n\nReturns an iterator for the indices of the population.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#Base.length-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"Base.length","text":"length(pop::AbstractPopulation)\n\nReturns the number of candidates in the population.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#Base.size-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"Base.size","text":"size(pop::AbstractPopulation)\n\nReturns the size of the population.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.DEPopulation_F64-Tuple{Integer, Integer}","page":"Internals","title":"GlobalOptimization.DEPopulation_F64","text":"DEPopulation_F64(num_candidates::Integer, num_dims::Integer)\n\nConstructs a Float64 DEPopulation with num_candidate candidates in num_dims dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.adapt!-Tuple{MATLABVelocityUpdate, Bool, Int64}","page":"Internals","title":"GlobalOptimization.adapt!","text":"adapt!(vu::AbstractVelocityUpdateScheme, improved::Bool, stall_iteration::Int)\n\nAdapt the velocity update scheme based on the improvement status of the swarm and the stall iteration counter.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.all_correlated-Tuple{Any, Any}","page":"Internals","title":"GlobalOptimization.all_correlated","text":"all_correlated(cor, tol)\n\nChecks if the absolute value of the lower-triangular part of the correlation matrix cor are above the correlation tolerance tol and returns true if so, otherwise, returns false.\n\nArguments:\n\ncor::AbstractMatrix: The correlation matrix\ntol::AbstractFloat: The correlation tolerance. Elements of cor with an absolute value   greater than tol are assumed to be correlated.\n\nReturns:\n\nBool: true if all elements are correlated and false otherwise\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.candidate-Tuple{GlobalOptimization.AbstractCandidate}","page":"Internals","title":"GlobalOptimization.candidate","text":"candidate(c::AbstractCandidate)\n\nReturns the candidate c.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.candidates-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"GlobalOptimization.candidates","text":"candidates(pop::AbstractPopulation, [i::Integer])\n\nReturns the candidates from a population. If i is specified, returns the i-th candidate.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.check_fitness!-Tuple{GlobalOptimization.AbstractCandidate, Val{false}}","page":"Internals","title":"GlobalOptimization.check_fitness!","text":"check_fitness!(c::AbstractCandidate, ::Val)\n\nChecks the fitness if the option has been enabled.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.check_fitness!-Tuple{GlobalOptimization.AbstractHopperSet, Val{false}}","page":"Internals","title":"GlobalOptimization.check_fitness!","text":"check_fitness!(c::AbstractHopperSet, options::Union{GeneralOptions,Val{true},Val{false}})\n\nChecks the fitness of the candidate c to ensure that it is valid iff the option has been enabled.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.check_fitness!-Tuple{GlobalOptimization.AbstractPopulation, Val{false}}","page":"Internals","title":"GlobalOptimization.check_fitness!","text":"check_fitness!(pop::AbstractPopulation, options::Union{GeneralOptions,Val{true},Val{false}})\n\nChecks the fitness of each candidate in the population pop to ensure that it is valid iff options <: Union{GeneralOptions{D,Val{true}}, Val{true}}, otherwise, does nothing.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.check_stopping_criteria-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.check_stopping_criteria","text":"check_stopping_criteria(opt::AbstractOptimizer)\n\nCheck if opt satisfies any stopping criteria. Returns the appropriate Status enum.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.construct_batch_evaluator-Tuple{SerialFunctionEvaluation, Any}","page":"Internals","title":"GlobalOptimization.construct_batch_evaluator","text":"construct_batch_evaluator(\n    method::AbstractFunctionEvaluationMethod,\n    prob::OptimizationProblem,\n)\n\nConstructs a batch evaluator for the given method and prob.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.construct_batch_job_evaluator-Tuple{SerialFunctionEvaluation}","page":"Internals","title":"GlobalOptimization.construct_batch_job_evaluator","text":"construct_batch_job_evaluator(method::AbstractFunctionEvaluationMethod)\n\nConstructs a batch job evaluator for the given method.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.construct_results-Tuple{GlobalOptimization.AbstractOptimizer, GlobalOptimization.Status}","page":"Internals","title":"GlobalOptimization.construct_results","text":"construct_results(opt::AbstractOptimizer, status::Status)\n\nConstruct the results from the optimizer opt with the appropriate Status to indicate the stopping criteria.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.crossover!-Tuple{GlobalOptimization.DEPopulation, GlobalOptimization.AbstractBinomialCrossoverParameters, Any}","page":"Internals","title":"GlobalOptimization.crossover!","text":"crossover!(population::DEPopulation{T}, crossover_params, search_space)\n\nPerforms the crossover operation on the population population using the DE crossover strategy.\n\nThis function also ensures that, after crossover, the mutants are within the search space.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_delta-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_delta","text":"dim_delta(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the difference between the maximum and minimum values for the i-th dimension of ss. If i is not specified, returns a vector of all differences.\n\nArguments\n\nss::ContinuousRectangularSearchSpace{T}`\ni::Integer: the dimension to return the difference between the maximum and minimum values for.\n\nReturns\n\nT or Vector{T} the difference between the maximum and minimum values for the i-th dimension of ss or a vector of all differences if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_max-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_max","text":"dim_max(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the maximum value for the i-th dimension of ss. If i is not specified, returns a vector of all maximum values.\n\nArguments\n\nss::ContinuousRectangularSearchSpace{T}\ni::Integer: the dimension to return the maximum value for.\n\nReturns\n\nT or Vector{T}: the minimum value for the i-th dimension of ss or a vector of all minimum values if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_min-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_min","text":"dim_min(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the minimum value for the i-th dimension of ss. If i is not specified, returns a vector of all minimum values.\n\nArguments\n\nss::RontinuousRectangularSearchSpace{T}\ni::Integer: the dimension to return the minimum value for.\n\nReturns\n\nT or Vector{T}: the minimum value for the i-th dimension of ss or a vector of all minimum values if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_range-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_range","text":"dim_range(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the range of values for the i-th dimension of ss. If i is not specified, returns a vector of all ranges.\n\nArguments\n\nss::ContinuousRectangularSearchSpace{T}\ni::Integer: the dimension to return the range of values for.\n\nReturns\n\nTuple{T, T} or Vector{Tuple{T, T}}: the range of values for the i-th dimension of ss or a vector of all ranges if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.draw_step!-Union{Tuple{T}, Tuple{AbstractVector{T}, MBHStaticDistribution{T}}} where T","page":"Internals","title":"GlobalOptimization.draw_step!","text":"draw_step!(step::AbstractVector{T}, dist::AbstractMBHDistribution{T})\n\nDraws a step from the distribution dist and stores it in step.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.draw_update!-Union{Tuple{T}, Tuple{GlobalOptimization.Hopper{T}, GlobalOptimization.AbstractMBHDistribution{T}}} where T","page":"Internals","title":"GlobalOptimization.draw_update!","text":"draw_update!(hopper::Hopper{T}, distribution::AbstractMBHDistribution{T})\n\nDraws a perterbation from distribution and updates candidate for the hopper hopper.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.enforce_bounds!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, ContinuousRectangularSearchSpace{T}}} where T","page":"Internals","title":"GlobalOptimization.enforce_bounds!","text":"enforce_bounds!(swarm::Swarm{T}, evaluator::BatchEvaluator)\n\nEnforces the bounds of the search space on each candidate in the swarm swarm. If a candidate\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate!-Tuple{Function, AbstractVector, GlobalOptimization.SerialBatchJobEvaluator}","page":"Internals","title":"GlobalOptimization.evaluate!","text":"evaluate!(job::Function, job_ids::AbstractVector{Int}, evaluator::BatchJobEvaluator)\n\nEvaluates job for each element of job_args using the given evaluator.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate!-Tuple{GlobalOptimization.AbstractPopulation, GlobalOptimization.SerialBatchEvaluator}","page":"Internals","title":"GlobalOptimization.evaluate!","text":"evaluate!(pop::AbstractPopulation, evaluator::BatchEvaluator)\n\nEvaluates the fitness of a population using the given evaluator.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate_fitness!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.BatchEvaluator}} where T","page":"Internals","title":"GlobalOptimization.evaluate_fitness!","text":"evaluate_fitness!(swarm::Swarm{T}, evaluator::BatchEvaluator)\n\nEvaluates the fitness of each candidate in the swarm swarm using the evaluator. Updates the swarms best candidates if any are found.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate_with_penalty-Tuple{GlobalOptimization.FeasibilityHandlingEvaluator, AbstractArray}","page":"Internals","title":"GlobalOptimization.evaluate_with_penalty","text":"evaluate_with_penalty(evaluator::FeasibilityHandlingEvaluator, candidate::AbstractArray)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.feasible-Union{Tuple{T}, Tuple{AbstractVector{T}, ContinuousRectangularSearchSpace{T}}} where T","page":"Internals","title":"GlobalOptimization.feasible","text":"feasible(x, ss::ContinuousRectangularSearchSpace)\n\nReturns true if the point x is feasible in the search space ss, otherwise returns false.\n\nArguments\n\nx::AbstractVector{T}: the point to check for feasibility.\nss::ContinuousRectangularSearchSpace{T}: the search space to check for feasibility in.\n\nReturns\n\nBool: true if x is in ss, otherwise false.\n\nThrows\n\nDimensionMismatch: if x does not have the same number of dimensions as ss.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.fill_identity!-Tuple{Any}","page":"Internals","title":"GlobalOptimization.fill_identity!","text":"fill_identity!(mat)\n\nFills the mat in-place with the identity matrix.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.fitness-Tuple{GlobalOptimization.AbstractCandidate}","page":"Internals","title":"GlobalOptimization.fitness","text":"fitness(c::AbstractCandidate)\n\nReturns the fitness of the candidate c.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.fitness-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"GlobalOptimization.fitness","text":"fitness(pop::AbstractPopulation, [i::Integer])\n\nReturns the fitness of the candidates from a population. If i is specified, returns the i-th fitness.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_batch_evaluator-Tuple{SingleHopper}","page":"Internals","title":"GlobalOptimization.get_batch_evaluator","text":"get_batch_evaluator(hopper_type::AbstractHopperType)\n\nReturns the batch job evaluator for a given hopper type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_best_candidate-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.get_best_candidate","text":"get_best_candidate(opt::AbstractOptimizer)\n\nGet the best candidate found by opt.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_best_candidate_in_subset-Tuple{GlobalOptimization.DEPopulation, Any}","page":"Internals","title":"GlobalOptimization.get_best_candidate_in_subset","text":"get_best_candidate_in_subset(population::DEPopulation, idxs)\n\nGet the best candidate in the selected subset of population (as specified by the indices in idxs).\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_best_fitness-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.get_best_fitness","text":"get_best_fitness(opt::AbstractOptimizer)\n\nGet the fitness of the best candidate found by opt.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_elapsed_time-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.get_elapsed_time","text":"get_elapsed_time(opt::AbstractOptimizer)\n\nGet the elapsed time of the optimizer opt in seconds.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_function_tolerance-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_function_tolerance","text":"get_function_tolerance(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the function tolerance option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_function_value_check-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_function_value_check","text":"get_function_value_check(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the function value check option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_general-Tuple{GlobalOptimization.AbstractAlgorithmSpecificOptions}","page":"Internals","title":"GlobalOptimization.get_general","text":"get_general(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the general options from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_hopper_set-Union{Tuple{T}, Tuple{has_penalty}, Tuple{GlobalOptimization.AbstractProblem{has_penalty, ContinuousRectangularSearchSpace{T}}, SingleHopper}} where {has_penalty, T}","page":"Internals","title":"GlobalOptimization.get_hopper_set","text":"get_hopper_set(prob::AbstractProblem, hopper_type::AbstractHopperType)\n\nReturns the hopper set for a given problem and hopper type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_iteration-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.get_iteration","text":"get_iteration(opt::AbstractOptimizer)\n\nGet the current iteration number of the optimizer opt.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_max_iterations-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_max_iterations","text":"get_max_iterations(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the max iterations option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_max_stall_iterations-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_max_stall_iterations","text":"get_max_stall_iterations(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the max stall iterations option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_max_stall_time-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_max_stall_time","text":"get_max_stall_time(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the max stall time option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_max_time-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_max_time","text":"get_max_time(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the max time option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_min_cost-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_min_cost","text":"get_min_cost(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the min cost option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_population-Tuple{GlobalOptimization.AbstractPopulationBasedOptimizer}","page":"Internals","title":"GlobalOptimization.get_population","text":"get_population(opt::AbstractPopulationBasedOptimizer)\n\nReturns the population of the optimizer opt. This should return a subtype of AbstractPopulation.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_scalar_function-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.get_scalar_function","text":"get_scalar_function(prob::AbstractProblem)\n\nReturns cost function plus the infeasibility penalty squared as a scalar value.\nThis is used for PSO (GA, Differential Evolution, etc. if we ever get around to adding those)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_scalar_function_with_penalty-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.get_scalar_function_with_penalty","text":"get_scalar_function_with_penalty(prob::AbstractProblem)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_trace-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_trace","text":"get_trace(opts::AbstractOptions)\n\nReturns the display option from an options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.handle_local_search-Tuple{Any, SingleHopper}","page":"Internals","title":"GlobalOptimization.handle_local_search","text":"handle_local_search(local_search::AbstractLocalSearch, hopper_type::AbstractHopperType)\n\nReturns the local search algorithm for a given hopper type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.handle_stall!-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.handle_stall!","text":"handle_stall!(opt::AbstractOptimizer)\n\nHandles updating the stall related fields of the AbstractOptimizerCache for opt.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.has_gradient-Tuple{GlobalOptimization.AbstractEvaluator}","page":"Internals","title":"GlobalOptimization.has_gradient","text":"has_gradient(evaluator::AbstractEvaluator)\n\nReturns true if the evaluator has a gradient, otherwise, false.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.hop!-Tuple{GlobalOptimization.Hopper, Vararg{Any, 4}}","page":"Internals","title":"GlobalOptimization.hop!","text":"hop!(hopper::Union{Hopper, AbstractHopperSet}, ss, eval, dist, ls)\n\nPerforms a single hop for the hopper hopper in the search space ss using the evaluator eval, the distribution dist, and the local search ls.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Tuple{GlobalOptimization.AbstractMBHDistribution, ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(dist::AbstractMBHDistribution, num_dims)\n\nInitializes the distribution dist with the number of dimensions num_dims.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Tuple{GlobalOptimization.AbstractOptimizerCache, Any}","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(cache::AbstractOptimizerCache, best_fitness)\n\nA helper function to initialize the cache of an optimizer. This function should be called in the initialize!(opt) method of the optimizer.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(opt::AbstractOptimizer)\n\nInitialize the optimizer opt. All memory allocations that are not possible to do in the constructor should be done here when possible.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Tuple{GlobalOptimization.AbstractPopulationBasedOptimizerCache}","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(cache::AbstractPopulationBasedOptimizerCache)\n\nA helper function to initialize the cache of a population based optimizer. This function should be called in the initialize!(opt) method of the optimizer, after initializing the global_best_candidate and global_best_fitness fields of the cache.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Tuple{MATLABVelocityUpdate, Any}","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(vu::AbstractVelocityUpdateScheme, swarm_size::Int)\n\nInitialize the velocity update scheme for a given swarm size.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Union{Tuple{T}, Tuple{GlobalOptimization.Hopper{T}, ContinuousRectangularSearchSpace{T}, GlobalOptimization.FeasibilityHandlingEvaluator}} where T","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(\n    hopper::Hopper{T},\n    search_space::ContinuousRectangularSearchSpace{T},\n    evaluator::FeasibilityHandlingEvaluator{T},\n)\n\nInitializes the hopper position in the search space.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.AbstractPopulationInitialization, ContinuousRectangularSearchSpace{T}}} where T","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(\n    swarm::Swarm{T},\n    pop_init_method::AbstractPopulationInitialization,\n    search_space::ContinuousRectangularSearchSpace{T}\n)\n\nInitializes the swarm population with pop_init_method in the search_space.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize_fitness!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.BatchEvaluator}} where T","page":"Internals","title":"GlobalOptimization.initialize_fitness!","text":"initialize_fitness!(swarm::Swarm{T}, evaluator::BatchEvaluator)\n\nInitializes the fitness of each candidate in the swarm swarm using the evaluator.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.intersection-Union{Tuple{T2}, Tuple{T1}, Tuple{ContinuousRectangularSearchSpace{T1}, ContinuousRectangularSearchSpace{T2}}} where {T1, T2}","page":"Internals","title":"GlobalOptimization.intersection","text":"intersection(\n    ss1::ContinuousRectangularSearchSpace{T1},\n    ss2::ContinuousRectangularSearchSpace{T2}\n)\n\nReturns the intersection of the two search spaces ss1 and ss2 as a new search space.\n\nArguments\n\nss1::ContinuousRectangularSearchSpace{T1}\nss2::ContinuousRectangularSearchSpace{T2}\n\nReturns\n\n`ContinuousRectangularSearchSpace{promote_type(T1, T2)}\n\nThrows\n\nDimensionMismatch: if ss1 and ss2 do not have the same number of dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.mutate!-Union{Tuple{MP}, Tuple{GlobalOptimization.DEPopulation, MP}} where MP<:GlobalOptimization.AbstractMutationParameters","page":"Internals","title":"GlobalOptimization.mutate!","text":"mutate!(population::DEPopulation{T}, F)\n\nMutates the population population using the DE mutation strategy.\n\nThis is an implementation of the unified mutation strategy proposed by Ji Qiang and Chad Mitchell in \"A Unified Differential Evolution Algorithm for Global Optimization\".\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.num_dims-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.num_dims","text":"num_dims(ss::ContinuousRectangularSearchSpace)\n\nReturns the number of dimensions in the search space ss.\n\nArguments\n\nss::ContinuousRectangularSearchSpace\n\nReturns\n\nInteger\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.num_dims-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.num_dims","text":"num_dims(prob::AbstractProblem)\n\nReturns the number of dimensions of the decision vector of the optimization problem prob.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.push!-Union{Tuple{T}, Tuple{GlobalOptimization.MBHStepMemory{T}, AbstractVector{T}, T, T}} where T","page":"Internals","title":"GlobalOptimization.push!","text":"push!(step_memory::MBHStepMemoory{T}, step::Vector{T}, pre_step_fitness::T, post_step_fitness::T)\n\nPushes a step into the step memory.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.push_accepted_step!-Union{Tuple{T}, Tuple{MBHAdaptiveDistribution{T}, AbstractVector{T}, T, T}} where T","page":"Internals","title":"GlobalOptimization.push_accepted_step!","text":"push_accepted_step!(\n    dist::MBHAdaptiveDistribution{T},\n    step::AbstractVector{T},\n    pre_step_fitness::T,\n    post_step_fitness::T,\n) where {T}\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.reset!-Tuple{GlobalOptimization.Hopper}","page":"Internals","title":"GlobalOptimization.reset!","text":"reset!(hopper::Hopper)\n\nResets the candidate to state prior to the last draw_update! call.\n\nNote: This just subtracts the last step from the candidate.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function-Union{Tuple{G}, Tuple{F}, Tuple{SS}, Tuple{has_penalty}, Tuple{OptimizationProblem{has_penalty, SS, F, G}, AbstractArray}} where {has_penalty, SS, F, G}","page":"Internals","title":"GlobalOptimization.scalar_function","text":"scalar_function(prob::OptimizationProblem, x::AbstractArray)\n\nEvaluates the objective function f of the optimization problem prob at x and returns the cost function plus the infeasibility.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function-Union{Tuple{has_penalty}, Tuple{GlobalOptimization.AbstractNonlinearEquationProblem{has_penalty}, AbstractArray}} where has_penalty","page":"Internals","title":"GlobalOptimization.scalar_function","text":"scalar_function(prob::AbstractNonlinearEquationProblem, x::AbstractArray)\n\nEvaluates the set of nonlinear equations f and returns the nonlinear least squares cost plus half the infeasibility squared.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function_with_penalty-Union{Tuple{G}, Tuple{F}, Tuple{SS}, Tuple{has_penalty}, Tuple{OptimizationProblem{has_penalty, SS, F, G}, AbstractArray}} where {has_penalty, SS, F, G}","page":"Internals","title":"GlobalOptimization.scalar_function_with_penalty","text":"scalar_function_with_penalty(prob::OptimizationProblem, x::AbstractArray)\n\nEvaluates the objective function f of the optimization problem prob at x and returns the cost function and the infeasibility penalty term as tuple. i.e., for an OptimizationProblem, this is simply the original function.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function_with_penalty-Union{Tuple{has_penalty}, Tuple{GlobalOptimization.AbstractNonlinearEquationProblem{has_penalty}, AbstractArray}} where has_penalty","page":"Internals","title":"GlobalOptimization.scalar_function_with_penalty","text":"scalar_function_with_penalty(prob::AbstractNonlinearEquationProblem, x::AbstractArray)\n\nEvaluates the set of nonlinear equations f and returns the nonlinear least squares cost and the infeasibility penalty term as a tuple.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.search_space-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.search_space","text":"search_space(prob::AbstractProblem)\n\nReturns the search space of the optimization problem prob.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.selection!-Tuple{GlobalOptimization.DEPopulation}","page":"Internals","title":"GlobalOptimization.selection!","text":"selection!(population::DEPopulation{T}, evaluator::BatchEvaluator)\n\nReplace candidates with mutants if they have a better fitness.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.set_fitness!-Tuple{GlobalOptimization.AbstractCandidate, Any}","page":"Internals","title":"GlobalOptimization.set_fitness!","text":"set_fitness!(c::AbstractCandidate, fitness)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.set_fitness!-Tuple{GlobalOptimization.AbstractPopulation, Vector}","page":"Internals","title":"GlobalOptimization.set_fitness!","text":"set_fitness(pop::AbstractPopulation, fitness, [i::Integer])\n\nSets the fitness of the candidates from a population. If i is specified, sets the i-th fitness.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.step!-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Internals","title":"GlobalOptimization.step!","text":"step!(opt::AbstractOptimizer)\n\nPerform a single step/iteration with the optimizer opt. This function should be non-allocating if possible.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.step!-Tuple{GlobalOptimization.Swarm}","page":"Internals","title":"GlobalOptimization.step!","text":"step!(swarm::Swarm)\n\nSteps the swarm swarm forward one iteration.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.step_MAD_median-Union{Tuple{T}, Tuple{GlobalOptimization.MBHStepMemory{T}, Integer}} where T","page":"Internals","title":"GlobalOptimization.step_MAD_median","text":"step_MAD_median(step_memory::MBHStepMemory{T}, var_idx::Integer)\n\nReturns the mean absolute deviation (MAD) around the median of the step memory. If var_idx is specified, then the MAD median of the step memory for the variable at index var_idx is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.step_std-Union{Tuple{T}, Tuple{GlobalOptimization.MBHStepMemory{T}, Integer}} where T","page":"Internals","title":"GlobalOptimization.step_std","text":"step_std(step_memory::MBHStepMemory{T}, var_idx::Integer)\n\nReturns the standard deviation of the step memory. If var_idx is specified, then the standard deviation of the step memory for the variable at index var_idx is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.update_fitness!-Union{Tuple{T}, Tuple{GlobalOptimization.SingleHopperSet{T}, MBHStaticDistribution{T}}} where T","page":"Internals","title":"GlobalOptimization.update_fitness!","text":"update_fitness!(hopper::AbstractHopperSet{T}, distribution::AbstractMBHDistribution{T})\n\nUpdates the hopper fitness information after previously evaluating the fitness of the hopper.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.update_global_best!-Tuple{GlobalOptimization.AbstractPopulationBasedOptimizer}","page":"Internals","title":"GlobalOptimization.update_global_best!","text":"update_global_best!(opt::AbstractPopulationBasedOptimizer)\n\nUpdates the global best candidate and fitness in the cache of the population based optimizer opt when a better candidate is found. Returns true if the global best candidate was updated, false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.update_velocity!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.AbstractRandomNeighborhoodVelocityUpdateScheme}} where T","page":"Internals","title":"GlobalOptimization.update_velocity!","text":"update_velocity!(swarm::Swarm, vu::AbstractVelocityUpdateScheme)\n\nUpdate the velocity of each candidate in the swarm using the specified velocity update scheme.\n\n\n\n\n\n","category":"method"},{"location":"algs/de/","page":"-","title":"-","text":"To be continued...","category":"page"},{"location":"algs/mbh/","page":"-","title":"-","text":"Not implemented...","category":"page"},{"location":"lib/public/#Public-API-Documentation","page":"Public API","title":"Public API Documentation","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Documentation for GlobalOptimization's public interface.","category":"page"},{"location":"lib/public/#Contents","page":"Public API","title":"Contents","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Pages = [\"public.md\"]\nDepth = 2:3","category":"page"},{"location":"lib/public/#Problems","page":"Public API","title":"Problems","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem{has_penalty, SS, F, G}\n\nA nonlinear least squares problem. Contains the nonlinear equations and search space.\n\nFields\n\nf::F: The nonlinear equations.\ng!::G: The jacobian of the nonlinear equations.\nss::SS: The search space.\nn::Int: The number of residuals.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem-Union{Tuple{F}, Tuple{F, AbstractVector{<:Real}, AbstractVector{<:Real}, Int64}} where F<:Function","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem(f, [g], LB, UB)\n\nConstructs a nonlinear least squares problem with nonlinear function f, optional Jacobian g, and a search space.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nLB::AbstractVector{<:Real}: The lower bounds of the search space.\nUB::AbstractVector{<:Real}: The upper bounds of the search space.\n\nReturns\n\nNonlinearLeastSquaresProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - x[3], x[2] - x[3]]\njulia> LB = [-5.0, -5.0, -5.0];\njulia> UB = [ 5.0, 5.0, 5.0];\njulia> prob = NonlinearLeastSquaresProblem(f, ss, LB, UB, 2)\nNonlinearLeastSquaresProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0, -5.0], [5.0, 5.0, 5.0], [10.0, 10.0, 10.0]), 2)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem-Union{Tuple{F}, Tuple{has_penalty}, Tuple{T}, Tuple{F, GlobalOptimization.SearchSpace{T}, Int64}} where {T, has_penalty, F<:Function}","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem{has_penalty}(f::F, [g::G], ss::SS, num_resid::Int)\n\nConstructs a nonlinear least squares problem with nonlinear functions f, optional jacobian g, and search space ss. If has_penalty is specified as true, then the nonlinear function must return a Tuple{AbstractArray{T},T} for a given x of type AbstractArray{T}.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\nnum_resid::Int: The number of residuals.\n\nReturns\n\nNonlinearLeastSquaresProblem{has_penalty, SS, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - x[3], x[2] - x[3]]\njulia> LB = [-5.0, -5.0, -5.0];\njulia> UB = [ 5.0, 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearLeastSquaresProblem(f, ss, 2)\nNonlinearLeastSquaresProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0, -5.0], [5.0, 5.0, 5.0], [10.0, 10.0, 10.0]), 2)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem-Union{Tuple{SS}, Tuple{F}, Tuple{F, SS, Int64}} where {F<:Function, SS<:GlobalOptimization.SearchSpace}","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem(f, [g], ss)\n\nConstructs a nonlinear least squares problem with nonlinear function f, optional Jacobian g, and a search space.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\n\nReturns\n\nNonlinearLeastSquaresProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - x[3], x[2] - x[3]]\njulia> LB = [-5.0, -5.0, -5.0];\njulia> UB = [ 5.0, 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearLeastSquaresProblem(f, ss, 2)\nNonlinearLeastSquaresProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0, -5.0], [5.0, 5.0, 5.0], [10.0, 10.0, 10.0]), 2)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem{has_penalty, SS, F, G}\n\nA nonlinear problem. Contains the nonlinear equations and search space.\n\nFields\n\nf::F: The nonlinear equations.\ng!::G: The jacobian of the nonlinear equations.\nss::SS: The search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem-Union{Tuple{F}, Tuple{F, AbstractVector{<:Real}, AbstractVector{<:Real}}} where F<:Function","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem(f, [g], LB, UB)\n\nConstructs a nonlinear problem with nonlinear function f, optional Jacobian g, and a continuous rectangular search space defined by the bounds LB and UB.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nLB::AbstractVector{<:Real}: The lower bounds of the search space.\nUB::AbstractVector{<:Real}: The upper bounds of the search space.\n\nReturns\n\nNonlinearProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - 2.0, x[2] - 2.0]\njulia> LB = [-5.0, -5.0];\njulia> UB = [ 5.0, 5.0];\njulia> prob = NonlinearProblem(f, LB, UB)\nNonlinearProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0], [5.0, 5.0], [10.0, 10.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem-Union{Tuple{F}, Tuple{has_penalty}, Tuple{T}, Tuple{F, GlobalOptimization.SearchSpace{T}}} where {T, has_penalty, F<:Function}","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem{has_penalty}(f::F, [g::G], ss::SS)\n\nConstructs a nonlinear problem with nonlinear functions f, optional jacobian g, and search space ss. If has_penalty is specified as true, then the nonlinear function must return a Tuple{AbstractArray{T},T} for a given x of type AbstractArray{T}.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\n\nReturns\n\nNonlinearProblem{has_penalty, SS, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - 2.0, x[2] - 2.0]\njulia> LB = [-5.0, -5.0];\njulia> UB = [ 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearProblem(f, ss)\nNonlinearProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0], [5.0, 5.0], [10.0, 10.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem-Union{Tuple{SS}, Tuple{F}, Tuple{F, SS}} where {F<:Function, SS<:GlobalOptimization.SearchSpace}","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem(f, [g], ss)\n\nConstructs a nonlinear problem with nonlinear function f, optional Jacobian g, and a search space.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\n\nReturns\n\nNonlinearProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - 2.0, x[2] - 2.0]\njulia> LB = [-5.0, -5.0];\njulia> UB = [ 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearProblem(f, ss)\nNonlinearProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0], [5.0, 5.0], [10.0, 10.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem{has_penalty, SS, F, G}\n\nAn optimization problem. Contains the objective function and search space.\n\nFields\n\nf::F: The objective function.\ng!::G: The gradient of the objective function.\nss::SS: The search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem-Union{Tuple{F}, Tuple{F, AbstractVector{<:Real}, AbstractVector{<:Real}}} where F<:Function","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem(f, [g], LB, UB)\n\nConstructs an optimization problem with objective function f, optional gradient g, and a ContinuousRectangularSearchSpace defined by LB and UB.\n\nArguments\n\nf::F: The objective function.\nLB::AbstractVector{<:Real}: The lower bounds of the search space.\nUB::AbstractVector{<:Real}: The upper bounds of the search space.\n\nReturns\n\nOptimizationProblem{ContinuousRectangularSearchSpace, F}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = sum(x.^2); # Simple sphere function\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> prob = OptimizationProblem(f, LB, UB)\nOptimizationProblem{ContinuousRectangularSearchSpace{Float64}, typeof(f)}(f, ContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem-Union{Tuple{F}, Tuple{has_penalty}, Tuple{T}, Tuple{F, GlobalOptimization.SearchSpace{T}}} where {T, has_penalty, F<:Function}","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem{has_penalty}(f::F, [g::G], ss::SS)\n\nConstructs an optimization problem with objective function f, optional gradient g, and search space ss. If has_penalty is specified as true, then the objective function must return a Tuple{T,T} for a given x of type AbstractArray{T}.\n\nArguments\n\nf::F: The objective function.\ng::G: The gradient of the objective function.\nss::SS: The search space.\n\nReturns\n\nOptimizationProblem{SS, F}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = sum(x.^2); # Simple sphere function\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = OptimizationProblem(f, ss)\nOptimizationProblem{ContinuousRectangularSearchSpace{Float64}, typeof(f)}(f, ContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem-Union{Tuple{SS}, Tuple{F}, Tuple{F, SS}} where {F<:Function, SS<:GlobalOptimization.SearchSpace}","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem(f, [g], ss)\n\nConstructs an optimization problem with objective function f, optimal gradient g, and a search space.\n\nArguments\n\nf::F: The objective function.\ng::G: The gradient of the objective function.\nss::SS: The search space.\n\nReturns\n\nOptimizationProblem{ContinuousRectangularSearchSpace, F}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = sum(x.^2); # Simple sphere function\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> prob = OptimizationProblem(f, ContinuousRectangularSearchSpace(LB, UB))\nOptimizationProblem{ContinuousRectangularSearchSpace{Float64}, typeof(f)}(f, ContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Search-Space","page":"Public API","title":"Search Space","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.ContinuousRectangularSearchSpace","page":"Public API","title":"GlobalOptimization.ContinuousRectangularSearchSpace","text":"ContinuousRectangularSearchSpace{T <: AbstractFloat}\n\nA RectangularSearchSpace formed by a single continuous set.\n\nFields\n\ndim_min::Vector{T}: A vector of minimum values for each dimension.\ndim_max::Vector{T}: A vector of maximum values for each dimension.\ndim_delta::Vector{T}: A vector of the difference between the maximum and minimum values for each dimension.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.ContinuousRectangularSearchSpace-Union{Tuple{T2}, Tuple{T1}, Tuple{AbstractVector{T1}, AbstractVector{T2}}} where {T1<:Real, T2<:Real}","page":"Public API","title":"GlobalOptimization.ContinuousRectangularSearchSpace","text":"ContinuousRectangularSearchSpace(dim_min::AbstractVector{T}, dim_max::AbstractVector{T})\n\nConstructs a new ContinuousRectangularSearchSpace with minimum values dim_min and maximum values dim_max.\n\nArguments\n\ndim_min::AbstractVector{T}: A vector of minimum values for each dimension.\ndim_max::AbstractVector{T}: A vector of maximum values for each dimension.\n\nReturns\n\nContinuousRectangularSearchSpace{T}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB)\nContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0])\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Optimization","page":"Public API","title":"Optimization","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.optimize!-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Public API","title":"GlobalOptimization.optimize!","text":"optimize!(opt::AbstractOptimizer)\n\nPerform optimization using the optimizer opt. Returns the results of the optimization.\n\nArguments\n\nopt::AbstractOptimizer: The optimizer to use.\n\nReturns\n\nResults: The results of the optimization. See the Results docstring for details   on its contents.\n\nExample\n\njulia> using GlobalOptimization\njulia> f(x) = sum(x.^2) # Simple sphere function\njulia> prob = OptimizationProblem(f, [-1.0, 0.0], [1.0, 2.0])\njulia> pso = SerialPSO(prob)\njulia> results = optimize!(pso)\nResults:\n - Best function value: 6.696180996034206e-20\n - Best candidate: [-2.587698010980842e-10, 0.0]\n - Iterations: 26\n - Time: 0.004351139068603516 seconds\n - Exit flag: MAXIMUM_STALL_ITERATIONS_EXCEEDED\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Population-Initialization","page":"Public API","title":"Population Initialization","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.LatinHypercubeInitialization","page":"Public API","title":"GlobalOptimization.LatinHypercubeInitialization","text":"LatinHypercubeInitialization\n\nInitializes a population using optimal Latin hypercube sampling as implemented in LatinHypercubeSampling.jl.\n\nFields:\n\ngens::Int: Number of GA generations to use to generate the Latin hypercube samples.\nrng::U: Random number generator to use for the Latin hypercube sampling.\npop_size::Int: Size of the GA population used to generate the Latin hypercube samples.\nn_tour::Int: Number of tours to use in the GA.\np_tour::Float64: Probability of tour to use in the GA.\ninter_sample_weight::Float64: Weight of the inter-sample distance in the GA.\nperiodic_ae::Bool: Whether to use periodic adaptive evolution in the GA.\nae_power::Float64: Power of the adaptive evolution in the GA.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.LatinHypercubeInitialization-Tuple{Int64}","page":"Public API","title":"GlobalOptimization.LatinHypercubeInitialization","text":"LatinHypercubeInitialization(gens::Int = 10; kwargs...)\n\nInitializes a Latin hypercube sampling method with the given parameters.\n\nArguments\n\ngens::Int: Number of GA generations to use to generate the Latin hypercube samples.   Defaults to 10.\n\nKeyword Arguments\n\nrng::U: Random number generator to use for the Latin hypercube sampling.   Defaults to Random.GLOBAL_RNG.\npop_size::Int: Size of the GA population used to generate the Latin hypercube samples.   Defaults to 100.\nn_tour::Int: Number of tours to use in the GA. Defaults to 2.\np_tour::Float64: Probability of tour to use in the GA. Defaults to 0.8.\ninter_sample_weight::Float64: Weight of the inter-sample distance in the GA.   Defaults to 1.0.\nperiodic_ae::Bool: Whether to use periodic adaptive evolution in the GA.   Defaults to false.\nae_power::Float64: Power of the adaptive evolution in the GA. Defaults to 2.0.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Function-Evaluation-Methods","page":"Public API","title":"Function Evaluation Methods","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.SerialFunctionEvaluation","page":"Public API","title":"GlobalOptimization.SerialFunctionEvaluation","text":"SerialFunctionEvaluation\n\nA function evaluation method that evaluates the fitness of a candidate in serial.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SerialFunctionEvaluation-Tuple{}","page":"Public API","title":"GlobalOptimization.SerialFunctionEvaluation","text":"SerialFunctionEvaluation()\n\nConstruct a SerialFunctionEvaluation object.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.ThreadedFunctionEvaluation","page":"Public API","title":"GlobalOptimization.ThreadedFunctionEvaluation","text":"ThreadedFunctionEvaluation{S <: ChunkSplitters.Split}\n\nA function evaluation method that evaluates the fitness of a candidate in parallel using     multi-threading from Base.Threads.jl.\n\nFields\n\nn::Int: The number of batch jobs to split the workload into using   ChunkSplitters.jl.\nsplit::S: The chunk splitter to use. See ChunkSplitters.jl   for more information.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.ThreadedFunctionEvaluation-Tuple{}","page":"Public API","title":"GlobalOptimization.ThreadedFunctionEvaluation","text":"ThreadedFunctionEvaluation(\n    n::Int=Threads.nthreads(),\n    split::S=ChunkSplitters.RoundRobin(),\n)\n\nConstruct a ThreadedFunctionEvaluation object.\n\nKeyword Arguments\n\nn::Int: The number of batch jobs to split the workload into using   ChunkSplitters.jl.\nsplit::S: The chunk splitter to use. See ChunkSplitters.jl   for more information.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.PolyesterFunctionEvaluation","page":"Public API","title":"GlobalOptimization.PolyesterFunctionEvaluation","text":"PolyesterFunctionEvaluation\n\nA function evaluation method that evaluates the fitness of a candidate in parallel using     Polyester.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.PolyesterFunctionEvaluation-Tuple{}","page":"Public API","title":"GlobalOptimization.PolyesterFunctionEvaluation","text":"PolyesterFunctionEvaluation()\n\nConstruct a PolyesterFunctionEvaluation object.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Algorithms","page":"Public API","title":"Algorithms","text":"","category":"section"},{"location":"lib/public/#Particle-Swarm-Optimization","page":"Public API","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.PSO","page":"Public API","title":"GlobalOptimization.PSO","text":"PSO\n\nParticle Swarm Optimization (PSO) algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.PSO-Union{Tuple{GlobalOptimization.AbstractProblem{has_penalty, SS}}, Tuple{has_penalty}, Tuple{SS}, Tuple{T}} where {T<:AbstractFloat, SS<:ContinuousRectangularSearchSpace{T}, has_penalty}","page":"Public API","title":"GlobalOptimization.PSO","text":"PSO(prob::AbstractProblem{has_penalty,SS}; kwargs...)\n\nConstructs a PSO algorithm with the given options.\n\nArguments\n\nprob::AbstractProblem{has_penalty,SS}: The problem to solve.\n\nKeyword Arguments\n\neval_method::AbstractFunctionEvaluationMethod=SerialFunctionEvaluation(): The method to use for evaluating the objective function.\nnum_particles::Int = 100: The number of particles to use.\npopulation_initialization::AbstractPopulationInitialization = UniformInitialization(): The method to use for initializing the population.\nvelocity_update::AbstractVelocityUpdateScheme = MATLABVelocityUpdate(): The method to use for updating the velocity of the particles.\ninitial_space::Union{Nothing,ContinuousRectangularSearchSpace}=nothing: The initial bounds for the search space.\nmax_iterations::Integer=1000: The maximum number of iterations.\nfunction_tolerance::Real=1e-6: The function tolerance (stall-based stopping criteria).\nmax_stall_time::Real=60.0: The maximum stall time (in seconds).\nmax_stall_iterations::Integer=100: The maximum number of stall iterations.\nmax_time::Real=60.0: The maximum time (in seconds) to allow for optimization.\nmin_cost::Real=(-Inf): The minimum cost to allow for optimization.\nfunction_value_check::Union{Val{false},Val{true}}=Val(true): Whether to check the function value   for bad values (i.e., Inf or NaN).\nshow_trace::Union{Val{false},Val{true}}=Val(false): Whether to show the trace.\nsave_trace::Union{Val{false},Val{true}}=Val(false): Whether to save the trace.\nsave_file::String=\"trace.txt\": The file to save the trace to.\ntrace_level::TraceLevel=TraceMinimal(1): The trace level to use.\n\nReturns\n\nPSO: The PSO algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Velocity-Update-Schemes","page":"Public API","title":"Velocity Update Schemes","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"The PSO velocity update is the primary mechanism that drives the stochastic  optimization process. The currently implemented velocity update schemes can  be described by the following:","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Consider a swarm of n particles mathcalS = mathbfp_i_i=12dotsn. Each particle mathbfp_i has the following attributes associated with it:","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"position: mathbfx_i\nvelocity: mathbfv_i,\nbest position: mathbfx_ib\nbest fitness: f_ib = f(mathbfx_ib)","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"At each iteration of the PSO algorithm, the velocity of each particle is updated prior to  updating the position of each particle with mathbfx_i = mathbfx_i + mathbfv_i. This velocity update is described (for the i-th particle) by the following expression:","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"mathbfv_i = w mathbfv_i +     y_1 mathbfr_1 (mathbfx_ib - mathbfx_i) +     y_2 mathbfr_2 (mathbfx_b - mathbfx_i)","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"where w is the inertia,  r_1 and r_2 are realizations of a random vector described by the multivariate uniform distribution mathcalU(mathbf0 mathbf1), y_1 is the self-adjustment weight, y_2 is the social adjustment weight, and mathbfx_b is the best position in the neighborhood of the i-th particle mathcalN_i. That is, mathbfx_b = undersetxinmathcalX_bmathrmargmin(f(x)) where mathcalX_bi =  mathbfx_ib _mathbfp_i in mathcalN_i and mathcalN_i is a set containing a randomly selected subset of the particles in mathcalS (not including mathbfp_i). Both the size of mathcalN_i and  the inertia w are handle differently depending on the velocity update scheme used.","category":"page"},{"location":"lib/public/#GlobalOptimization.MATLABVelocityUpdate","page":"Public API","title":"GlobalOptimization.MATLABVelocityUpdate","text":"MATLABVelocityUpdate <: AbstractRandomNeighborhoodVelocityUpdateScheme\n\nA velocity update scheme employed by the MATLAB PSO algorithm. This scheme is described as follows:\n\nIn this velocity update scheme, the size of the neighborhood, as well as the inertia weight, are adaptively updated as follows:\n\nPrior to First Iteration:\n\nSet the inertial weight w: w = inertia_range[2]\nSet the minimum neighborhood size: minimum_neighborhood_size = max(2, floor(Int, swarm_size * minimum_neighborhood_fraction))\nSet the neighborhood size: N = minimum_neighborhood_size\nSet counter: c = 0\n\nAfter Evaluating Swarm Fitness Each Iteration:\n\nIf the best fitness of the swarm has improved:\nDecrease the counter: c = max(0, c - 1)\nSet the neighborhood size to the minimum: N = minimum_neighborhood_size\nUpdate the inertia weight:\nIf c < 2; w = 2.0 * w\nIf c > 5; w = 0.5 * w\nClamp w to lie in [inertia_range[1], inertia_range[2]]\nIf the best fitness of the swarm has not improved:\nIncrease the counter: c += 1\nIncrease the neighborhood size:  N = min(N + minimum_neighborhood_size, swarm_size - 1)\n\nFields\n\nswarm_size::Int: The size of the swarm.\ninertia_range::Tuple{Float64,Float64}: The range of inertia weights.\nminimum_neighborhood_fraction::Float64: The minimum fraction of the swarm size to be used as the neighborhood size.\nminimum_neighborhood_size::Int: The minimum neighborhood size.\nself_adjustment_weight::Float64: The self-adjustment weight.\nsocial_adjustment_weight::Float64: The social adjustment weight.\nw::Float64: The inertia weight.\nc::Int: The stall iteration counter.\nN::Int: The neighborhood size.\nindex_vector::Vector{UInt16}: A vector used to store the indices of the particles in the   swarm. Used for random neighborhood selection without allocations.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MATLABVelocityUpdate-Tuple{}","page":"Public API","title":"GlobalOptimization.MATLABVelocityUpdate","text":"MATLABVelocityUpdate(;\n    inertia_range::Tuple{AbstractFloat,AbstractFloat}=(0.1, 1.0),\n    minimum_neighborhood_fraction::AbstractFloat=0.25,\n    self_adjustment_weight::AbstractFloat=1.49,\n    social_adjustment_weight::AbstractFloat=1.49,\n)\n\nCreate a new instance of the MATLABVelocityUpdate velocity update scheme.\n\nKeyword Arguments\n\ninertia_range::Tuple{AbstractFloat,AbstractFloat}: The range of inertia weights.\nminimum_neighborhood_fraction::AbstractFloat: The minimum fraction of the swarm size to be used as the neighborhood size.\nself_adjustment_weight::AbstractFloat: The self-adjustment weight.\nsocial_adjustment_weight::AbstractFloat: The social adjustment weight.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.CSRNVelocityUpdate","page":"Public API","title":"GlobalOptimization.CSRNVelocityUpdate","text":"CSRNVelocityUpdate <: AbstractRandomNeighborhoodVelocityUpdateScheme\n\nA velocity update scheme employed a Constant Size Random Neighborhood (CSRN).\n\nIn this velocity update scheme, the size of the neighborhood is constant and set based on the specified neighborhood_fraction (i.e., the fraction of the swarm size to be considered to lie in a neighborhood). However, the inertia is adaptively updated as follows:\n\nPrior to First Iteration:  Set the inertial weight w: w = inertia_range[2]\n\nAfter Evaluating Swarm Fitness Each Iteration:\n\nIf stall_iteration < 2; w = 2.0 * w\nIf stall_iteration > 5; w = 0.5 * w\nClamp w to lie in [inertia_range[1], inertia_range[2]]\n\nNote that stall_iteration is the number of iterations since the global best position found so far was improved by a specified function_tolerance (see PSO keyword arguments).\n\nFields\n\ninertia_range::Tuple{Float64,Float64}: The range of inertia weights.\nneighborhood_fraction::Float64: The fraction of the swarm size to be used as the neighborhood size.\nN::Int: The neighborhood size.\nself_adjustment_weight::Float64: The self-adjustment weight.\nsocial_adjustment_weight::Float64: The social adjustment weight.\nw::Float64: The inertia weight.\nindex_vector::Vector{UInt16}: A vector used to store the indices of the particles in the   swarm. Used for random neighborhood selection without allocations.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CSRNVelocityUpdate-Tuple{}","page":"Public API","title":"GlobalOptimization.CSRNVelocityUpdate","text":"CSRNVelocityUpdate(;\n    inertia_range::Tuple{AbstractFloat,AbstractFloat}=(0.1, 1.0),\n    neighborhood_fraction::AbstractFloat=0.25,\n    self_adjustment_weight::AbstractFloat=1.49,\n    social_adjustment_weight::AbstractFloat=1.49,\n)\n\nCreate a new instance of the CSRNVelocityUpdate velocity update scheme.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Differential-Evolution","page":"Public API","title":"Differential Evolution","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.DE","page":"Public API","title":"GlobalOptimization.DE","text":"DE\n\nDifferential Evolution (DE) algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.DE-Union{Tuple{GlobalOptimization.AbstractProblem{has_penalty, SS}}, Tuple{has_penalty}, Tuple{SS}, Tuple{T}, Tuple{CP}, Tuple{MP}} where {MP<:GlobalOptimization.AbstractMutationParameters, CP<:GlobalOptimization.AbstractCrossoverParameters, T<:AbstractFloat, SS<:ContinuousRectangularSearchSpace{T}, has_penalty}","page":"Public API","title":"GlobalOptimization.DE","text":"DE(prob::AbstractProblem{has_penalty,SS}; kwargs...)\n\nConstruct a serial Differential Evolution (DE) algorithm with the given options.\n\nArguments\n\nprob::AbstractProblem{has_penalty,SS}: The problem to solve.\n\nKeyword Arguments\n\neval_method::AbstractFunctionEvaluationMethod=SerialFunctionEvaluation(): The method to use for evaluating the objective function.\nnum_candidates::Integer=100: The number of candidates in the population.\npopulation_initialization::AbstractPopulationInitialization=UniformInitialization(): The population initialization method.\nmutation_params::MP=SelfMutationParameters(Rand1()): The mutation strategy parameters.\ncrossover_params::CP=BinomialCrossoverParameters(0.6): The crossover strategy parameters.\ninitial_space::Union{Nothing,ContinuousRectangularSearchSpace}=nothing: The initial bounds for the search space.\nmax_iterations::Integer=1000: The maximum number of iterations.\nfunction_tolerance::Real=1e-6: The function tolerance (stall-based stopping criteria).\nmax_stall_time::Real=60.0: The maximum stall time (in seconds).\nmax_stall_iterations::Integer=100: The maximum number of stall iterations.\nmax_time::Real=60.0: The maximum time (in seconds) to allow for optimization.\nmin_cost::Real=(-Inf): The minimum cost to allow for optimization.\nfunction_value_check::Union{Val{false},Val{true}}=Val(true): Whether to check the function value   for bad values (i.e., Inf or NaN).\nshow_trace::Union{Val{false},Val{true}}=Val(false): Whether to show the trace.\nsave_trace::Union{Val{false},Val{true}}=Val(false): Whether to save the trace.\nsave_file::String=\"trace.txt\": The file to save the trace to.\ntrace_level::TraceLevel=TraceMinimal(1): The trace level to use.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Mutation-Parameters","page":"Public API","title":"Mutation Parameters","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MutationParameters","page":"Public API","title":"GlobalOptimization.MutationParameters","text":"MutationParameters{\n    AS<:AbstractAdaptationStrategy,\n    MS<:AbstractMutationOperator,\n    S<:AbstractSelector,\n    D,\n}\n\nThe parameters for a DE mutation strategy that applies to all current and future candidates in the population.\n\nFields\n\nF1::Float64: The F₁ weight in the unified mutation strategy.\nF2::Float64: The F₂ weight in the unified mutation strategy.\nF3::Float64: The F₃ weight in the unified mutation strategy.\nF4::Float64: The F₄ weight in the unified mutation strategy.\nsel<:AbstractSelector: The selector used to select the candidates considered in   mutation.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the mutation   parameters. Note that this should generally be a distribution from   Distributions.jl, but the only strict   requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MutationParameters-NTuple{4, Any}","page":"Public API","title":"GlobalOptimization.MutationParameters","text":"MutationParameters(F1, F2, F3, F4; sel=SimpleSelector())\n\nCreates a MutationParameters object with the specified (constant) mutation parameters. These constant mutation parameters are used for all candidates in the population and define a unified mutation strategy as defined in Ji Qiang and Chad Mitchell \"A Unified Differential Evolution Algorithm for Global Optimization,\" 2014, https://www.osti.gov/servlets/purl/1163659\n\nArguments\n\nF1::Float64: The F₁ weight in the unified mutation strategy.\nF2::Float64: The F₂ weight in the unified mutation strategy.\nF3::Float64: The F₃ weight in the unified mutation strategy.\nF4::Float64: The F₄ weight in the unified mutation strategy.\n\nKeyword Arguments\n\nsel::AbstractSelector: The selector used to select the candidates considered in   mutation. Defaults to SimpleSelector().\n\nReturns\n\nMutationParameters{NoAdaptation,Unified,typeof(sel),Nothing}: A mutation parameters   object with the specified mutation parameters and selector.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = MutationParameters(0.5, 0.5, 0.5, 0.5)\nMutationParameters{GlobalOptimization.NoAdaptation, Unified, SimpleSelector, Nothing}(0.5, 0.5, 0.5, 0.5, SimpleSelector(), nothing)\n\njulia> using GlobalOptimization\njulia> params = MutationParameters(0.5, 0.5, 0.5, 0.5; sel=RadiusLimitedSelector(2))\nMutationParameters{GlobalOptimization.NoAdaptation, Unified, RadiusLimitedSelector, Nothing}(0.5, 0.5, 0.5, 0.5, RadiusLimitedSelector(2, UInt16[0x6cf0, 0x0c33, 0x0001, 0x0000, 0x0560]), nothing)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.MutationParameters-Tuple{GlobalOptimization.AbstractMutationOperator}","page":"Public API","title":"GlobalOptimization.MutationParameters","text":"MutationParameters(\n    strategy::MS;\n    dist=default_mutation_dist,\n    sel=SimpleSelector(),\n)\n\nCreates a MutationParameters object with the specified mutation strategy with mutation parameter random adaptation. The mutation parameters are adaptively sampled from the provided dist, clamped to the range (0, 1].\n\nArguments\n\nstrategy::MS: The mutation strategy to use. This should be one of the mutation   strategies defined in this module (e.g., Rand1, Best2, etc.).\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   mutation parameters each iteration. Note that this should generally be a   distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   GlobalOptimization.default_mutation_dist, which is a mixture model comprised of   two Cauchy distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2).\nwhere mu = 065 10 and sigma = 01 01.\nsel::AbstractSelector: The selector used to select the candidates considered in   mutation. Defaults to SimpleSelector().\n\nReturns\n\nMutationParameters{RandomAdaptation,typeof(strategy),typeof(sel),typeof(dist)}: A   mutation parameters object with the specified mutation strategy and selector.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = MutationParameters(Rand1())\nMutationParameters{GlobalOptimization.RandomAdaptation, Rand1, SimpleSelector, Distributions.MixtureModel{Distributions.Univariate, Distributions.Continuous, Distributions.Cauchy, Distributions.Categorical{Float64, Vector{Float64}}}}(0.0, 1.0, 0.8450801042502032, 0.0, SimpleSelector(), MixtureModel{Distributions.Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Distributions.Cauchy{Float64}(μ=0.65, σ=0.1)\ncomponents[2] (prior = 0.5000): Distributions.Cauchy{Float64}(μ=1.0, σ=0.1)\n)\n\njulia> using GlobalOptimization\njulia> using Distributions\njulia> params = MutationParameters(Rand1(); dist=Normal(0.5, 0.1))\nMutationParameters{GlobalOptimization.RandomAdaptation, Rand1, SimpleSelector, Normal{Float64}}(0.0, 1.0, 0.5061103661726901, 0.0, SimpleSelector(), Normal{Float64}(μ=0.5, σ=0.1))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.SelfMutationParameters","page":"Public API","title":"GlobalOptimization.SelfMutationParameters","text":"SelfMutationParameters{\n    AS<:AbstractAdaptationStrategy,\n    MS<:AbstractMutationOperator,\n    S<:AbstractSelector,\n    D,\n}\n\nThe parameters for a DE mutation strategy that applies a mutation strategy with unique parameters for each candidate in the population.\n\nFields\n\nFs::Vector{SVector{4,Float64}}: The mutation parameters for each candidate in the   population. Each element of the vector is an SVector{4} containing the F₁, F₂, F₃, and   F₄ weights for the unified mutation strategy.\nsel<:AbstractSelector: The selector used to select the candidates considered in mutation.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the mutation   parameters. Note that this should generally be a distribution from   Distributions.jl, but the only strict   requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SelfMutationParameters-Tuple{GlobalOptimization.AbstractMutationOperator}","page":"Public API","title":"GlobalOptimization.SelfMutationParameters","text":"SelfMutationParameters(\n    strategy::MS;\n    dist=default_mutation_dist,\n    sel=SimpleSelector(),\n)\n\nCreates a SelfMutationParameters object with the specified mutation strategy and mutation parameter random adaptation. The mutation parameters are adaptively sampled from the provided dist, clamped to the range (0, 1].\n\nArguments\n\nstrategy::MS: The mutation strategy to use. This should be one of the mutation   strategies defined in this module (e.g., Rand1, Best2, etc.).\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   mutation parameters each iteration. Note that this should generally be a   distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   GlobalOptimization.default_mutation_dist, which is a mixture model comprised of   two Cauchy distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2).\nwhere mu = 065 10 and sigma = 01 01.\nsel::AbstractSelector: The selector used to select the candidates considered in   mutation. Defaults to SimpleSelector().\n\nReturns\n\nSelfMutationParameters{RandomAdaptation,typeof(strategy),typeof(sel),typeof(dist)}:   A mutation parameters object with the specified mutation strategy and selector.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = SelfMutationParameters(Rand1())\nSelfMutationParameters{GlobalOptimization.RandomAdaptation, Rand1, SimpleSelector, MixtureModel{Univariate, Continuous, Cauchy, Categorical{Float64, Vector{Float64}}}}(StaticArraysCore.SVector{4, Float64}[], SimpleSelector(), MixtureModel{Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Cauchy{Float64}(μ=0.65, σ=0.1)\ncomponents[2] (prior = 0.5000): Cauchy{Float64}(μ=1.0, σ=0.1)\n)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.Rand1","page":"Public API","title":"GlobalOptimization.Rand1","text":"Rand1\n\nThe DE/rand/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + Fleft(mathbfx_r_2 - mathbfx_r_3right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, and r_1, r_2, and r_3 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Rand2","page":"Public API","title":"GlobalOptimization.Rand2","text":"Rand2\n\nThe DE/rand/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + Fleft(mathbfx_r_2 - mathbfx_r_3right) + Fleft(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Best1","page":"Public API","title":"GlobalOptimization.Best1","text":"Best1\n\nThe DE/best/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_b + Fleft(mathbfx_r_1 - mathbfx_r_2right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, subscript b denotes the best candidate (in terms of the objective/fitness function), and r_1 and r_2 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Best2","page":"Public API","title":"GlobalOptimization.Best2","text":"Best2\n\nThe DE/best/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_b + Fleft(mathbfx_r_1 - mathbfx_r_2right) + Fleft(mathbfx_r_3 - mathbfx_r_4right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, subscript b denotes the best candidate, and r_1, r_2, r_3, and r_4 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToBest1","page":"Public API","title":"GlobalOptimization.CurrentToBest1","text":"CurrentToBest1\n\nThe DE/current-to-best/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_1 - mathbfx_r_2right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1 and r_2 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToBest2","page":"Public API","title":"GlobalOptimization.CurrentToBest2","text":"CurrentToBest2\n\nThe DE/current-to-best/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_1 - mathbfx_r_2right) + Fleft(mathbfx_r_3 - mathbfx_r_4right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1, r_2, r_3, and r_4 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToRand1","page":"Public API","title":"GlobalOptimization.CurrentToRand1","text":"CurrentToRand1\n\nThe DE/current-to-rand/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_r_1 - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, and r_1, r_2, and r_3 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToRand2","page":"Public API","title":"GlobalOptimization.CurrentToRand2","text":"CurrentToRand2\n\nThe DE/current-to-rand/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_r_1 - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right) + Fleft(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RandToBest1","page":"Public API","title":"GlobalOptimization.RandToBest1","text":"RandToBest1\n\nThe DE/rand-to-best/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1, r_2, and r_3 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RandToBest2","page":"Public API","title":"GlobalOptimization.RandToBest2","text":"RandToBest2\n\nThe DE/rand-to-best/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right) + Fleft(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Unified","page":"Public API","title":"GlobalOptimization.Unified","text":"Unified\n\nThe unified DE mutation strategy proposed by Ji Qiang and Chad Mitchell in \"A Unified Differential Evolution Algorithm for Global Optimization,\" 2014, https://www.osti.gov/servlets/purl/1163659.\n\nThis mutation strategy is given by:\n\nmathbfv_i = mathbfx_i + F_1left(mathbfx_b - mathbfx_iright) + F_2left(mathbfx_r_1 - mathbfx_iright) + F_3left(mathbfx_r_2 - mathbfx_r_3right) + F_4left(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_1, F_2, F_3, and F_4 are scaling factors, subscript b denotes the best candidate, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\nNote that in the underlying implementation, all mutation strategies are implemented with this formulation, where each unique strategy has a different set of F_i  i in 1234 that are set to 0.0.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SimpleSelector","page":"Public API","title":"GlobalOptimization.SimpleSelector","text":"SimpleSelector\n\nA selector that simply selects all candidates in the population.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RadiusLimitedSelector","page":"Public API","title":"GlobalOptimization.RadiusLimitedSelector","text":"RadiusLimitedSelector\n\nA selector that selects candidates within a given radius of the target candidate.\n\nFor example, for population size of 10 and a radius of 2, the following will be selected for the given target indices:\n\ntarget = 5 will select [3, 4, 5, 6, 7]\n\ntarget = 1 will select [9, 10, 1, 2, 3]\n\ntarget = 9 will select [7, 8, 9, 10, 1]\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RandomSubsetSelector","page":"Public API","title":"GlobalOptimization.RandomSubsetSelector","text":"RandomSubsetSelector\n\nA selector that selects a random subset of candidates from the population. The size of the subset is determined by the size parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#DE-Crossover-Strategies","page":"Public API","title":"DE Crossover Strategies","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.BinomialCrossoverParameters","page":"Public API","title":"GlobalOptimization.BinomialCrossoverParameters","text":"BinomialCrossoverParameters{\n    AS<:AbstractAdaptationStrategy,\n    T<:AbstractCrossoverTransformation,\n    D,\n}\n\nThe parameters for a DE binomial crossover strategy.\n\nFields\n\nCR::Float64: The crossover rate.\ntransform::T: The transformation to apply to the candidate and mutant.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the crossover   rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.BinomialCrossoverParameters-Tuple{Float64}","page":"Public API","title":"GlobalOptimization.BinomialCrossoverParameters","text":"BinomialCrossoverParameters(CR::Float64; transform=NoTransformation())\n\nCreates a BinomialCrossoverParameters object with a fixed crossover rate CR and optional transformation transform.\n\nArguments\n\nCR::Float64: The crossover rate.\n\nKeyword Arguments\n\ntransform::AbstractCrossoverTransformation: The transformation to apply to the   candidate and mutant. Defaults to NoTransformation().\n\nReturns\n\nBinomialCrossoverParameters{NoAdaptation,typeof(transform),Nothing}: A   BinomialCrossoverParameters object with a fixed crossover rate and the optionally   specified transformation.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters(0.5)\nBinomialCrossoverParameters{GlobalOptimization.NoAdaptation, GlobalOptimization.NoTransformation, Nothing}(0.5, GlobalOptimization.NoTransformation(), nothing)\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters(0.5, transform=CovarianceTransformation(0.5, 0.5, 10))\nBinomialCrossoverParameters{GlobalOptimization.NoAdaptation, CovarianceTransformation, Nothing}(0.5, CovarianceTransformation(0.5, 0.5, [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), nothing)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.BinomialCrossoverParameters-Tuple{}","page":"Public API","title":"GlobalOptimization.BinomialCrossoverParameters","text":"BinomialCrossoverParameters(; dist=default_binomial_crossover_dist, transform=NoTransformation())\n\nCreates a BinomialCrossoverParameters object with an adaptive crossover rate and optional transformation transform.\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   crossover rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   default_binomial_crossover_dist, which is a mixture model comprised of two Cauchy   distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2)\nwhere mu = 01 095 and sigma = 01 01.\ntransform::AbstractCrossoverTransformation: The transformation to apply to the   candidate and mutant. Defaults to NoTransformation().\n\nReturns\n\nBinomialCrossoverParameters{RandomAdaptation,typeof(transform),typeof(dist)}: A   BinomialCrossoverParameters object with an adaptive crossover rate and the   optionally specified transformation.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters()\nBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, GlobalOptimization.NoTransformation, Distributions.MixtureModel{Distributions.Univariate, Distributions.Continuous, Distributions.Cauchy, Distributions.Categorical{Float64, Vector{Float64}}}}(0.0, GlobalOptimization.NoTransformation(), MixtureModel{Distributions.Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Distributions.Cauchy{Float64}(μ=0.1, σ=0.1)\ncomponents[2] (prior = 0.5000): Distributions.Cauchy{Float64}(μ=0.95, σ=0.1)\n)\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters(transform=CovarianceTransformation(0.5, 0.5, 10))\nBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, CovarianceTransformation, Distributions.MixtureModel{Distributions.Univariate, Distributions.Continuous, Distributions.Cauchy, Distributions.Categorical{Float64, Vector{Float64}}}}(0.0, CovarianceTransformation(0.5, 0.5, [2.195780764e-314 2.2117846174e-314 … 2.1293782266e-314 1.5617889024864e-311; 2.366805627e-314 2.316670011e-314 … 2.3355803934e-314 1.4259811738567e-311; … ; 2.195781025e-314 2.195781096e-314 … 1.4531427176862e-310 1.27319747493e-313; 2.366805627e-314 2.366805627e-314 … 1.0270459628367e-310 2.121995795e-314], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), MixtureModel{Distributions.Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Distributions.Cauchy{Float64}(μ=0.1, σ=0.1)\ncomponents[2] (prior = 0.5000): Distributions.Cauchy{Float64}(μ=0.95, σ=0.1)\n)\n\njulia> using GlobalOptimization\njulia> using Distributions\njulia> params = BinomialCrossoverParameters(dist=Uniform(0.0, 1.0))\nBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, GlobalOptimization.NoTransformation, Uniform{Float64}}(0.0, GlobalOptimization.NoTransformation(), Uniform{Float64}(a=0.0, b=1.0))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.SelfBinomialCrossoverParameters","page":"Public API","title":"GlobalOptimization.SelfBinomialCrossoverParameters","text":"SelfBinomialCrossoverParameters{\n    AS<:AbstractAdaptationStrategy,\n    T<:AbstractCrossoverTransformation,\n    D,\n}\n\nThe parameters for a DE self-adaptive binomial crossover strategy.\n\nFields\n\nCRs::Vector{Float64}: The crossover rates for each candidate in the population.\ntransform::T: The transformation to apply to the candidate and mutant.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the crossover   rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SelfBinomialCrossoverParameters-Tuple{}","page":"Public API","title":"GlobalOptimization.SelfBinomialCrossoverParameters","text":"SelfBinomialCrossoverParameters(;\n    dist=default_binomial_crossover_dist,\n    transform=NoTransformation()\n)\n\nCreates a SelfBinomialCrossoverParameters object with an adaptive crossover rate for each candidate in the population and an optional transformation transform.\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   crossover rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   default_binomial_crossover_dist, which is a mixture model comprised of two Cauchy   distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2)\nwhere mu = 01 095 and sigma = 01 01.\ntransform::AbstractCrossoverTransformation: The transformation to apply to the   candidate and mutant. Defaults to NoTransformation().\n\nReturns\n\nSelfBinomialCrossoverParameters{RandomAdaptation,typeof(transform),typeof(dist)}: A   SelfBinomialCrossoverParameters object with an adaptive crossover rate for each   candidate and the optionally specified transformation.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = SelfBinomialCrossoverParameters()\nSelfBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, GlobalOptimization.NoTransformation, MixtureModel{Univariate, Continuous, Cauchy, Categorical{Float64, Vector{Float64}}}}(Float64[], GlobalOptimization.NoTransformation(), MixtureModel{Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Cauchy{Float64}(μ=0.1, σ=0.1)\ncomponents[2] (prior = 0.5000): Cauchy{Float64}(μ=0.95, σ=0.1)\n)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.CovarianceTransformation","page":"Public API","title":"GlobalOptimization.CovarianceTransformation","text":"CovarianceTransformation{T<:AbstractCrossoverTransformation}\n\nA transformation for performing crossover in the eigen-space of the covariance matrix of the best candidates in the population.\n\nThis is an implementation of the method proposed by Wang and Li in \"Differential Evolution Based on Covariance Matrix Learning and Bimodal Distribution Parameter Setting, \" 2014, DOI: 10.1016/j.asoc.2014.01.038.\n\nFields\n\nps::Float64: The proportion of candidates to consider in the covariance matrix. That is,   for a population size of N, the covariance matrix is calculated using the   clamp(ceil(ps * N), 2, N) best candidates.\npb::Float64: The probability of applying the transformation.\nB::Matrix{Float64}: The real part of the eigenvectors of the covariance matrix.\nct::Vector{Float64}: The transformed candidate.\nmt::Vector{Float64}: The transformed mutant.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CovarianceTransformation-Tuple{Any, Any, Any}","page":"Public API","title":"GlobalOptimization.CovarianceTransformation","text":"CovarianceTransformation(ps::Float64, pb::Float64, num_dims::Int)\n\nCreates a CovarianceTransformation object with the specified proportion of candidates to consider in the covariance matrix ps, the probability of applying the transformation pb, and the number of dimensions num_dims.\n\nThis is an implementation of the method proposed by Wang and Li in \"Differential Evolution Based on Covariance Matrix Learning and Bimodal Distribution Parameter Setting, \" 2014, DOI: 10.1016/j.asoc.2014.01.038.\n\nArguments\n\nps::Float64: The proportion of candidates to consider in the covariance matrix.\npb::Float64: The probability of applying the transformation.\nnum_dims::Int: The number of dimensions in the search space.\n\nReturns\n\nCovarianceTransformation: A CovarianceTransformation object with the specified   parameters.\n\nExamples\n\njulia> using GlobalOptimization\njulia> transformation = CovarianceTransformation(0.5, 0.5, 10)\nCovarianceTransformation(0.5, 0.5, [2.3352254645e-314 6.3877104275e-314 … 1.0e-323 5.0e-324; 6.3877051114e-314 6.3877104196e-314 … 6.3877054276e-314 6.387705455e-314; … ; 2.3352254645e-314 2.333217732e-314 … 0.0 6.3877095184e-314; 6.387705143e-314 2.130067282e-314 … 6.387705459e-314 6.387705463e-314], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.UncorrelatedCovarianceTransformation","page":"Public API","title":"GlobalOptimization.UncorrelatedCovarianceTransformation","text":"UncorrelatedCovarianceTransformation{T<:AbstractCrossoverTransformation}\n\nA transformation for performing crossover in the eigen-space of the covariance matrix of the best candidates in the population which are also not too closely correlated.\n\nThis is an implementation of the method proposed by Wang and Li in \"Differential Evolution Based on Covariance Matrix Learning and Bimodal Distribution Parameter Setting, \" 2014, DOI: 10.1016/j.asoc.2014.01.038.\n\nCorrelation alteration based on \"Covariance Matrix Learning Differential Evolution Algorithm Based on Correlation\" DOI: https://doi.org/10.4236/ijis.2021.111002\n\nFields\n\nps::Float64: The proportion of candidates to consider in the covariance matrix. That is,   for a population size of N with M candidates remaining after the correlation check, the covariance matrix is calculated using the   clamp(ceil(ps * M), 2, M) best candidates.\npb::Float64: The probability of applying the transformation.\na::Float64: The correlation threshold for which two candidates are considered 'too close' to both be used in the covariance matrix construction.\nB::Matrix{Float64}: The real part of the eigenvectors of the covariance matrix.\nct::Vector{Float64}: The transformed candidate.\nmt::Vector{Float64}: The transformed mutant.\nidxs::Vector{UInt16}: A vector of indexes for the population\ncidxs::Vector{UInt16}: A vector of unique correlated indexes for the population set for removal\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.UncorrelatedCovarianceTransformation-Tuple{Any, Any, Any}","page":"Public API","title":"GlobalOptimization.UncorrelatedCovarianceTransformation","text":"UncorrelatedCovarianceTransformation{T<:AbstractCrossoverTransformation}\n\nA transformation for performing crossover in the eigen-space of the covariance matrix of the best candidates in the population which are also not too closely correlated.\n\nThis is an implementation of the method proposed by Yuan and Feng in \"Covariance Matrix Learning Differential Evolution Algorithm Based on Correlation\" DOI: https://doi.org/10.4236/ijis.2021.111002\n\nArguments\n\npb::Float64: The probability of applying the transformation.\na::Float64: The correlation threshold for two candidates being 'too close'.\nnum_dims::Int: The number of dimensions in the search space.\n\nKeyword Arguments:\n\nps::Float64: The proportion of candidates to consider in the covariance matrix.   Defaults to 1.0 (i.e., all uncorrelated candidates are considered)\n\nReturns\n\nUncorrelatedCovarianceTransformation: A UncorrelatedCovarianceTransformation object with the specified   parameters.\n\nExamples\n\njulia> using GlobalOptimization\njulia> transformation = UncorrelatedCovarianceTransformation(0.5, .95, 10; ps = 1.0)\nUncorrelatedCovarianceTransformation(1.0, 0.5, 0.95, [1.0630691323565e-311 1.0630691151907e-311 … 1.063069230316e-311 1.063069119645e-311; 1.0630691158705e-311 1.063069115333e-311 … 1.063069172704e-311 1.0630692904614e-311; … ; 1.063069115428e-311 1.063069114246e-311 … 1.063069124886e-311 1.0630694190924e-311; 1.0630691153804e-311 1.0630691141986e-311 … 1.0630691348624e-311 1.063069428614e-311], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], UInt16[], UInt16[])\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Monotonic-Basin-Hopping","page":"Public API","title":"Monotonic Basin Hopping","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MBH","page":"Public API","title":"GlobalOptimization.MBH","text":"MBH\n\nMonotonic Basin Hopping (MBH) algorithm.\n\nThis implementation employs a single candidate rather than a population.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MBH-Union{Tuple{GlobalOptimization.AbstractProblem{has_penalty, ContinuousRectangularSearchSpace{T}}}, Tuple{has_penalty}, Tuple{T}} where {T<:Number, has_penalty}","page":"Public API","title":"GlobalOptimization.MBH","text":"MBH(prob::AbstractOptimizationProblem{SS}; kwargs...)\n\nConstruct the standard Monotonic Basin Hopping (MBJ) algorithm with the specified options.\n\nKeyword Arguments\n\nhopper_type::AbstractHopperType: The type of hopper to use. Default is   SingleHopper().\nhop_distribution::AbstractMBHDistribution{T}: The distribution from which hops are   drawn. Default is MBHAdaptiveDistribution{T}(100, 5).\nlocal_search::AbstractLocalSearch{T}: The local search algorithm to use. Default is   LBFGSLocalSearch{T}().\ninitial_space::Union{Nothing,ContinuousRectangularSearchSpace}=nothing: The initial bounds for the search space.\nmax_iterations::Integer=1000: The maximum number of iterations.\nfunction_tolerance::Real=1e-6: The function tolerance (stall-based stopping criteria).\nmax_stall_time::Real=60.0: The maximum stall time (in seconds).\nmax_stall_iterations::Integer=100: The maximum number of stall iterations.\nmax_time::Real=60.0: The maximum time (in seconds) to allow for optimization.\nmin_cost::Real=(-Inf): The minimum cost to allow for optimization.\nfunction_value_check::Union{Val{false},Val{true}}=Val(true): Whether to check the function value   for bad values (i.e., Inf or NaN).\nshow_trace::Union{Val{false},Val{true}}=Val(false): Whether to show the trace.\nsave_trace::Union{Val{false},Val{true}}=Val(false): Whether to save the trace.\nsave_file::String=\"trace.txt\": The file to save the trace to.\ntrace_level::TraceLevel=TraceMinimal(1): The trace level to use.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Hopper-Types","page":"Public API","title":"Hopper Types","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MCH","page":"Public API","title":"GlobalOptimization.MCH","text":"MCH{EM<:AbstractFunctionEvaluationMethod} <: GlobalOptimization.AbstractHopperType\n\nEmploys the method of Multiple Communicating Hoppers (MCH) to explore the search space as described in Englander, Arnold C., \"Speeding-Up a Random Search for the Global Minimum of a Non-Convex, Non-Smooth Objective Function\" (2021). Doctoral Dissertations. 2569. https://scholars.unh.edu/dissertation/2569.\n\nThe struct fields are the same as the constructor arguments.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MCH-Union{Tuple{}, Tuple{EM}} where EM<:GlobalOptimization.AbstractFunctionEvaluationMethod","page":"Public API","title":"GlobalOptimization.MCH","text":"MCH(;\n    num_hoppers::Integer=4,\n    eval_method<:AbstractFunctionEvaluationMethod=SerialFunctionEvaluation(),\n)\n\nConstructs a new MCH object with the specified number of hoppers and evaluation method.\n\nKeyword Arguments\n\nnum_hoppers::Integer: The number of hoppers to use. Default is 4.\neval_method<:AbstractFunctionEvaluationMethod: The evaluation method to use. Default   is SerialFunctionEvaluation().\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.SingleHopper","page":"Public API","title":"GlobalOptimization.SingleHopper","text":"SingleHopper <: GlobalOptimization.AbstractHopperType\n\nA single hopper that is used to explore the search space. Note that no parallelism is employed.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#Hop-Distributions","page":"Public API","title":"Hop Distributions","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MBHAdaptiveDistribution","page":"Public API","title":"GlobalOptimization.MBHAdaptiveDistribution","text":"MBHAdaptiveDistribution{T}\n\nAn adaptive distribution for MBH. In this implementation, each element of a hop is drawn from a univariate adaptive mixture model comprised of two Laplace distributions as defined in Englander, Arnold C., \"Speeding-Up a Random Search for the Global Minimum of a Non-Convex, Non-Smooth Objective Function\" (2021). Doctoral Dissertations. 2569. https://scholars.unh.edu/dissertation/2569.\n\nThe univariate mixture model for the i-th element of a hop has a PDF given by:\n\nf_textmixi(x b c hatlambda_i) = k_ileft(1 - b) f(xmu = 0theta = c*hatlambda_i) + b f(xmu = 0 theta = 1)right\n\nwhere mu denotes the location parameter and theta the scale parameter of a Laplace distribution (i.e., with probability density f(xmutheta)). Note that k_i denotes half of the length of the search space in the i-th dimension.\n\nThe scale parameter hatlambda_i is adaptively updated after each successful hop with a low-delay estimate given by:\n\nhatlambda_i = (1 - a) Psi_i + a hatlambda_i\n\nNote that hatlambda_i is held constant at λhat0 until min_memory_update successful steps have been made. Englander (2021) proposed taking Psi_i to be the standard deviation of the i-th element of the last step_memory successful hops. Alternatively, the mean absolute deviation (MAD) around the median of the last step_memory steps can be used. Note that the MAD median is the maximum likelihood estimator for a Laplace distribution's shape parameter. In this implementation, setting use_mad = true will take Psi_i to be the MAD median, otherwise, the standard deviation is used.\n\nFields\n\nstep_memory::MBHStepMemoory{T}: The step memory for the distribution\nmin_memory_update::Int: The minimum number of steps in memory before updating the scale parameter\na: A parameter that defines the influence of a new successful step in the adaptation of   the distribution.\nb::T: The mixing parameter for the two Laplace distributions\nc::T: The scale parameter for the first Laplace distribution\nλhat::Vector{T}: The estimated scale parameter of the first Laplace distribution\nλhat0::T: The initial value of the scale parameter\nuse_mad::Bool: Flag to indicate if we will use the STD (proposed by Englander) or MAD median (MVE) to   update the estimated scale parameter\ndim_delta::Vector{T}: The length of the search space in each dimension\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MBHAdaptiveDistribution-Union{Tuple{Int64, Int64}, Tuple{T}} where T","page":"Public API","title":"GlobalOptimization.MBHAdaptiveDistribution","text":"MBHAdaptiveDistribution{T}(\n    memory_len::Int, min_memory_update::Int;\n    a=0.93,\n    b=0.05,\n    c=1.0,\n    λhat0=1.0,\n) where T\n\nCreates a new MBHAdaptiveDistribution with the given parameters.\n\nArguments\n\nmemory_len::Int: The length of the memory for the distribution adaptation.\nmin_memory_update::Int: The minimum number of steps in memory before updating the scale parameter.\n\nKeyword Arguments\n\na: A parameter that defines the influence of a new successful step in the adaptation of   the distribution.\nb: The mixing parameter for the two Laplace distributions\nc: The scale parameter for the first Laplace distribution\nλhat0: The initial value of the scale parameter\nuse_mad::Bool: Flag to indicate which metric to use for estimating the scale parameter.   If true, the MAD median is used, which is the maximum likelihood estimator for a   Laplace distribution's shape parameter. If false, the standard deviation is used   as proposed by Englander (2021).\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.MBHStaticDistribution","page":"Public API","title":"GlobalOptimization.MBHStaticDistribution","text":"MBHStaticDistribution{T}\n\nA static distribution for MBH. In this implementation, each element of a hop is drawn from a mixture model comprised of two Laplace distributions with PDF given by:\n\nf_mix(x b lambda) = k_ileft(1 - b) f(xmu = 0theta = lambda) + b f(xmu = 0 theta = 1)right\n\nwhere mu denotes the location parameter and theta the scale parameter of a Laplace distribution (i.e., with probability density f(xmutheta)). Additionally, k_i is half of the length of the search space in the i-th dimension.\n\nFields\n\nb::T: The mixing parameter for the two Laplace distributions\nλ::T: The scale parameter for the first Laplace distribution in the mixture model\ndim_delta::Vector{T}: The length of the search space in each dimension\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MBHStaticDistribution-Union{Tuple{}, Tuple{T}} where T","page":"Public API","title":"GlobalOptimization.MBHStaticDistribution","text":"MBHStaticDistribution{T}(; b=0.05, λ=0.7) where {T}\n\nCreates a new MBHStaticDistribution with the given parameters.\n\nKeyword Arguments\n\nb: The mixing parameter for the two Laplace distributions\nλ: The scale parameter for the first Laplace distribution in the mixture model\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Local-Search-Methods","page":"Public API","title":"Local Search Methods","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.LocalStochasticSearch","page":"Public API","title":"GlobalOptimization.LocalStochasticSearch","text":"LocalStochasticSearch{T}\n\nA local search algorithm that uses a stochastic approach to locally improve the candidate solution.\n\nNote that this local search algorithm is able to guarantee satisfaction of both the box constraints and the nonlinear inequality constraint (if any).\n\nFields\n\nb::T: The local step standard deviation.\niters::Int: The number of iterations to perform.\nstep::Vector{T}: The candidate step and candidate storage.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.LocalStochasticSearch-Union{Tuple{T}, Tuple{Real, Int64}} where T<:AbstractFloat","page":"Public API","title":"GlobalOptimization.LocalStochasticSearch","text":"LocalStochasticSearch{T}(b::Real, iters::Int) where {T<:AbstractFloat}\n\nCreate a new LocalStochasticSearch object with the given step size and number of iterations.\n\nArguments\n\nb::Real: The local step standard deviation.\niters::Int: The number of iterations to perform.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.UserLocalSearch","page":"Public API","title":"GlobalOptimization.UserLocalSearch","text":"UserLocalSearch{T,F<:Function}\n\nA user provided local search algorithm to locally improve the candidate solution.\n\n# Fields\n- `user_search_fun!::F`: The user provided function. This must accept a Hopper{T} as the\n    single argument, mutating the Hopper{T} after performing the local search.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.UserLocalSearch-Union{Tuple{F}, Tuple{T}} where {T<:AbstractFloat, F<:Function}","page":"Public API","title":"GlobalOptimization.UserLocalSearch","text":"UserLocalSearch{T}(user_search_fun!::F) where {T<:AbstractFloat,F<:Function}\n\nCreate a new UserLocalSearch object with the provided user_search_fun!.\n\nArguments\n\nuser_search_fun::F: The user provided function. This must accept a Hopper{T} as the   single argument, mutating the Hopper{T} after performing the local search.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Trace-Options","page":"Public API","title":"Trace Options","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Each algorithm provides the ability trace solve information to the terminal  or a specified file through setting the keyword arguments show_trace = Val(true) and save_trace = Val(true), respectively. Additionally, the amount of information provided in the trace can be controlled by setting the trace_level keyword argument with one of the following TraceLevel constructors:","category":"page"},{"location":"lib/public/#GlobalOptimization.TraceMinimal-Tuple{}","page":"Public API","title":"GlobalOptimization.TraceMinimal","text":"TraceMinimal(freq)\nTraceMinimal(; print_frequency = 1, save_frequency = 1)\n\nTrace Minimal information about the optimization process.\n\nFor example, this will set PSO or DE to print the elapsed time, iteration number, stall iterations, and global best fitness.\n\nReturns\n\nTraceLevel{Val{:minimal}}: A trace level object with the minimal trace level.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.TraceDetailed-Tuple{}","page":"Public API","title":"GlobalOptimization.TraceDetailed","text":"TraceDetailed(freq)\nTraceDetailed(; print_frequency = 1, save_frequency = 1)\n\nTrace Detailed information about the optimization process (including the information in the minimal trace).\n\nReturns\n\nTraceLevel{Val{:detailed}}: A trace level object with the detailed trace level.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.TraceAll-Tuple{}","page":"Public API","title":"GlobalOptimization.TraceAll","text":"TraceAll(freq)\nTraceAll(; print_frequency = 1, save_frequency = 1)\n\nTrace All information about the optimization process (including the information in the detailed trace). This trace option should likely only be used for debugging purposes.\n\nReturns\n\nTraceLevel{Val{:all}}: A trace level object with the all trace level.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Index","page":"Public API","title":"Index","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Pages = [\"public.md\"]","category":"page"},{"location":"#GlobalOptimization","page":"Home","title":"GlobalOptimization","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Currently, GlobalOptimization provides Particle Swarm Optimization (PSO) and several variants of Differential Evolution (DE) as the only global optimization algorithms supported. Monotonic Basin Hopping (MBH) is in the works.","category":"page"},{"location":"#Simple-PSO-Example","page":"Home","title":"Simple PSO Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Let's use PSO to find the minimum to the non-convex Ackley function given by","category":"page"},{"location":"","page":"Home","title":"Home","text":"J(mathbfx) = -a expleft(-bsqrtfrac1dsum_i=1^d x_i^2right) - expleft(frac1dsum_i=1^d cos (cx_i)right) + a + exp(1)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where a = 20, b = 02, c = 2pi, and d is the length of the decision vector mathbfx, subject to the constraint that -32768 leq x_i leq 32768 hspace1mm forall hspace1mm x_i hspace1mm in hspace1mm mathbfx.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To begin, we'll first define an Ackley function in Julia as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function ackley(x)\n    a = 20\n    b = 0.2\n    c = 2*π\n    d = length(x)\n\n    sum1 = 0.0\n    sum2 = 0.0\n    for val in x\n        sum1 += val^2\n        sum2 += cos(c*val)\n    end\n    return -a*exp(-b*sqrt(sum1/d)) - exp(sum2/d) + a + exp(1)\nend\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Next, we'll define the OptimizationProblem by providing its constructor our new ackley function and bounds that define the search space. Then, we'll instantiate a StaticPSO (an implementation of the PSO algorithm that does not use parallel computing to evaluate the cost function) to perform the optimization!","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GlobalOptimization\n\nN   = 10 # The number of decision variables\nLB  = [-32.768 for _ in 1:10] # The lower bounds\nUB  = [ 32.768 for _ in 1:10] # The upper bounds\n\n# Construct the optimization problem\nop  = OptimizationProblem(ackley, LB, UB)\n\n# Instantiate PSO instance\npso = PSO(op)\n\n# Perform optimization with pso\nres = optimize!(pso)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, we can get the final optimal decision vector with","category":"page"},{"location":"","page":"Home","title":"Home","text":"best_candidate = res.xbest","category":"page"},{"location":"","page":"Home","title":"Home","text":"and the fitness of the final optimal decision vector with","category":"page"},{"location":"","page":"Home","title":"Home","text":"best_candidate_fitness = res.fbest","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
