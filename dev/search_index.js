var documenterSearchIndex = {"docs":
[{"location":"dev/contributing/#Developing-Documentation","page":"Contributing","title":"Developing Documentation","text":"","category":"section"},{"location":"dev/contributing/","page":"Contributing","title":"Contributing","text":"When developing documentation locally, it is suggested to use servedocs() provided by LiveServer.jl to tests the documentation  build process while viewing updates to them dynamically as they are made. This can be  done by running the following command in your terminal while at the base level of your local instance of GlobalOptimization.jl:","category":"page"},{"location":"dev/contributing/","page":"Contributing","title":"Contributing","text":"julia --project=docs -ie 'using GlobalOptimization, LiveServer; servedocs(include_dirs=[\"src/\"])'","category":"page"},{"location":"dev/contributing/","page":"Contributing","title":"Contributing","text":"If the documentation build is successful, this will print a link to a spawned local server that you can open in any browser.","category":"page"},{"location":"algs/pso/","page":"-","title":"-","text":"To be continued...","category":"page"},{"location":"lib/internal/temp/#Internals","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"lib/internal/temp/","page":"Internals","title":"Internals","text":"Listing all non-exported types and functions here for now, but split off categories onto separate pages in the future!","category":"page"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractAlgorithmSpecificOptions","page":"Internals","title":"GlobalOptimization.AbstractAlgorithmSpecificOptions","text":"AbstractAlgorithmSpecificOptions\n\nAbstract type for algorithm specific options\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractCandidate","page":"Internals","title":"GlobalOptimization.AbstractCandidate","text":"AbstractCandidate\n\nAbstract type for a candidate\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractEvaluator","page":"Internals","title":"GlobalOptimization.AbstractEvaluator","text":"AbstractEvaluator\n\nAbstract type for an evaluator. An evaluator is responsible for evaluating the fitness of a population or candidate.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractFunctionEvaluationMethod","page":"Internals","title":"GlobalOptimization.AbstractFunctionEvaluationMethod","text":"AbstractFunctionEvaluationMethod\n\nA function evaluation method is a strategy for evaluating the fitness/objective, as well as     possibly other algorithm specific things.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractMBHDistribution","page":"Internals","title":"GlobalOptimization.AbstractMBHDistribution","text":"AbstractMBHDistribution{T}\n\nAbstract type for MBH distributions.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractNonlinearEquationProblem","page":"Internals","title":"GlobalOptimization.AbstractNonlinearEquationProblem","text":"AbstractNonlinearEquationProblem\n\nAbstract type for problems involving a set of nonlinear equations to solve.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractOptimizationProblem","page":"Internals","title":"GlobalOptimization.AbstractOptimizationProblem","text":"AbstractOptimizationProblem\n\nAbstract type for optimization problems.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractOptimizer","page":"Internals","title":"GlobalOptimization.AbstractOptimizer","text":"AbstractOptimizer\n\nAbstract type of all optimization algorithms.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractOptions","page":"Internals","title":"GlobalOptimization.AbstractOptions","text":"AbstractOptions\n\nAbstract type for multiple options\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractPopulation","page":"Internals","title":"GlobalOptimization.AbstractPopulation","text":"AbstractPopulation\n\nAbstract type for a population of candidates.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractProblem","page":"Internals","title":"GlobalOptimization.AbstractProblem","text":"AbstractProblem\n\nAbstract type for all solvable problems.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractRandomNeighborhoodVelocityUpdateScheme","page":"Internals","title":"GlobalOptimization.AbstractRandomNeighborhoodVelocityUpdateScheme","text":"AbstractRandomNeighborhoodVelocityUpdateScheme\n\nAbstract type for a velocity update scheme that randomly selects the particles in the neighborhood of a given particle.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AbstractVelocityUpdateScheme","page":"Internals","title":"GlobalOptimization.AbstractVelocityUpdateScheme","text":"AbstractVelocityUpdateScheme\n\nAbstract type for velocity update schemes in Particle Swarm Optimization (PSO).\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.AsyncEvaluator","page":"Internals","title":"GlobalOptimization.AsyncEvaluator","text":"AsyncEvaluator\n\nAbstract type for an evaluator that evaluates the fitness of a single candidate asyncronously.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.BatchEvaluator","page":"Internals","title":"GlobalOptimization.BatchEvaluator","text":"BatchEvaluator\n\nAbstract type for an evaluator that evaluates the fitness of an entire population.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.BatchJobEvaluator","page":"Internals","title":"GlobalOptimization.BatchJobEvaluator","text":"BatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEBasePopulation","page":"Internals","title":"GlobalOptimization.DEBasePopulation","text":"DEBasePopulation{T <: AbstractFloat} <: AbstractPopulation{T}\n\nThe base representation of some population for the DE algorithms.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DECache","page":"Internals","title":"GlobalOptimization.DECache","text":"DECache\n\nCache for the DE algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEOptions","page":"Internals","title":"GlobalOptimization.DEOptions","text":"DEOptions\n\nOptions for the Differential Evolution (DE) algorithms.\n\nFields:\n\ngeneral<:GeneralOptions: The general options.\npop_init_method<:AbstractPopulationInitialization: The population initialization method.\nmutation_params<:AbstractMutationParameters: The mutation strategy parameters.\ncrossover_params<:AbstractCrossoverParameters: The crossover strategy parameters.\ninitial_space<:Union{Nothing,ContinuousRectangularSearchSpace}: The initial space to initialize the population.\nmax_iterations::Int: The maximum number of iterations.\nfunction_tolerance::Float64: The function tolerance for the stall condition.\nmax_stall_time::Float64: The maximum stall time for the stall condition.\nmax_stall_iterations::Int: The maximum number of stall iterations for the stall condition.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEPopulation","page":"Internals","title":"GlobalOptimization.DEPopulation","text":"DEPopulation{T <: AbstractFloat} <: AbstractPopulation{T}\n\nThe full population representation for the DE algorithms, including both the candidates and the mutants.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.DEPopulation-Tuple{Integer, Integer}","page":"Internals","title":"GlobalOptimization.DEPopulation","text":"DEPopulation(num_candidates::Integer, num_dims::Integer)\n\nConstructs a DEPopulation with num_candidates candidates in num_dims dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.FeasibilityHandlingEvaluator","page":"Internals","title":"GlobalOptimization.FeasibilityHandlingEvaluator","text":"FeasibilityHandlingEvaluator\n\nAn evaluator that handled a functions returned infeasibility penalty\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.FixedDimensionSearchSpace","page":"Internals","title":"GlobalOptimization.FixedDimensionSearchSpace","text":"FixedDimensionSearchSpace\n\nThe base abstract type for a search space with a fixed finite number of dimensions. Applicable to the vast majority of optimization problems.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.GeneralOptions","page":"Internals","title":"GlobalOptimization.GeneralOptions","text":"GeneralOptions{display, function_value_check}\n\nGeneral options for all optimizers\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.MBHOptions","page":"Internals","title":"GlobalOptimization.MBHOptions","text":"MBHOptions <: AbstractAlgorithmSpecificOptions\n\nOptions for the Monotonic Basin Hopping (MBH) algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.MBHStepMemory","page":"Internals","title":"GlobalOptimization.MBHStepMemory","text":"MBHStepMemory{T}\n\nMemory about MBH accepted steps.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.PSOCache","page":"Internals","title":"GlobalOptimization.PSOCache","text":"PSOCache\n\nCache for PSO algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.PSOOptions","page":"Internals","title":"GlobalOptimization.PSOOptions","text":"PSOOptions <: AbstractAlgorithmSpecificOptions\n\nOptions for the PSO algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.PolyesterBatchEvaluator","page":"Internals","title":"GlobalOptimization.PolyesterBatchEvaluator","text":"PolyesterBatchEvaluator\n\nAn evaluator that evaluates the fitness of a population in parallel using multi-threading using Polyester.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.PolyesterBatchJobEvaluator","page":"Internals","title":"GlobalOptimization.PolyesterBatchJobEvaluator","text":"PolyesterBatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs in parallel using Polyester.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.RectangularSearchSpace","page":"Internals","title":"GlobalOptimization.RectangularSearchSpace","text":"RectangularSearchSpace\n\nA FixedDimensionSearchSpace with N dimensional rectangle as the set of feasible points.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.Results","page":"Internals","title":"GlobalOptimization.Results","text":"Results{T}\n\nA simple struct for returning results.\n\nFields\n\nfbest::T: The best function value found.\nxbest::Vector{T}: The best candidate found.\niters::Int: The number of iterations performed.\ntime::Float64: The time taken to perform the optimization in seconds.\nexitFlag::Int: The exit flag of the optimization.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.Results-Union{Tuple{T}, Tuple{T, AbstractVector{T}, Any, Any, Any}} where T","page":"Internals","title":"GlobalOptimization.Results","text":"Results(fbest::T, xbest::AbstractVector{T}, iters, time, exitFlag)\n\nConstructs a new Results struct.\n\nArguments\n\nfbest::T: The best function value found.\nxbest::AbstractVector{T}: The best candidate found.\niters::Int: The number of iterations performed.\ntime::AbstractFloat: The time taken to perform the optimization in seconds.\nexitFlag::Int: The exit flag of the optimization.\n\nReturns\n\nResults{T}\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.SearchSpace","page":"Internals","title":"GlobalOptimization.SearchSpace","text":"SearchSpace\n\nThe base abstract type for a Problem search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.SerialBatchEvaluator","page":"Internals","title":"GlobalOptimization.SerialBatchEvaluator","text":"SerialBatchEvaluator\n\nAn evaluator that evaluates the fitness of a population in serial.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.SerialBatchJobEvaluator","page":"Internals","title":"GlobalOptimization.SerialBatchJobEvaluator","text":"SerialBatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs in serial.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.SingleEvaluator","page":"Internals","title":"GlobalOptimization.SingleEvaluator","text":"SingleEvaluator\n\nAbstract type for an evaluator that evaluates the fitness of a single candidate\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.Swarm","page":"Internals","title":"GlobalOptimization.Swarm","text":"Swarm{T <: AbstractFloat} <: AbstractPopulation\n\nA population of particles for the PSO algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.ThreadedBatchEvaluator","page":"Internals","title":"GlobalOptimization.ThreadedBatchEvaluator","text":"ThreadedBatchEvaluator\n\nAn evaluator that evaluates the fitness of a population in parallel using multi-threading.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.ThreadedBatchJobEvaluator","page":"Internals","title":"GlobalOptimization.ThreadedBatchJobEvaluator","text":"ThreadedBatchJobEvaluator\n\nAn evaluator that evaluates a batch of jobs in parallel using Threads.jl and ChunkSplitters.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#GlobalOptimization.UniformInitialization","page":"Internals","title":"GlobalOptimization.UniformInitialization","text":"UniformInitialization\n\nInitializes a population from a uniform distribution in the search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/internal/temp/#Base.eachindex-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"Base.eachindex","text":"eachindex(pop::AbstractPopulation)\n\nReturns an iterator for the indices of the population.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#Base.length-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"Base.length","text":"length(pop::AbstractPopulation)\n\nReturns the number of candidates in the population.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#Base.size-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"Base.size","text":"size(pop::AbstractPopulation)\n\nReturns the size of the population.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.DEPopulation_F64-Tuple{Integer, Integer}","page":"Internals","title":"GlobalOptimization.DEPopulation_F64","text":"DEPopulation_F64(num_candidates::Integer, num_dims::Integer)\n\nConstructs a Float64 DEPopulation with num_candidate candidates in num_dims dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization._MBH-Union{Tuple{T}, Tuple{has_penalty}, Tuple{SingleHopper, GlobalOptimization.AbstractProblem{has_penalty, ContinuousRectangularSearchSpace{T}}, Any, Any, Any}} where {has_penalty, T}","page":"Internals","title":"GlobalOptimization._MBH","text":"_MBH(args...)\n\nInternal constructor for the Monotonic Basin Hopping (MBH) algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.adapt!-Tuple{MATLABVelocityUpdate, Bool, Int64}","page":"Internals","title":"GlobalOptimization.adapt!","text":"adapt!(vu::AbstractVelocityUpdateScheme, improved::Bool, stall_iteration::Int)\n\nAdapt the velocity update scheme based on the improvement status of the swarm and the stall iteration counter.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.candidate-Tuple{GlobalOptimization.AbstractCandidate}","page":"Internals","title":"GlobalOptimization.candidate","text":"candidate(c::AbstractCandidate)\n\nReturns the candidate c.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.candidates-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"GlobalOptimization.candidates","text":"candidates(pop::AbstractPopulation, [i::Integer])\n\nReturns the candidates from a population. If i is specified, returns the i-th candidate.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.check_fitness!-Union{Tuple{FVC}, Tuple{D}, Tuple{GlobalOptimization.AbstractCandidate, GlobalOptimization.GeneralOptions{D, FVC}}} where {D, FVC}","page":"Internals","title":"GlobalOptimization.check_fitness!","text":"check_fitness!(c::AbstractCandidate, options::Union{GeneralOptions,Val{true},Val{false}})\n\nChecks the fitness of the candidate c to ensure that it is valid iff options <: Union{GeneralOptions{D,Val{true}}, Val{true}}, otherwise, does nothing.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.check_fitness!-Union{Tuple{FVC}, Tuple{D}, Tuple{GlobalOptimization.AbstractHopperSet, GlobalOptimization.GeneralOptions{D, FVC}}} where {D, FVC}","page":"Internals","title":"GlobalOptimization.check_fitness!","text":"check_fitness!(c::AbstractHopperSet, options::Union{GeneralOptions,Val{true},Val{false}})\n\nChecks the fitness of the candidate c to ensure that it is valid iff options <: Union{GeneralOptions{D,Val{true}}, Val{true}}, otherwise, does nothing.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.check_fitness!-Union{Tuple{FVC}, Tuple{D}, Tuple{GlobalOptimization.AbstractPopulation, GlobalOptimization.GeneralOptions{D, FVC}}} where {D, FVC}","page":"Internals","title":"GlobalOptimization.check_fitness!","text":"check_fitness!(pop::AbstractPopulation, options::Union{GeneralOptions,Val{true},Val{false}})\n\nChecks the fitness of each candidate in the population pop to ensure that it is valid iff options <: Union{GeneralOptions{D,Val{true}}, Val{true}}, otherwise, does nothing.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.construct_batch_evaluator-Tuple{SerialFunctionEvaluation, Any}","page":"Internals","title":"GlobalOptimization.construct_batch_evaluator","text":"construct_batch_evaluator(\n    method::AbstractFunctionEvaluationMethod,\n    prob::OptimizationProblem,\n)\n\nConstructs a batch evaluator for the given method and prob.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.construct_batch_job_evaluator-Tuple{SerialFunctionEvaluation}","page":"Internals","title":"GlobalOptimization.construct_batch_job_evaluator","text":"construct_batch_job_evaluator(method::AbstractFunctionEvaluationMethod)\n\nConstructs a batch job evaluator for the given method.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.crossover!-Tuple{GlobalOptimization.DEPopulation, GlobalOptimization.AbstractBinomialCrossoverParameters, Any}","page":"Internals","title":"GlobalOptimization.crossover!","text":"crossover!(population::DEPopulation{T}, crossover_params, search_space)\n\nPerforms the crossover operation on the population population using the DE crossover strategy.\n\nThis function also ensures that, after crossover, the mutants are within the search space.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_delta-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_delta","text":"dim_delta(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the difference between the maximum and minimum values for the i-th dimension of ss. If i is not specified, returns a vector of all differences.\n\nArguments\n\nss::ContinuousRectangularSearchSpace{T}`\ni::Integer: the dimension to return the difference between the maximum and minimum values for.\n\nReturns\n\nT or Vector{T} the difference between the maximum and minimum values for the i-th dimension of ss or a vector of all differences if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_max-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_max","text":"dim_max(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the maximum value for the i-th dimension of ss. If i is not specified, returns a vector of all maximum values.\n\nArguments\n\nss::ContinuousRectangularSearchSpace{T}\ni::Integer: the dimension to return the maximum value for.\n\nReturns\n\nT or Vector{T}: the minimum value for the i-th dimension of ss or a vector of all minimum values if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_min-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_min","text":"dim_min(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the minimum value for the i-th dimension of ss. If i is not specified, returns a vector of all minimum values.\n\nArguments\n\nss::RontinuousRectangularSearchSpace{T}\ni::Integer: the dimension to return the minimum value for.\n\nReturns\n\nT or Vector{T}: the minimum value for the i-th dimension of ss or a vector of all minimum values if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.dim_range-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.dim_range","text":"dim_range(ss::ContinuousRectangularSearchSpace{T}, [i::Integer])\n\nReturns the range of values for the i-th dimension of ss. If i is not specified, returns a vector of all ranges.\n\nArguments\n\nss::ContinuousRectangularSearchSpace{T}\ni::Integer: the dimension to return the range of values for.\n\nReturns\n\nTuple{T, T} or Vector{Tuple{T, T}}: the range of values for the i-th dimension of ss or a vector of all ranges if i not provided.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.display_status-Union{Tuple{PSO{VU, T, E, IBSS, PI, GlobalOptimization.GeneralOptions{Val{false}, FVC}}}, Tuple{FVC}, Tuple{PI}, Tuple{IBSS}, Tuple{E}, Tuple{T}, Tuple{VU}} where {VU, T, E, IBSS, PI, FVC}","page":"Internals","title":"GlobalOptimization.display_status","text":"display_status(pso::PSO)\n\nDisplays the status of the PSO algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.draw_step!-Union{Tuple{T}, Tuple{AbstractVector{T}, MBHStaticDistribution{T}}} where T","page":"Internals","title":"GlobalOptimization.draw_step!","text":"draw_step!(step::AbstractVector{T}, dist::AbstractMBHDistribution{T})\n\nDraws a step from the distribution dist and stores it in step.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.draw_update!-Union{Tuple{T}, Tuple{GlobalOptimization.Hopper{T}, GlobalOptimization.AbstractMBHDistribution{T}}} where T","page":"Internals","title":"GlobalOptimization.draw_update!","text":"draw_update!(hopper::Hopper{T}, distribution::AbstractMBHDistribution{T})\n\nDraws a perterbation from distribution and updates candidate for the hopper hopper.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.enforce_bounds!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, ContinuousRectangularSearchSpace{T}}} where T","page":"Internals","title":"GlobalOptimization.enforce_bounds!","text":"enforce_bounds!(swarm::Swarm{T}, evaluator::BatchEvaluator)\n\nEnforces the bounds of the search space on each candidate in the swarm swarm. If a candidate\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate!-Tuple{Function, AbstractVector, GlobalOptimization.SerialBatchJobEvaluator}","page":"Internals","title":"GlobalOptimization.evaluate!","text":"evaluate!(job::Function, job_ids::AbstractVector{Int}, evaluator::BatchJobEvaluator)\n\nEvaluates job for each element of job_args using the given evaluator.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate!-Tuple{GlobalOptimization.AbstractPopulation, GlobalOptimization.SerialBatchEvaluator}","page":"Internals","title":"GlobalOptimization.evaluate!","text":"evaluate!(pop::AbstractPopulation, evaluator::BatchEvaluator)\n\nEvaluates the fitness of a population using the given evaluator.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate_fitness!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.BatchEvaluator}} where T","page":"Internals","title":"GlobalOptimization.evaluate_fitness!","text":"evaluate_fitness!(swarm::Swarm{T}, evaluator::BatchEvaluator)\n\nEvaluates the fitness of each candidate in the swarm swarm using the evaluator. Updates the swarms best candidates if any are found.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.evaluate_with_penalty-Tuple{GlobalOptimization.FeasibilityHandlingEvaluator, AbstractArray}","page":"Internals","title":"GlobalOptimization.evaluate_with_penalty","text":"evaluate_with_penalty(evaluator::FeasibilityHandlingEvaluator, candidate::AbstractArray)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.feasible-Union{Tuple{T}, Tuple{AbstractVector{T}, ContinuousRectangularSearchSpace{T}}} where T","page":"Internals","title":"GlobalOptimization.feasible","text":"feasible(x, ss::ContinuousRectangularSearchSpace)\n\nReturns true if the point x is feasible in the search space ss, otherwise returns false.\n\nArguments\n\nx::AbstractVector{T}: the point to check for feasibility.\nss::ContinuousRectangularSearchSpace{T}: the search space to check for feasibility in.\n\nReturns\n\nBool: true if x is in ss, otherwise false.\n\nThrows\n\nDimensionMismatch: if x does not have the same number of dimensions as ss.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.fitness-Tuple{GlobalOptimization.AbstractCandidate}","page":"Internals","title":"GlobalOptimization.fitness","text":"fitness(c::AbstractCandidate)\n\nReturns the fitness of the candidate c.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.fitness-Tuple{GlobalOptimization.AbstractPopulation}","page":"Internals","title":"GlobalOptimization.fitness","text":"fitness(pop::AbstractPopulation, [i::Integer])\n\nReturns the fitness of the candidates from a population. If i is specified, returns the i-th fitness.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_best_candidate_in_selection-Tuple{GlobalOptimization.DEPopulation, Any}","page":"Internals","title":"GlobalOptimization.get_best_candidate_in_selection","text":"get_best_candidate_in_selection(population::DEPopulation, idxs)\n\nGet the best candidate in the selected subset of population (as specified by the indices in idxs).\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_display-Union{Tuple{GlobalOptimization.GeneralOptions{Val{true}, fvc}}, Tuple{fvc}} where fvc","page":"Internals","title":"GlobalOptimization.get_display","text":"get_display(opts::AbstractOptions)\n\nReturns the display option from an options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_display_interval-Tuple{GlobalOptimization.GeneralOptions}","page":"Internals","title":"GlobalOptimization.get_display_interval","text":"get_display_interval(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the display interval from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_function_value_check-Union{Tuple{GlobalOptimization.GeneralOptions{d, Val{true}}}, Tuple{d}} where d","page":"Internals","title":"GlobalOptimization.get_function_value_check","text":"get_function_value_check(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the function value check option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_general-Tuple{GlobalOptimization.AbstractAlgorithmSpecificOptions}","page":"Internals","title":"GlobalOptimization.get_general","text":"get_general(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the general options from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_max_time-Tuple{GlobalOptimization.AbstractAlgorithmSpecificOptions}","page":"Internals","title":"GlobalOptimization.get_max_time","text":"get_max_time(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the max time option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_min_cost-Tuple{GlobalOptimization.AbstractAlgorithmSpecificOptions}","page":"Internals","title":"GlobalOptimization.get_min_cost","text":"get_min_cost(opts::AbstractAlgorithmSpecificOptions)\n\nReturns the min cost option from an algorithm options type.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_scalar_function-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.get_scalar_function","text":"get_scalar_function(prob::AbstractProblem)\n\nReturns cost function plus the infeasibility penalty squared as a scalar value.\nThis is used for PSO (GA, Differential Evolution, etc. if we ever get around to adding those)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.get_scalar_function_with_penalty-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.get_scalar_function_with_penalty","text":"get_scalar_function_with_penalty(prob::AbstractProblem)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.has_gradient-Tuple{GlobalOptimization.AbstractEvaluator}","page":"Internals","title":"GlobalOptimization.has_gradient","text":"has_gradient(evaluator::AbstractEvaluator)\n\nReturns true if the evaluator has a gradient, otherwise, false.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Tuple{GlobalOptimization.AbstractMBHDistribution, ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(dist::AbstractMBHDistribution, num_dims)\n\nInitializes the distribution dist with the number of dimensions num_dims.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Tuple{MATLABVelocityUpdate, Any}","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(vu::AbstractVelocityUpdateScheme, swarm_size::Int)\n\nInitialize the velocity update scheme for a given swarm size.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Union{Tuple{T}, Tuple{GlobalOptimization.Hopper{T}, ContinuousRectangularSearchSpace{T}, GlobalOptimization.FeasibilityHandlingEvaluator}} where T","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(\n    hopper::Hopper{T},\n    search_space::ContinuousRectangularSearchSpace{T},\n    evaluator::FeasibilityHandlingEvaluator{T},\n)\n\nInitializes the hopper position in the search space.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.AbstractPopulationInitialization, ContinuousRectangularSearchSpace{T}}} where T","page":"Internals","title":"GlobalOptimization.initialize!","text":"initialize!(\n    swarm::Swarm{T},\n    pop_init_method::AbstractPopulationInitialization,\n    search_space::ContinuousRectangularSearchSpace{T}\n)\n\nInitializes the swarm population with pop_init_method in the search_space.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.initialize_fitness!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.BatchEvaluator}} where T","page":"Internals","title":"GlobalOptimization.initialize_fitness!","text":"initialize_fitness!(swarm::Swarm{T}, evaluator::BatchEvaluator)\n\nInitializes the fitness of each candidate in the swarm swarm using the evaluator.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.intersection-Union{Tuple{T2}, Tuple{T1}, Tuple{ContinuousRectangularSearchSpace{T1}, ContinuousRectangularSearchSpace{T2}}} where {T1, T2}","page":"Internals","title":"GlobalOptimization.intersection","text":"intersection(\n    ss1::ContinuousRectangularSearchSpace{T1},\n    ss2::ContinuousRectangularSearchSpace{T2}\n)\n\nReturns the intersection of the two search spaces ss1 and ss2 as a new search space.\n\nArguments\n\nss1::ContinuousRectangularSearchSpace{T1}\nss2::ContinuousRectangularSearchSpace{T2}\n\nReturns\n\n`ContinuousRectangularSearchSpace{promote_type(T1, T2)}\n\nThrows\n\nDimensionMismatch: if ss1 and ss2 do not have the same number of dimensions.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.mutate!-Union{Tuple{MP}, Tuple{GlobalOptimization.DEPopulation, MP}} where MP<:GlobalOptimization.AbstractMutationParameters","page":"Internals","title":"GlobalOptimization.mutate!","text":"mutate!(population::DEPopulation{T}, F)\n\nMutates the population population using the DE mutation strategy.\n\nThis is an implementation of the unified mutation strategy proposed by Ji Qiang and Chad Mitchell in \"A Unified Differential Evolution Algorithm for Global Optimization\".\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.num_dims-Tuple{ContinuousRectangularSearchSpace}","page":"Internals","title":"GlobalOptimization.num_dims","text":"num_dims(ss::ContinuousRectangularSearchSpace)\n\nReturns the number of dimensions in the search space ss.\n\nArguments\n\nss::ContinuousRectangularSearchSpace\n\nReturns\n\nInteger\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.num_dims-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.num_dims","text":"num_dims(prob::AbstractProblem)\n\nReturns the number of dimensions of the decision vector of the optimization problem prob.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.push!-Union{Tuple{T}, Tuple{GlobalOptimization.MBHStepMemory{T}, AbstractVector{T}, T, T}} where T","page":"Internals","title":"GlobalOptimization.push!","text":"push!(step_memory::MBHStepMemoory{T}, step::Vector{T}, pre_step_fitness::T, post_step_fitness::T)\n\nPushes a step into the step memory.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.push_accepted_step!-Union{Tuple{T}, Tuple{MBHAdaptiveDistribution{T}, AbstractVector{T}, T, T}} where T","page":"Internals","title":"GlobalOptimization.push_accepted_step!","text":"push_accepted_step!(\n    dist::MBHAdaptiveDistribution{T},\n    step::AbstractVector{T},\n    pre_step_fitness::T,\n    post_step_fitness::T,\n) where {T}\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.reset!-Tuple{GlobalOptimization.Hopper}","page":"Internals","title":"GlobalOptimization.reset!","text":"reset!(hopper::Hopper)\n\nResets the candidate to state prior to the last draw_update! call.\n\nNote: This just subtracts the last step from the candidate.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function-Union{Tuple{G}, Tuple{F}, Tuple{SS}, Tuple{has_penalty}, Tuple{OptimizationProblem{has_penalty, SS, F, G}, AbstractArray}} where {has_penalty, SS, F, G}","page":"Internals","title":"GlobalOptimization.scalar_function","text":"scalar_function(prob::OptimizationProblem, x::AbstractArray)\n\nEvaluates the objective function f of the optimization problem prob at x and returns the cost function plus the infeasibility.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function-Union{Tuple{has_penalty}, Tuple{GlobalOptimization.AbstractNonlinearEquationProblem{has_penalty}, AbstractArray}} where has_penalty","page":"Internals","title":"GlobalOptimization.scalar_function","text":"scalar_function(prob::AbstractNonlinearEquationProblem, x::AbstractArray)\n\nEvaluates the set of nonlinear equations f and returns the nonlinear least squares cost plus half the infeasibility squared.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function_with_penalty-Union{Tuple{G}, Tuple{F}, Tuple{SS}, Tuple{has_penalty}, Tuple{OptimizationProblem{has_penalty, SS, F, G}, AbstractArray}} where {has_penalty, SS, F, G}","page":"Internals","title":"GlobalOptimization.scalar_function_with_penalty","text":"scalar_function_with_penalty(prob::OptimizationProblem, x::AbstractArray)\n\nEvaluates the objective function f of the optimization problem prob at x and returns the cost function and the infeasibility penalty term as tuple. i.e., for an OptimizationProblem, this is simply the original function.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.scalar_function_with_penalty-Union{Tuple{has_penalty}, Tuple{GlobalOptimization.AbstractNonlinearEquationProblem{has_penalty}, AbstractArray}} where has_penalty","page":"Internals","title":"GlobalOptimization.scalar_function_with_penalty","text":"scalar_function_with_penalty(prob::AbstractNonlinearEquationProblem, x::AbstractArray)\n\nEvaluates the set of nonlinear equations f and returns the nonlinear least squares cost and the infeasibility penalty term as a tuple.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.search_space-Tuple{GlobalOptimization.AbstractProblem}","page":"Internals","title":"GlobalOptimization.search_space","text":"search_space(prob::AbstractProblem)\n\nReturns the search space of the optimization problem prob.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.selection!-Tuple{GlobalOptimization.DEPopulation}","page":"Internals","title":"GlobalOptimization.selection!","text":"selection!(population::DEPopulation{T}, evaluator::BatchEvaluator)\n\nReplace candidates with mutants if they have a better fitness.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.set_fitness!-Tuple{GlobalOptimization.AbstractCandidate, Any}","page":"Internals","title":"GlobalOptimization.set_fitness!","text":"set_fitness!(c::AbstractCandidate, fitness)\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.set_fitness!-Tuple{GlobalOptimization.AbstractPopulation, Vector}","page":"Internals","title":"GlobalOptimization.set_fitness!","text":"set_fitness(pop::AbstractPopulation, fitness, [i::Integer])\n\nSets the fitness of the candidates from a population. If i is specified, sets the i-th fitness.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.step!-Tuple{GlobalOptimization.Swarm}","page":"Internals","title":"GlobalOptimization.step!","text":"step!(swarm::Swarm)\n\nSteps the swarm swarm forward one iteration.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.step_MAD_median-Union{Tuple{T}, Tuple{GlobalOptimization.MBHStepMemory{T}, Integer}} where T","page":"Internals","title":"GlobalOptimization.step_MAD_median","text":"step_MAD_median(step_memory::MBHStepMemory{T}, var_idx::Integer)\n\nReturns the mean absolute deviation (MAD) around the median of the step memory. If var_idx is specified, then the MAD median of the step memory for the variable at index var_idx is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.step_std-Union{Tuple{T}, Tuple{GlobalOptimization.MBHStepMemory{T}, Integer}} where T","page":"Internals","title":"GlobalOptimization.step_std","text":"step_std(step_memory::MBHStepMemory{T}, var_idx::Integer)\n\nReturns the standard deviation of the step memory. If var_idx is specified, then the standard deviation of the step memory for the variable at index var_idx is returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.update_fitness!-Union{Tuple{T}, Tuple{GlobalOptimization.SingleHopperSet{T}, MBHStaticDistribution{T}}} where T","page":"Internals","title":"GlobalOptimization.update_fitness!","text":"update_fitness!(hopper::AbstractHopperSet{T}, distribution::AbstractMBHDistribution{T})\n\nUpdates the hopper fitness information after previously evaluating the fitness of the hopper.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.update_global_best!-Tuple{PSO}","page":"Internals","title":"GlobalOptimization.update_global_best!","text":"update_global_best!(pso::PSO)\n\nUpdates the global best candidate in the PSO algorithm pso if a better candidate is found.\n\n\n\n\n\n","category":"method"},{"location":"lib/internal/temp/#GlobalOptimization.update_velocity!-Union{Tuple{T}, Tuple{GlobalOptimization.Swarm{T}, GlobalOptimization.AbstractRandomNeighborhoodVelocityUpdateScheme}} where T","page":"Internals","title":"GlobalOptimization.update_velocity!","text":"update_velocity!(swarm::Swarm, vu::AbstractVelocityUpdateScheme)\n\nUpdate the velocity of each candidate in the swarm using the specified velocity update scheme.\n\n\n\n\n\n","category":"method"},{"location":"algs/de/","page":"-","title":"-","text":"To be continued...","category":"page"},{"location":"algs/mbh/","page":"-","title":"-","text":"Not implemented...","category":"page"},{"location":"lib/public/#Public-API-Documentation","page":"Public API","title":"Public API Documentation","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Documentation for GlobalOptimization's public interface.","category":"page"},{"location":"lib/public/#Contents","page":"Public API","title":"Contents","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Pages = [\"public.md\"]\nDepth = 2:3","category":"page"},{"location":"lib/public/#Problems","page":"Public API","title":"Problems","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem{has_penalty, SS, F, G}\n\nA nonlinear least squares problem. Contains the nonlinear equations and search space.\n\nFields\n\nf::F: The nonlinear equations.\ng!::G: The jacobian of the nonlinear equations.\nss::SS: The search space.\nn::Int: The number of residuals.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem-Union{Tuple{F}, Tuple{F, AbstractVector{<:Real}, AbstractVector{<:Real}, Int64}} where F<:Function","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem(f, [g], LB, UB)\n\nConstructs a nonlinear least squares problem with nonlinear function f, optional Jacobian g, and a search space.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nLB::AbstractVector{<:Real}: The lower bounds of the search space.\nUB::AbstractVector{<:Real}: The upper bounds of the search space.\n\nReturns\n\nNonlinearLeastSquaresProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - x[3], x[2] - x[3]]\njulia> LB = [-5.0, -5.0, -5.0];\njulia> UB = [ 5.0, 5.0, 5.0];\njulia> prob = NonlinearLeastSquaresProblem(f, ss, LB, UB, 2)\nNonlinearLeastSquaresProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0, -5.0], [5.0, 5.0, 5.0], [10.0, 10.0, 10.0]), 2)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem-Union{Tuple{F}, Tuple{has_penalty}, Tuple{T}, Tuple{F, GlobalOptimization.SearchSpace{T}, Int64}} where {T, has_penalty, F<:Function}","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem{has_penalty}(f::F, [g::G], ss::SS, num_resid::Int)\n\nConstructs a nonlinear least squares problem with nonlinear functions f, optional jacobian g, and search space ss. If has_penalty is specified as true, then the nonlinear function must return a Tuple{AbstractArray{T},T} for a given x of type AbstractArray{T}.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\nnum_resid::Int: The number of residuals.\n\nReturns\n\nNonlinearLeastSquaresProblem{has_penalty, SS, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - x[3], x[2] - x[3]]\njulia> LB = [-5.0, -5.0, -5.0];\njulia> UB = [ 5.0, 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearLeastSquaresProblem(f, ss, 2)\nNonlinearLeastSquaresProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0, -5.0], [5.0, 5.0, 5.0], [10.0, 10.0, 10.0]), 2)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearLeastSquaresProblem-Union{Tuple{SS}, Tuple{F}, Tuple{F, SS, Int64}} where {F<:Function, SS<:GlobalOptimization.SearchSpace}","page":"Public API","title":"GlobalOptimization.NonlinearLeastSquaresProblem","text":"NonlinearLeastSquaresProblem(f, [g], ss)\n\nConstructs a nonlinear least squares problem with nonlinear function f, optional Jacobian g, and a search space.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\n\nReturns\n\nNonlinearLeastSquaresProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - x[3], x[2] - x[3]]\njulia> LB = [-5.0, -5.0, -5.0];\njulia> UB = [ 5.0, 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearLeastSquaresProblem(f, ss, 2)\nNonlinearLeastSquaresProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0, -5.0], [5.0, 5.0, 5.0], [10.0, 10.0, 10.0]), 2)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem{has_penalty, SS, F, G}\n\nA nonlinear problem. Contains the nonlinear equations and search space.\n\nFields\n\nf::F: The nonlinear equations.\ng!::G: The jacobian of the nonlinear equations.\nss::SS: The search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem-Union{Tuple{F}, Tuple{F, AbstractVector{<:Real}, AbstractVector{<:Real}}} where F<:Function","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem(f, [g], LB, UB)\n\nConstructs a nonlinear problem with nonlinear function f, optional Jacobian g, and a continuous rectangular search space defined by the bounds LB and UB.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nLB::AbstractVector{<:Real}: The lower bounds of the search space.\nUB::AbstractVector{<:Real}: The upper bounds of the search space.\n\nReturns\n\nNonlinearProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - 2.0, x[2] - 2.0]\njulia> LB = [-5.0, -5.0];\njulia> UB = [ 5.0, 5.0];\njulia> prob = NonlinearProblem(f, LB, UB)\nNonlinearProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0], [5.0, 5.0], [10.0, 10.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem-Union{Tuple{F}, Tuple{has_penalty}, Tuple{T}, Tuple{F, GlobalOptimization.SearchSpace{T}}} where {T, has_penalty, F<:Function}","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem{has_penalty}(f::F, [g::G], ss::SS)\n\nConstructs a nonlinear problem with nonlinear functions f, optional jacobian g, and search space ss. If has_penalty is specified as true, then the nonlinear function must return a Tuple{AbstractArray{T},T} for a given x of type AbstractArray{T}.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\n\nReturns\n\nNonlinearProblem{has_penalty, SS, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - 2.0, x[2] - 2.0]\njulia> LB = [-5.0, -5.0];\njulia> UB = [ 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearProblem(f, ss)\nNonlinearProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0], [5.0, 5.0], [10.0, 10.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearProblem-Union{Tuple{SS}, Tuple{F}, Tuple{F, SS}} where {F<:Function, SS<:GlobalOptimization.SearchSpace}","page":"Public API","title":"GlobalOptimization.NonlinearProblem","text":"NonlinearProblem(f, [g], ss)\n\nConstructs a nonlinear problem with nonlinear function f, optional Jacobian g, and a search space.\n\nArguments\n\nf::F: The nonlinear function.\ng::G: The Jacobian of the nonlinear function.\nss::SS: The search space.\n\nReturns\n\nNonlinearProblem{has_penalty, ContinuousRectangularSearchSpace, F, G}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = [x[1] - 2.0, x[2] - 2.0]\njulia> LB = [-5.0, -5.0];\njulia> UB = [ 5.0, 5.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = NonlinearProblem(f, ss)\nNonlinearProblem{Val{false}(), ContinuousRectangularSearchSpace{Float64}, typeof(f), Nothing}(f, nothing, ContinuousRectangularSearchSpace{Float64}([-5.0, -5.0], [5.0, 5.0], [10.0, 10.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem{has_penalty, SS, F, G}\n\nAn optimization problem. Contains the objective function and search space.\n\nFields\n\nf::F: The objective function.\ng!::G: The gradient of the objective function.\nss::SS: The search space.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem-Union{Tuple{F}, Tuple{F, AbstractVector{<:Real}, AbstractVector{<:Real}}} where F<:Function","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem(f, [g], LB, UB)\n\nConstructs an optimization problem with objective function f, optional gradient g, and a ContinuousRectangularSearchSpace defined by LB and UB.\n\nArguments\n\nf::F: The objective function.\nLB::AbstractVector{<:Real}: The lower bounds of the search space.\nUB::AbstractVector{<:Real}: The upper bounds of the search space.\n\nReturns\n\nOptimizationProblem{ContinuousRectangularSearchSpace, F}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = sum(x.^2); # Simple sphere function\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> prob = OptimizationProblem(f, LB, UB)\nOptimizationProblem{ContinuousRectangularSearchSpace{Float64}, typeof(f)}(f, ContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem-Union{Tuple{F}, Tuple{has_penalty}, Tuple{T}, Tuple{F, GlobalOptimization.SearchSpace{T}}} where {T, has_penalty, F<:Function}","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem{has_penalty}(f::F, [g::G], ss::SS)\n\nConstructs an optimization problem with objective function f, optional gradient g, and search space ss. If has_penalty is specified as true, then the objective function must return a Tuple{T,T} for a given x of type AbstractArray{T}.\n\nArguments\n\nf::F: The objective function.\ng::G: The gradient of the objective function.\nss::SS: The search space.\n\nReturns\n\nOptimizationProblem{SS, F}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = sum(x.^2); # Simple sphere function\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB);\njulia> prob = OptimizationProblem(f, ss)\nOptimizationProblem{ContinuousRectangularSearchSpace{Float64}, typeof(f)}(f, ContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.OptimizationProblem-Union{Tuple{SS}, Tuple{F}, Tuple{F, SS}} where {F<:Function, SS<:GlobalOptimization.SearchSpace}","page":"Public API","title":"GlobalOptimization.OptimizationProblem","text":"OptimizationProblem(f, [g], ss)\n\nConstructs an optimization problem with objective function f, optimal gradient g, and a search space.\n\nArguments\n\nf::F: The objective function.\ng::G: The gradient of the objective function.\nss::SS: The search space.\n\nReturns\n\nOptimizationProblem{ContinuousRectangularSearchSpace, F}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> f(x) = sum(x.^2); # Simple sphere function\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> prob = OptimizationProblem(f, ContinuousRectangularSearchSpace(LB, UB))\nOptimizationProblem{ContinuousRectangularSearchSpace{Float64}, typeof(f)}(f, ContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0]))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Search-Space","page":"Public API","title":"Search Space","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.ContinuousRectangularSearchSpace","page":"Public API","title":"GlobalOptimization.ContinuousRectangularSearchSpace","text":"ContinuousRectangularSearchSpace{T <: AbstractFloat}\n\nA RectangularSearchSpace formed by a single continuous set.\n\nFields\n\ndim_min::Vector{T}: A vector of minimum values for each dimension.\ndim_max::Vector{T}: A vector of maximum values for each dimension.\ndim_delta::Vector{T}: A vector of the difference between the maximum and minimum values for each dimension.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.ContinuousRectangularSearchSpace-Union{Tuple{T2}, Tuple{T1}, Tuple{AbstractVector{T1}, AbstractVector{T2}}} where {T1<:Real, T2<:Real}","page":"Public API","title":"GlobalOptimization.ContinuousRectangularSearchSpace","text":"ContinuousRectangularSearchSpace(dim_min::AbstractVector{T}, dim_max::AbstractVector{T})\n\nConstructs a new ContinuousRectangularSearchSpace with minimum values dim_min and maximum values dim_max.\n\nArguments\n\ndim_min::AbstractVector{T}: A vector of minimum values for each dimension.\ndim_max::AbstractVector{T}: A vector of maximum values for each dimension.\n\nReturns\n\nContinuousRectangularSearchSpace{T}\n\nExamples\n\njulia> using GlobalOptimization;\njulia> LB = [-1.0, 0.0];\njulia> UB = [ 1.0, 2.0];\njulia> ss = ContinuousRectangularSearchSpace(LB, UB)\nContinuousRectangularSearchSpace{Float64}([-1.0, 0.0], [1.0, 2.0], [2.0, 2.0])\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Optimization","page":"Public API","title":"Optimization","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.optimize!-Tuple{GlobalOptimization.AbstractOptimizer}","page":"Public API","title":"GlobalOptimization.optimize!","text":"optimize!(opt::AbstractOptimizer)\n\nPerform optimization using the optimizer opt. Returns the results of the optimization.\n\nArguments\n\nopt::AbstractOptimizer: The optimizer to use.\n\nReturns\n\nResults: The results of the optimization. See the Results docstring for details   on its contents.\n\nExample\n\njulia> using GlobalOptimization\njulia> f(x) = sum(x.^2) # Simple sphere function\njulia> prob = OptimizationProblem(f, [-1.0, 0.0], [1.0, 2.0])\njulia> pso = SerialPSO(prob)\njulia> results = optimize!(pso)\nResults:\n - Best function value: 6.696180996034206e-20\n - Best candidate: [-2.587698010980842e-10, 0.0]\n - Iterations: 26\n - Time: 0.004351139068603516 seconds\n - Exit flag: 3\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Population-Initialization","page":"Public API","title":"Population Initialization","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.LatinHypercubeInitialization","page":"Public API","title":"GlobalOptimization.LatinHypercubeInitialization","text":"LatinHypercubeInitialization\n\nInitializes a population using optimal Latin hypercube sampling as implemented in LatinHypercubeSampling.jl.\n\nFields:\n\ngens::Int: Number of GA generations to use to generate the Latin hypercube samples.\nrng::U: Random number generator to use for the Latin hypercube sampling.\npop_size::Int: Size of the GA population used to generate the Latin hypercube samples.\nn_tour::Int: Number of tours to use in the GA.\np_tour::Float64: Probability of tour to use in the GA.\ninter_sample_weight::Float64: Weight of the inter-sample distance in the GA.\nperiodic_ae::Bool: Whether to use periodic adaptive evolution in the GA.\nae_power::Float64: Power of the adaptive evolution in the GA.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.LatinHypercubeInitialization-Tuple{Int64}","page":"Public API","title":"GlobalOptimization.LatinHypercubeInitialization","text":"LatinHypercubeInitialization(gens::Int = 10; kwargs...)\n\nInitializes a Latin hypercube sampling method with the given parameters.\n\nArguments\n\ngens::Int: Number of GA generations to use to generate the Latin hypercube samples.   Defaults to 10.\n\nKeyword Arguments\n\nrng::U: Random number generator to use for the Latin hypercube sampling.   Defaults to Random.GLOBAL_RNG.\npop_size::Int: Size of the GA population used to generate the Latin hypercube samples.   Defaults to 100.\nn_tour::Int: Number of tours to use in the GA. Defaults to 2.\np_tour::Float64: Probability of tour to use in the GA. Defaults to 0.8.\ninter_sample_weight::Float64: Weight of the inter-sample distance in the GA.   Defaults to 1.0.\nperiodic_ae::Bool: Whether to use periodic adaptive evolution in the GA.   Defaults to false.\nae_power::Float64: Power of the adaptive evolution in the GA. Defaults to 2.0.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Function-Evaluation-Methods","page":"Public API","title":"Function Evaluation Methods","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.SerialFunctionEvaluation","page":"Public API","title":"GlobalOptimization.SerialFunctionEvaluation","text":"SerialFunctionEvaluation\n\nA function evaluation method that evaluates the fitness of a candidate in serial.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SerialFunctionEvaluation-Tuple{}","page":"Public API","title":"GlobalOptimization.SerialFunctionEvaluation","text":"SerialFunctionEvaluation()\n\nConstruct a SerialFunctionEvaluation object.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.ThreadedFunctionEvaluation","page":"Public API","title":"GlobalOptimization.ThreadedFunctionEvaluation","text":"ThreadedFunctionEvaluation{S <: ChunkSplitters.Split}\n\nA function evaluation method that evaluates the fitness of a candidate in parallel using     multi-threading from Base.Threads.jl.\n\nFields\n\nn::Int: The number of batch jobs to split the workload into using   ChunkSplitters.jl.\nsplit::S: The chunk splitter to use. See ChunkSplitters.jl   for more information.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.ThreadedFunctionEvaluation-Tuple{}","page":"Public API","title":"GlobalOptimization.ThreadedFunctionEvaluation","text":"ThreadedFunctionEvaluation(\n    n::Int=Threads.nthreads(),\n    split::S=ChunkSplitters.RoundRobin(),\n)\n\nConstruct a ThreadedFunctionEvaluation object.\n\nKeyword Arguments\n\nn::Int: The number of batch jobs to split the workload into using   ChunkSplitters.jl.\nsplit::S: The chunk splitter to use. See ChunkSplitters.jl   for more information.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.PolyesterFunctionEvaluation","page":"Public API","title":"GlobalOptimization.PolyesterFunctionEvaluation","text":"PolyesterFunctionEvaluation\n\nA function evaluation method that evaluates the fitness of a candidate in parallel using     Polyester.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.PolyesterFunctionEvaluation-Tuple{}","page":"Public API","title":"GlobalOptimization.PolyesterFunctionEvaluation","text":"PolyesterFunctionEvaluation()\n\nConstruct a PolyesterFunctionEvaluation object.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Algorithms","page":"Public API","title":"Algorithms","text":"","category":"section"},{"location":"lib/public/#Particle-Swarm-Optimization","page":"Public API","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.PSO","page":"Public API","title":"GlobalOptimization.PSO","text":"PSO\n\nParticle Swarm Optimization (PSO) algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.PSO-Union{Tuple{GlobalOptimization.AbstractProblem{has_penalty, SS}}, Tuple{has_penalty}, Tuple{SS}, Tuple{T}} where {T<:AbstractFloat, SS<:ContinuousRectangularSearchSpace{T}, has_penalty}","page":"Public API","title":"GlobalOptimization.PSO","text":"PSO(prob::AbstractProblem{has_penalty,SS}; kwargs...)\n\nConstructs a PSO algorithm with the given options.\n\nArguments\n\nprob::AbstractProblem{has_penalty,SS}: The problem to solve.\n\nKeyword Arguments\n\neval_method::AbstractFunctionEvaluationMethod=SerialFunctionEvaluation(): The method to use for evaluating the objective function.\nnum_particles::Int = 100: The number of particles to use.\npopulation_init_method::AbstractPopulationInitialization = UniformInitialization(): The method to use for initializing the population.\ninitial_space::Union{Nothing,ContinuousRectangularSearchSpace} = nothing: The initial bounds to use when initializing particle positions.\nmax_iterations::Int = 1000: The maximum number of iterations to perform.\nfunction_tolerence::AbstractFloat = 1e-6: The function value tolerence to use for stopping criteria.\nmax_stall_time::Real = Inf: The maximum amount of time to allow for stall time.\nmax_stall_iterations::Int = 25: The maximum number of stall iterations to allow.\ninertia_range::Tuple{AbstractFloat,AbstractFloat} = (0.1, 1.0): The range of allowable inertia weights.\nminimum_neighborhood_fraction::AbstractFloat = 0.25: The minimum neighborhood fraction to use.\nself_adjustment_weight::Real = 1.49: The self adjustment weight to use.\nsocial_adjustment_weight::Real = 1.49: The social adjustment weight to use.\ndisplay::Bool = false: Whether or not to display the status of the algorithm.\ndisplay_interval::Int = 1: The display interval to use.\nfunction_value_check::Bool = true: Whether or not to check for bad function values (Inf or NaN).\nmax_time::Real = 60.0: The maximum amount of time to allow for optimization.\n\nReturns\n\nPSO: The PSO algorithm.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Velocity-Update-Schemes","page":"Public API","title":"Velocity Update Schemes","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"The PSO velocity update is the primary mechanism that drives the stochastic  optimization process. The currently implemented velocity update schemes can  be described by the following:","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Consider a swarm of n particles mathcalS = mathbfp_i_i=12dotsn. Each particle mathbfp_i has the following attributes associated with it:","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"position: mathbfx_i\nvelocity: mathbfv_i,\nbest position: mathbfx_ib\nbest fitness: f_ib = f(mathbfx_ib)","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"At each iteration of the PSO algorithm, the velocity of each particle is updated prior to  updating the position of each particle with mathbfx_i = mathbfx_i + mathbfv_i. This velocity update is described (for the i-th particle) by the following expression:","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"mathbfv_i = w mathbfv_i +     y_1 mathbfr_1 (mathbfx_ib - mathbfx_i) +     y_2 mathbfr_2 (mathbfx_b - mathbfx_i)","category":"page"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"where w is the inertia,  r_1 and r_2 are realizations of a random vector described by the multivariate uniform distribution mathcalU(mathbf0 mathbf1), y_1 is the self-adjustment weight, y_2 is the social adjustment weight, and mathbfx_b is the best position in the neighborhood of the i-th particle mathcalN_i. That is, mathbfx_b = undersetxinmathcalX_bmathrmargmin(f(x)) where mathcalX_bi =  mathbfx_ib _mathbfp_i in mathcalN_i and mathcalN_i is a set containing a randomly selected subset of the particles in mathcalS (not including mathbfp_i). Both the size of mathcalN_i and  the inertia w are handle differently depending on the velocity update scheme used.","category":"page"},{"location":"lib/public/#GlobalOptimization.MATLABVelocityUpdate","page":"Public API","title":"GlobalOptimization.MATLABVelocityUpdate","text":"MATLABVelocityUpdate <: AbstractRandomNeighborhoodVelocityUpdateScheme\n\nA velocity update scheme employed by the MATLAB PSO algorithm. This scheme is described as follows:\n\nIn this velocity update scheme, the size of the neighborhood, as well as the inertia weight, are adaptively updated as follows:\n\nPrior to First Iteration:\n\nSet the inertial weight w: w = inertia_range[2]\nSet the minimum neighborhood size: minimum_neighborhood_size = max(2, floor(Int, swarm_size * minimum_neighborhood_fraction))\nSet the neighborhood size: N = minimum_neighborhood_size\nSet counter: c = 0\n\nAfter Evaluating Swarm Fitness Each Iteration:\n\nIf the best fitness of the swarm has improved:\nDecrease the counter: c = max(0, c - 1)\nSet the neighborhood size to the minimum: N = minimum_neighborhood_size\nUpdate the inertia weight:\nIf c < 2; w = 2.0 * w\nIf c > 5; w = 0.5 * w\nClamp w to lie in [inertia_range[1], inertia_range[2]]\nIf the best fitness of the swarm has not improved:\nIncrease the counter: c += 1\nIncrease the neighborhood size:  N = min(N + minimum_neighborhood_size, swarm_size - 1)\n\nFields\n\nswarm_size::Int: The size of the swarm.\ninertia_range::Tuple{Float64,Float64}: The range of inertia weights.\nminimum_neighborhood_fraction::Float64: The minimum fraction of the swarm size to be used as the neighborhood size.\nminimum_neighborhood_size::Int: The minimum neighborhood size.\nself_adjustment_weight::Float64: The self-adjustment weight.\nsocial_adjustment_weight::Float64: The social adjustment weight.\nw::Float64: The inertia weight.\nc::Int: The stall iteration counter.\nN::Int: The neighborhood size.\nindex_vector::Vector{UInt16}: A vector used to store the indices of the particles in the   swarm. Used for random neighborhood selection without allocations.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MATLABVelocityUpdate-Tuple{}","page":"Public API","title":"GlobalOptimization.MATLABVelocityUpdate","text":"MATLABVelocityUpdate(;\n    inertia_range::Tuple{AbstractFloat,AbstractFloat}=(0.1, 1.0),\n    minimum_neighborhood_fraction::AbstractFloat=0.25,\n    self_adjustment_weight::AbstractFloat=1.49,\n    social_adjustment_weight::AbstractFloat=1.49,\n)\n\nCreate a new instance of the MATLABVelocityUpdate velocity update scheme.\n\nKeyword Arguments\n\ninertia_range::Tuple{AbstractFloat,AbstractFloat}: The range of inertia weights.\nminimum_neighborhood_fraction::AbstractFloat: The minimum fraction of the swarm size to be used as the neighborhood size.\nself_adjustment_weight::AbstractFloat: The self-adjustment weight.\nsocial_adjustment_weight::AbstractFloat: The social adjustment weight.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.CSRNVelocityUpdate","page":"Public API","title":"GlobalOptimization.CSRNVelocityUpdate","text":"CSRNVelocityUpdate <: AbstractRandomNeighborhoodVelocityUpdateScheme\n\nA velocity update scheme employed a Constant Size Random Neighborhood (CSRN).\n\nIn this velocity update scheme, the size of the neighborhood is constant and set based on the specified neighborhood_fraction (i.e., the fraction of the swarm size to be considered to lie in a neighborhood). However, the inertia is adaptively updated as follows:\n\nPrior to First Iteration:  Set the inertial weight w: w = inertia_range[2]\n\nAfter Evaluating Swarm Fitness Each Iteration:\n\nIf stall_iteration < 2; w = 2.0 * w\nIf stall_iteration > 5; w = 0.5 * w\nClamp w to lie in [inertia_range[1], inertia_range[2]]\n\nNote that stall_iteration is the number of iterations since the global best position found so far was improved by a specified function_tolerance (see PSO keyword arguments).\n\nFields\n\ninertia_range::Tuple{Float64,Float64}: The range of inertia weights.\nneighborhood_fraction::Float64: The fraction of the swarm size to be used as the neighborhood size.\nN::Int: The neighborhood size.\nself_adjustment_weight::Float64: The self-adjustment weight.\nsocial_adjustment_weight::Float64: The social adjustment weight.\nw::Float64: The inertia weight.\nindex_vector::Vector{UInt16}: A vector used to store the indices of the particles in the   swarm. Used for random neighborhood selection without allocations.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CSRNVelocityUpdate-Tuple{}","page":"Public API","title":"GlobalOptimization.CSRNVelocityUpdate","text":"CSRNVelocityUpdate(;\n    inertia_range::Tuple{AbstractFloat,AbstractFloat}=(0.1, 1.0),\n    neighborhood_fraction::AbstractFloat=0.25,\n    self_adjustment_weight::AbstractFloat=1.49,\n    social_adjustment_weight::AbstractFloat=1.49,\n)\n\nCreate a new instance of the CSRNVelocityUpdate velocity update scheme.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Differential-Evolution","page":"Public API","title":"Differential Evolution","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.DE","page":"Public API","title":"GlobalOptimization.DE","text":"DE\n\nDifferential Evolution (DE) algorithm.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.DE-Union{Tuple{GlobalOptimization.AbstractProblem{has_penalty, SS}}, Tuple{has_penalty}, Tuple{SS}, Tuple{T}, Tuple{CP}, Tuple{MP}} where {MP<:GlobalOptimization.AbstractMutationParameters, CP<:GlobalOptimization.AbstractCrossoverParameters, T<:AbstractFloat, SS<:ContinuousRectangularSearchSpace{T}, has_penalty}","page":"Public API","title":"GlobalOptimization.DE","text":"SerialDE(prob::AbstractProblem{has_penalty,SS}; kwargs...)\n\nConstruct a serial Differential Evolution (DE) algorithm with the given options.\n\nArguments\n\nprob::AbstractProblem{has_penalty,SS}: The problem to solve.\n\nKeyword Arguments\n\neval_method::AbstractFunctionEvaluationMethod=SerialFunctionEvaluation(): The method to use for evaluating the objective function.\nnum_candidates::Integer=100: The number of candidates in the population.\npopulation_init_method::AbstractPopulationInitialization=UniformInitialization(): The population initialization method.\nmutation_params::MP=SelfMutationParameters(Rand1()): The mutation strategy parameters.\ncrossover_params::CP=BinomialCrossoverParameters(0.6): The crossover strategy parameters.\ninitial_space::Union{Nothing,ContinuousRectangularSearchSpace}=nothing: The initial bounds for the search space.\nmax_iterations::Integer=1000: The maximum number of iterations.\nmax_time::Real=60.0: The maximum time to run the algorithm.\nfunction_tolerance::Real=1e-6: The function tolerance for the stall condition.\nmax_stall_time::Real=60.0: The maximum stall time for the stall condition.\nmax_stall_iterations::Integer=100: The maximum number of stall iterations for the stall condition.\nmin_cost::Real=-Inf: The minimum cost for the algorithm to stop.\nfunction_value_check::Bool=true: Whether to check the function value.\ndisplay::Bool=true: Whether to display the algorithm status.\ndisplay_interval::Integer=1: The interval at which to display the algorithm status.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Mutation-Parameters","page":"Public API","title":"Mutation Parameters","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MutationParameters","page":"Public API","title":"GlobalOptimization.MutationParameters","text":"MutationParameters{\n    AS<:AbstractAdaptationStrategy,\n    MS<:AbstractMutationStrategy,\n    S<:AbstractSelector,\n    D,\n}\n\nThe parameters for a DE mutation strategy that applies to all current and future candidates in the population.\n\nFields\n\nF1::Float64: The F weight in the unified mutation strategy.\nF2::Float64: The F weight in the unified mutation strategy.\nF3::Float64: The F weight in the unified mutation strategy.\nF4::Float64: The F weight in the unified mutation strategy.\nsel<:AbstractSelector: The selector used to select the candidates considered in   mutation.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the mutation   parameters. Note that this should generally be a distribution from   Distributions.jl, but the only strict   requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MutationParameters-NTuple{4, Any}","page":"Public API","title":"GlobalOptimization.MutationParameters","text":"MutationParameters(F1, F2, F3, F4; sel=SimpleSelector())\n\nCreates a MutationParameters object with the specified (constant) mutation parameters. These constant mutation parameters are used for all candidates in the population and define a unified mutation strategy as defined in Ji Qiang and Chad Mitchell \"A Unified Differential Evolution Algorithm for Global Optimization,\" 2014, https://www.osti.gov/servlets/purl/1163659\n\nArguments\n\nF1::Float64: The F weight in the unified mutation strategy.\nF2::Float64: The F weight in the unified mutation strategy.\nF3::Float64: The F weight in the unified mutation strategy.\nF4::Float64: The F weight in the unified mutation strategy.\n\nKeyword Arguments\n\nsel::AbstractSelector: The selector used to select the candidates considered in   mutation. Defaults to SimpleSelector().\n\nReturns\n\nMutationParameters{NoAdaptation,Unified,typeof(sel),Nothing}: A mutation parameters   object with the specified mutation parameters and selector.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = MutationParameters(0.5, 0.5, 0.5, 0.5)\nMutationParameters{GlobalOptimization.NoAdaptation, Unified, SimpleSelector, Nothing}(0.5, 0.5, 0.5, 0.5, SimpleSelector(), nothing)\n\njulia> using GlobalOptimization\njulia> params = MutationParameters(0.5, 0.5, 0.5, 0.5; sel=RadiusLimitedSelector(2))\nMutationParameters{GlobalOptimization.NoAdaptation, Unified, RadiusLimitedSelector, Nothing}(0.5, 0.5, 0.5, 0.5, RadiusLimitedSelector(2, UInt16[0x6cf0, 0x0c33, 0x0001, 0x0000, 0x0560]), nothing)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.MutationParameters-Tuple{GlobalOptimization.AbstractMutationStrategy}","page":"Public API","title":"GlobalOptimization.MutationParameters","text":"MutationParameters(\n    strategy::MS;\n    dist=default_mutation_dist,\n    sel=SimpleSelector(),\n)\n\nCreates a MutationParameters object with the specified mutation strategy with mutation parameter random adaptation. The mutation parameters are adaptively sampled from the provided dist, clamped to the range (0, 1].\n\nArguments\n\nstrategy::MS: The mutation strategy to use. This should be one of the mutation   strategies defined in this module (e.g., Rand1, Best2, etc.).\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   mutation parameters each iteration. Note that this should generally be a   distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   GlobalOptimization.default_mutation_dist, which is a mixture model comprised of   two Cauchy distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2).\nwhere mu = 065 10 and sigma = 01 01.\nsel::AbstractSelector: The selector used to select the candidates considered in   mutation. Defaults to SimpleSelector().\n\nReturns\n\nMutationParameters{RandomAdaptation,typeof(strategy),typeof(sel),typeof(dist)}: A   mutation parameters object with the specified mutation strategy and selector.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = MutationParameters(Rand1())\nMutationParameters{GlobalOptimization.RandomAdaptation, Rand1, SimpleSelector, Distributions.MixtureModel{Distributions.Univariate, Distributions.Continuous, Distributions.Cauchy, Distributions.Categorical{Float64, Vector{Float64}}}}(0.0, 1.0, 0.8450801042502032, 0.0, SimpleSelector(), MixtureModel{Distributions.Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Distributions.Cauchy{Float64}(=0.65, =0.1)\ncomponents[2] (prior = 0.5000): Distributions.Cauchy{Float64}(=1.0, =0.1)\n)\n\njulia> using GlobalOptimization\njulia> using Distributions\njulia> params = MutationParameters(Rand1(); dist=Normal(0.5, 0.1))\nMutationParameters{GlobalOptimization.RandomAdaptation, Rand1, SimpleSelector, Normal{Float64}}(0.0, 1.0, 0.5061103661726901, 0.0, SimpleSelector(), Normal{Float64}(=0.5, =0.1))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.SelfMutationParameters","page":"Public API","title":"GlobalOptimization.SelfMutationParameters","text":"SelfMutationParameters{\n    AS<:AbstractAdaptationStrategy,\n    MS<:AbstractMutationStrategy,\n    S<:AbstractSelector,\n    D,\n}\n\nThe parameters for a DE mutation strategy that applies a mutation strategy with unique parameters for each candidate in the population.\n\nFields\n\nFs::Vector{SVector{4,Float64}}: The mutation parameters for each candidate in the   population. Each element of the vector is an SVector{4} containing the F, F, F, and   F weights for the unified mutation strategy.\nsel<:AbstractSelector: The selector used to select the candidates considered in mutation.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the mutation   parameters. Note that this should generally be a distribution from   Distributions.jl, but the only strict   requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SelfMutationParameters-Tuple{GlobalOptimization.AbstractMutationStrategy}","page":"Public API","title":"GlobalOptimization.SelfMutationParameters","text":"SelfMutationParameters(\n    strategy::MS;\n    dist=default_mutation_dist,\n    sel=SimpleSelector(),\n)\n\nCreates a SelfMutationParameters object with the specified mutation strategy and mutation parameter random adaptation. The mutation parameters are adaptively sampled from the provided dist, clamped to the range (0, 1].\n\nArguments\n\nstrategy::MS: The mutation strategy to use. This should be one of the mutation   strategies defined in this module (e.g., Rand1, Best2, etc.).\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   mutation parameters each iteration. Note that this should generally be a   distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   GlobalOptimization.default_mutation_dist, which is a mixture model comprised of   two Cauchy distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2).\nwhere mu = 065 10 and sigma = 01 01.\nsel::AbstractSelector: The selector used to select the candidates considered in   mutation. Defaults to SimpleSelector().\n\nReturns\n\nSelfMutationParameters{RandomAdaptation,typeof(strategy),typeof(sel),typeof(dist)}:   A mutation parameters object with the specified mutation strategy and selector.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = SelfMutationParameters(Rand1())\nSelfMutationParameters{GlobalOptimization.RandomAdaptation, Rand1, SimpleSelector, MixtureModel{Univariate, Continuous, Cauchy, Categorical{Float64, Vector{Float64}}}}(StaticArraysCore.SVector{4, Float64}[], SimpleSelector(), MixtureModel{Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Cauchy{Float64}(=0.65, =0.1)\ncomponents[2] (prior = 0.5000): Cauchy{Float64}(=1.0, =0.1)\n)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.Rand1","page":"Public API","title":"GlobalOptimization.Rand1","text":"Rand1\n\nThe DE/rand/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + Fleft(mathbfx_r_2 - mathbfx_r_3right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, and r_1, r_2, and r_3 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Rand2","page":"Public API","title":"GlobalOptimization.Rand2","text":"Rand2\n\nThe DE/rand/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + Fleft(mathbfx_r_2 - mathbfx_r_3right) + Fleft(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Best1","page":"Public API","title":"GlobalOptimization.Best1","text":"Best1\n\nThe DE/best/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_b + Fleft(mathbfx_r_1 - mathbfx_r_2right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, subscript b denotes the best candidate (in terms of the objective/fitness function), and r_1 and r_2 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Best2","page":"Public API","title":"GlobalOptimization.Best2","text":"Best2\n\nThe DE/best/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_b + Fleft(mathbfx_r_1 - mathbfx_r_2right) + Fleft(mathbfx_r_3 - mathbfx_r_4right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F is a scaling factor, subscript b denotes the best candidate, and r_1, r_2, r_3, and r_4 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToBest1","page":"Public API","title":"GlobalOptimization.CurrentToBest1","text":"CurrentToBest1\n\nThe DE/current-to-best/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_1 - mathbfx_r_2right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1 and r_2 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToBest2","page":"Public API","title":"GlobalOptimization.CurrentToBest2","text":"CurrentToBest2\n\nThe DE/current-to-best/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_1 - mathbfx_r_2right) + Fleft(mathbfx_r_3 - mathbfx_r_4right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1, r_2, r_3, and r_4 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToRand1","page":"Public API","title":"GlobalOptimization.CurrentToRand1","text":"CurrentToRand1\n\nThe DE/current-to-rand/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_r_1 - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, and r_1, r_2, and r_3 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CurrentToRand2","page":"Public API","title":"GlobalOptimization.CurrentToRand2","text":"CurrentToRand2\n\nThe DE/current-to-rand/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_i + F_crleft(mathbfx_r_1 - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right) + Fleft(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RandToBest1","page":"Public API","title":"GlobalOptimization.RandToBest1","text":"RandToBest1\n\nThe DE/rand-to-best/1 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1, r_2, and r_3 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RandToBest2","page":"Public API","title":"GlobalOptimization.RandToBest2","text":"RandToBest2\n\nThe DE/rand-to-best/2 mutation strategy given by:\n\nmathbfv_i = mathbfx_r_1 + F_crleft(mathbfx_b - mathbfx_iright) + Fleft(mathbfx_r_2 - mathbfx_r_3right) + Fleft(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_cs and F are a scaling factors, subscript b denotes the best candidate, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.Unified","page":"Public API","title":"GlobalOptimization.Unified","text":"Unified\n\nThe unified DE mutation strategy proposed by Ji Qiang and Chad Mitchell in \"A Unified Differential Evolution Algorithm for Global Optimization,\" 2014, https://www.osti.gov/servlets/purl/1163659.\n\nThis mutation strategy is given by:\n\nmathbfv_i = mathbfx_i + F_1left(mathbfx_b - mathbfx_iright) + F_2left(mathbfx_r_1 - mathbfx_iright) + F_3left(mathbfx_r_2 - mathbfx_r_3right) + F_4left(mathbfx_r_4 - mathbfx_r_5right)\n\nwhere mathbfv_i is the target (i-th) mutant, mathbfx_j denotes the j-th candidate, F_1, F_2, F_3, and F_4 are scaling factors, subscript b denotes the best candidate, and r_1, r_2, r_3, r_4, and r_5 are randomly selected integers in the set returned by the selector.\n\nNote that in the underlying implementation, all mutation strategies are implemented with this formulation, where each unique strategy has a different set of F_i  i in 1234 that are set to 0.0.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SimpleSelector","page":"Public API","title":"GlobalOptimization.SimpleSelector","text":"SimpleSelector\n\nA selector that simply selects all candidates in the population.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RadiusLimitedSelector","page":"Public API","title":"GlobalOptimization.RadiusLimitedSelector","text":"RadiusLimitedSelector\n\nA selector that selects candidates within a given radius of the target candidate.\n\nFor example, for population size of 10 and a radius of 2, the following will be selected for the given target indices:\n\ntarget = 5 will select [3, 4, 5, 6, 7]\n\ntarget = 1 will select [9, 10, 1, 2, 3]\n\ntarget = 9 will select [7, 8, 9, 10, 1]\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.RandomSubsetSelector","page":"Public API","title":"GlobalOptimization.RandomSubsetSelector","text":"RandomSubsetSelector\n\nA selector that selects a random subset of candidates from the population. The size of the subset is determined by the size parameter.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#DE-Crossover-Strategies","page":"Public API","title":"DE Crossover Strategies","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.BinomialCrossoverParameters","page":"Public API","title":"GlobalOptimization.BinomialCrossoverParameters","text":"BinomialCrossoverParameters{\n    AS<:AbstractAdaptationStrategy,\n    T<:AbstractCrossoverTransformation,\n    D,\n}\n\nThe parameters for a DE binomial crossover strategy.\n\nFields\n\nCR::Float64: The crossover rate.\ntransform::T: The transformation to apply to the candidate and mutant.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the crossover   rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.BinomialCrossoverParameters-Tuple{Float64}","page":"Public API","title":"GlobalOptimization.BinomialCrossoverParameters","text":"BinomialCrossoverParameters(CR::Float64; transform=NoTransformation())\n\nCreates a BinomialCrossoverParameters object with a fixed crossover rate CR and optional transformation transform.\n\nArguments\n\nCR::Float64: The crossover rate.\n\nKeyword Arguments\n\ntransform::AbstractCrossoverTransformation: The transformation to apply to the   candidate and mutant. Defaults to NoTransformation().\n\nReturns\n\nBinomialCrossoverParameters{NoAdaptation,typeof(transform),Nothing}: A   BinomialCrossoverParameters object with a fixed crossover rate and the optionally   specified transformation.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters(0.5)\nBinomialCrossoverParameters{GlobalOptimization.NoAdaptation, GlobalOptimization.NoTransformation, Nothing}(0.5, GlobalOptimization.NoTransformation(), nothing)\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters(0.5, transform=CovarianceTransformation(0.5, 0.5, 10))\nBinomialCrossoverParameters{GlobalOptimization.NoAdaptation, CovarianceTransformation, Nothing}(0.5, CovarianceTransformation(0.5, 0.5, [0.0 0.0  0.0 0.0; 0.0 0.0  0.0 0.0;  ; 0.0 0.0  0.0 0.0; 0.0 0.0  0.0 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), nothing)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.BinomialCrossoverParameters-Tuple{}","page":"Public API","title":"GlobalOptimization.BinomialCrossoverParameters","text":"BinomialCrossoverParameters(; dist=default_binomial_crossover_dist, transform=NoTransformation())\n\nCreates a BinomialCrossoverParameters object with an adaptive crossover rate and optional transformation transform.\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   crossover rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   default_binomial_crossover_dist, which is a mixture model comprised of two Cauchy   distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2)\nwhere mu = 01 095 and sigma = 01 01.\ntransform::AbstractCrossoverTransformation: The transformation to apply to the   candidate and mutant. Defaults to NoTransformation().\n\nReturns\n\nBinomialCrossoverParameters{RandomAdaptation,typeof(transform),typeof(dist)}: A   BinomialCrossoverParameters object with an adaptive crossover rate and the   optionally specified transformation.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters()\nBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, GlobalOptimization.NoTransformation, Distributions.MixtureModel{Distributions.Univariate, Distributions.Continuous, Distributions.Cauchy, Distributions.Categorical{Float64, Vector{Float64}}}}(0.0, GlobalOptimization.NoTransformation(), MixtureModel{Distributions.Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Distributions.Cauchy{Float64}(=0.1, =0.1)\ncomponents[2] (prior = 0.5000): Distributions.Cauchy{Float64}(=0.95, =0.1)\n)\n\njulia> using GlobalOptimization\njulia> params = BinomialCrossoverParameters(transform=CovarianceTransformation(0.5, 0.5, 10))\nBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, CovarianceTransformation, Distributions.MixtureModel{Distributions.Univariate, Distributions.Continuous, Distributions.Cauchy, Distributions.Categorical{Float64, Vector{Float64}}}}(0.0, CovarianceTransformation(0.5, 0.5, [2.195780764e-314 2.2117846174e-314  2.1293782266e-314 1.5617889024864e-311; 2.366805627e-314 2.316670011e-314  2.3355803934e-314 1.4259811738567e-311;  ; 2.195781025e-314 2.195781096e-314  1.4531427176862e-310 1.27319747493e-313; 2.366805627e-314 2.366805627e-314  1.0270459628367e-310 2.121995795e-314], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), MixtureModel{Distributions.Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Distributions.Cauchy{Float64}(=0.1, =0.1)\ncomponents[2] (prior = 0.5000): Distributions.Cauchy{Float64}(=0.95, =0.1)\n)\n\njulia> using GlobalOptimization\njulia> using Distributions\njulia> params = BinomialCrossoverParameters(dist=Uniform(0.0, 1.0))\nBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, GlobalOptimization.NoTransformation, Uniform{Float64}}(0.0, GlobalOptimization.NoTransformation(), Uniform{Float64}(a=0.0, b=1.0))\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.SelfBinomialCrossoverParameters","page":"Public API","title":"GlobalOptimization.SelfBinomialCrossoverParameters","text":"SelfBinomialCrossoverParameters{\n    AS<:AbstractAdaptationStrategy,\n    T<:AbstractCrossoverTransformation,\n    D,\n}\n\nThe parameters for a DE self-adaptive binomial crossover strategy.\n\nFields\n\nCRs::Vector{Float64}: The crossover rates for each candidate in the population.\ntransform::T: The transformation to apply to the candidate and mutant.\ndist<:Distribution{Univariate,Continuous}: The distribution used to adapt the crossover   rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.SelfBinomialCrossoverParameters-Tuple{}","page":"Public API","title":"GlobalOptimization.SelfBinomialCrossoverParameters","text":"SelfBinomialCrossoverParameters(;\n    dist=default_binomial_crossover_dist,\n    transform=NoTransformation()\n)\n\nCreates a SelfBinomialCrossoverParameters object with an adaptive crossover rate for each candidate in the population and an optional transformation transform.\n\nKeyword Arguments\n\ndist::Distribution{Univariate,Continuous}: The distribution used to adapt the   crossover rate parameter. Note that this should generally be a distribution from   Distributions.jl, but the only   strict requirement is that rand(dist) returns a floating point value. Defaults to   default_binomial_crossover_dist, which is a mixture model comprised of two Cauchy   distributions, with probability density given by:\nf_mix(x mu sigma) = 05 f(xmu_1sigma_1) + 05 f(xmu_2sigma_2)\nwhere mu = 01 095 and sigma = 01 01.\ntransform::AbstractCrossoverTransformation: The transformation to apply to the   candidate and mutant. Defaults to NoTransformation().\n\nReturns\n\nSelfBinomialCrossoverParameters{RandomAdaptation,typeof(transform),typeof(dist)}: A   SelfBinomialCrossoverParameters object with an adaptive crossover rate for each   candidate and the optionally specified transformation.\n\nExamples\n\njulia> using GlobalOptimization\njulia> params = SelfBinomialCrossoverParameters()\nSelfBinomialCrossoverParameters{GlobalOptimization.RandomAdaptation, GlobalOptimization.NoTransformation, MixtureModel{Univariate, Continuous, Cauchy, Categorical{Float64, Vector{Float64}}}}(Float64[], GlobalOptimization.NoTransformation(), MixtureModel{Cauchy}(K = 2)\ncomponents[1] (prior = 0.5000): Cauchy{Float64}(=0.1, =0.1)\ncomponents[2] (prior = 0.5000): Cauchy{Float64}(=0.95, =0.1)\n)\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.CovarianceTransformation","page":"Public API","title":"GlobalOptimization.CovarianceTransformation","text":"CovarianceTransformation{T<:AbstractCrossoverTransformation}\n\nA transformation for performing crossover in the eigen-space of the covariance matrix of the best candidates in the population.\n\nThis is an implementation of the method proposed by Wang and Li in \"Differential Evolution Based on Covariance Matrix Learning and Bimodal Distribution Parameter Setting, \" 2014, DOI: 10.1016/j.asoc.2014.01.038.\n\nFields\n\nps::Float64: The proportion of candidates to consider in the covariance matrix. That is,   for a population size of N, the covariance matrix is calculated using the   clamp(ceil(ps * N), 2, N) best candidates.\npb::Float64: The probability of applying the transformation.\nB::Matrix{Float64}: The real part of the eigenvectors of the covariance matrix.\nct::Vector{Float64}: The transformed candidate.\nmt::Vector{Float64}: The transformed mutant.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.CovarianceTransformation-Tuple{Any, Any, Any}","page":"Public API","title":"GlobalOptimization.CovarianceTransformation","text":"CovarianceTransformation(ps::Float64, pb::Float64, num_dims::Int)\n\nCreates a CovarianceTransformation object with the specified proportion of candidates to consider in the covariance matrix ps, the probability of applying the transformation pb, and the number of dimensions num_dims.\n\nThis is an implementation of the method proposed by Wang and Li in \"Differential Evolution Based on Covariance Matrix Learning and Bimodal Distribution Parameter Setting, \" 2014, DOI: 10.1016/j.asoc.2014.01.038.\n\nArguments\n\nps::Float64: The proportion of candidates to consider in the covariance matrix.\npb::Float64: The probability of applying the transformation.\nnum_dims::Int: The number of dimensions in the search space.\n\nReturns\n\nCovarianceTransformation: A CovarianceTransformation object with the specified   parameters.\n\nExamples\n\njulia> using GlobalOptimization\njulia> transformation = CovarianceTransformation(0.5, 0.5, 10)\nCovarianceTransformation(0.5, 0.5, [2.3352254645e-314 6.3877104275e-314  1.0e-323 5.0e-324; 6.3877051114e-314 6.3877104196e-314  6.3877054276e-314 6.387705455e-314;  ; 2.3352254645e-314 2.333217732e-314  0.0 6.3877095184e-314; 6.387705143e-314 2.130067282e-314  6.387705459e-314 6.387705463e-314], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0])\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Monotonic-Basin-Hopping","page":"Public API","title":"Monotonic Basin Hopping","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MBH","page":"Public API","title":"GlobalOptimization.MBH","text":"MBH\n\nMonotonic Basin Hopping (MBH) algorithm.\n\nThis implementation employs a single candidate rather than a population.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MBH-Union{Tuple{GlobalOptimization.AbstractProblem{has_penalty, ContinuousRectangularSearchSpace{T}}}, Tuple{has_penalty}, Tuple{T}} where {T<:Number, has_penalty}","page":"Public API","title":"GlobalOptimization.MBH","text":"MBH(prob::AbstractOptimizationProblem{SS}; kwargs...)\n\nConstruct the standard Monotonic Basin Hopping (MBJ) algorithm with the specified options.\n\nKeyword Arguments\n\nhopper_type::AbstractHopperType: The type of hopper to use. Default is   SingleHopper().\nhop_distribution::AbstractMBHDistribution{T}: The distribution from which hops are   drawn. Default is MBHAdaptiveDistribution{T}(100, 5).\nlocal_search::AbstractLocalSearch{T}: The local search algorithm to use. Default is   LBFGSLocalSearch{T}().\ninitial_space::Union{Nothing,ContinuousRectangularSearchSpace}: The initial search space   to use. Default is nothing.\nfunction_value_check::Bool: Whether to check the function value. Default is true.\nmax_time::Real: The maximum time to run the algorithm in seconds. Default is 60.0.\nmin_cost::Real: The minimum cost to reach. Default is -Inf.\ndisplay::Bool: Whether to display the status of the algorithm. Default is false.\ndisplay_interval::Int: The interval at which to display the status of the algorithm.   Default is 1.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Hopper-Types","page":"Public API","title":"Hopper Types","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MCH","page":"Public API","title":"GlobalOptimization.MCH","text":"MCH{EM<:AbstractFunctionEvaluationMethod} <: GlobalOptimization.AbstractHopperType\n\nEmploys the method of Multiple Communicating Hoppers (MCH) to explore the search space as described in Englander, Arnold C., \"Speeding-Up a Random Search for the Global Minimum of a Non-Convex, Non-Smooth Objective Function\" (2021). Doctoral Dissertations. 2569. https://scholars.unh.edu/dissertation/2569.\n\nThe struct fields are the same as the constructor arguments.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MCH-Union{Tuple{}, Tuple{EM}} where EM<:GlobalOptimization.AbstractFunctionEvaluationMethod","page":"Public API","title":"GlobalOptimization.MCH","text":"MCH(;\n    num_hoppers::Integer=4,\n    eval_method<:AbstractFunctionEvaluationMethod=SerialFunctionEvaluation(),\n)\n\nConstructs a new MCH object with the specified number of hoppers and evaluation method.\n\nKeyword Arguments\n\nnum_hoppers::Integer: The number of hoppers to use. Default is 4.\neval_method<:AbstractFunctionEvaluationMethod: The evaluation method to use. Default   is SerialFunctionEvaluation().\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.SingleHopper","page":"Public API","title":"GlobalOptimization.SingleHopper","text":"SingleHopper <: GlobalOptimization.AbstractHopperType\n\nA single hopper that is used to explore the search space. Note that no parallelism is employed.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#Hop-Distributions","page":"Public API","title":"Hop Distributions","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.MBHAdaptiveDistribution","page":"Public API","title":"GlobalOptimization.MBHAdaptiveDistribution","text":"MBHAdaptiveDistribution{T}\n\nAn adaptive distribution for MBH. In this implementation, each element of a hop is drawn from a univariate adaptive mixture model comprised of two Laplace distributions as defined in Englander, Arnold C., \"Speeding-Up a Random Search for the Global Minimum of a Non-Convex, Non-Smooth Objective Function\" (2021). Doctoral Dissertations. 2569. https://scholars.unh.edu/dissertation/2569.\n\nThe univariate mixture model for the i-th element of a hop has a PDF given by:\n\nf_textmixi(x b c hatlambda_i) = k_ileft(1 - b) f(xmu = 0theta = c*hatlambda_i) + b f(xmu = 0 theta = 1)right\n\nwhere mu denotes the location parameter and theta the scale parameter of a Laplace distribution (i.e., with probability density f(xmutheta)). Note that k_i denotes half of the length of the search space in the i-th dimension.\n\nThe scale parameter hatlambda_i is adaptively updated after each successful hop with a low-delay estimate given by:\n\nhatlambda_i = (1 - a) Psi_i + a hatlambda_i\n\nNote that hatlambda_i is held constant at hat0 until min_memory_update successful steps have been made. Englander (2021) proposed taking Psi_i to be the standard deviation of the i-th element of the last step_memory successful hops. Alternatively, the mean absolute deviation (MAD) around the median of the last step_memory steps can be used. Note that the MAD median is the maximum likelihood estimator for a Laplace distribution's shape parameter. In this implementation, setting use_mad = true will take Psi_i to be the MAD median, otherwise, the standard deviation is used.\n\nFields\n\nstep_memory::MBHStepMemoory{T}: The step memory for the distribution\nmin_memory_update::Int: The minimum number of steps in memory before updating the scale parameter\na: A parameter that defines the influence of a new successful step in the adaptation of   the distribution.\nb::T: The mixing parameter for the two Laplace distributions\nc::T: The scale parameter for the first Laplace distribution\nhat::Vector{T}: The estimated scale parameter of the first Laplace distribution\nhat0::T: The initial value of the scale parameter\nuse_mad::Bool: Flag to indicate if we will use the STD (proposed by Englander) or MAD median (MVE) to   update the estimated scale parameter\ndim_delta::Vector{T}: The length of the search space in each dimension\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MBHAdaptiveDistribution-Union{Tuple{Int64, Int64}, Tuple{T}} where T","page":"Public API","title":"GlobalOptimization.MBHAdaptiveDistribution","text":"MBHAdaptiveDistribution{T}(\n    memory_len::Int, min_memory_update::Int;\n    a=0.93,\n    b=0.05,\n    c=1.0,\n    hat0=1.0,\n) where T\n\nCreates a new MBHAdaptiveDistribution with the given parameters.\n\nArguments\n\nmemory_len::Int: The length of the memory for the distribution adaptation.\nmin_memory_update::Int: The minimum number of steps in memory before updating the scale parameter.\n\nKeyword Arguments\n\na: A parameter that defines the influence of a new successful step in the adaptation of   the distribution.\nb: The mixing parameter for the two Laplace distributions\nc: The scale parameter for the first Laplace distribution\nhat0: The initial value of the scale parameter\nuse_mad::Bool: Flag to indicate which metric to use for estimating the scale parameter.   If true, the MAD median is used, which is the maximum likelihood estimator for a   Laplace distribution's shape parameter. If false, the standard deviation is used   as proposed by Englander (2021).\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.MBHStaticDistribution","page":"Public API","title":"GlobalOptimization.MBHStaticDistribution","text":"MBHStaticDistribution{T}\n\nA static distribution for MBH. In this implementation, each element of a hop is drawn from a mixture model comprised of two Laplace distributions with PDF given by:\n\nf_mix(x b lambda) = k_ileft(1 - b) f(xmu = 0theta = lambda) + b f(xmu = 0 theta = 1)right\n\nwhere mu denotes the location parameter and theta the scale parameter of a Laplace distribution (i.e., with probability density f(xmutheta)). Additionally, k_i is half of the length of the search space in the i-th dimension.\n\nFields\n\nb::T: The mixing parameter for the two Laplace distributions\n::T: The scale parameter for the first Laplace distribution in the mixture model\ndim_delta::Vector{T}: The length of the search space in each dimension\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.MBHStaticDistribution-Union{Tuple{}, Tuple{T}} where T","page":"Public API","title":"GlobalOptimization.MBHStaticDistribution","text":"MBHStaticDistribution{T}(; b=0.05, =0.7) where {T}\n\nCreates a new MBHStaticDistribution with the given parameters.\n\nKeyword Arguments\n\nb: The mixing parameter for the two Laplace distributions\n: The scale parameter for the first Laplace distribution in the mixture model\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Local-Search-Methods","page":"Public API","title":"Local Search Methods","text":"","category":"section"},{"location":"lib/public/#GlobalOptimization.LBFGSLocalSearch","page":"Public API","title":"GlobalOptimization.LBFGSLocalSearch","text":"LBFGSLocalSearch{T,AT,OT,AD<:Union{ADTypes.AbstractADType, Nothing}}\n\nA local search algorithm that uses the LBFGS algorithm with box constraints to locally improve the candidate solution.\n\nNote that this method employs the LBFGS algorithm with the Fminbox wrapper from Optim.jl.\n\nFields\n\npercent_decrease_tolerance::T: The tolerance on the percent decrease of the objective   function for performing another local search. I.e., if after a local search involving   iters_per_solve iterations, the objective function value is reduced by more than   percent_decrease_tolerance percent, then another local search is performed.\nalg::AT: The LBFGS algorithm with the Fminbox wrapper.\noptions::OT: The Optim.jl options. Only used to enforce the number of iterations   performed in each local search.\nmax_solve_time::Float64: The maximum time per solve in seconds. If a solve does not   finish in this time, the solve process is terminated.\ncache::LocalSearchSolutionCache{T}: The solution cache for storing the solution from   optimization with Optim.jl.\nad::AD: The autodiff method to use. If nothing, then the default of ForwardDiff.jl is   used. Can be any of the autodiff methods from   ADTypes.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.LBFGSLocalSearch-Union{Tuple{}, Tuple{T}} where T<:AbstractFloat","page":"Public API","title":"GlobalOptimization.LBFGSLocalSearch","text":"LBFGSLocalSearch{T}(;\n    iters_per_solve::Int=5,\n    percent_decrease_tol::Number=50.0,\n    m::Int=10,\n    alphaguess=LineSearches.InitialStatic(),\n    linesearch=LineSearches.HagerZhang(),\n    manifold=Optim.Flat(),\n    max_solve_time::Float64=0.1,\n    ad=nothing,\n)\n\nCreate a new LBFGSLocalSearch object with the given parameters.\n\nKeyword Arguments\n\niters_per_solve::Int: The number of iterations to perform in each local search.\npercent_decrease_tol::Number: The tolerance on the percent decrease of the objective   function for performing another local search. I.e., if after a local search involving   iters_per_solve iterations, the objective function value is reduced by more than   percent_decrease_tol percent, then another local search is performed.\nm::Int: The number of recent steps to employ in approximating the Hessian.\nalphaguess: The initial guess for the step length. Default is   LineSearches.InitialStatic().\nlinesearch: The line search method to use. Default is LineSearches.HagerZhang().\nmanifold: The manifold to use. Default is Optim.Flat().\nmax_solve_time::Float64: The maximum time per solve in seconds. If a solve does not   finish in this time, the solve process is terminated.\nad: The autodiff method to use. If nothing, then the default of ForwardDiff.jl is   used. Can be any of the autodiff methods from   ADTypes.jl.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.LocalStochasticSearch","page":"Public API","title":"GlobalOptimization.LocalStochasticSearch","text":"LocalStochasticSearch{T}\n\nA local search algorithm that uses a stochastic approach to locally improve the candidate solution.\n\nNote that this local search algorithm is able to guarantee satisfaction of both the box constraints and the nonlinear inequality constraint (if any).\n\nFields\n\nb::T: The local step standard deviation.\niters::Int: The number of iterations to perform.\nstep::Vector{T}: The candidate step and candidate storage.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.LocalStochasticSearch-Union{Tuple{T}, Tuple{Real, Int64}} where T<:AbstractFloat","page":"Public API","title":"GlobalOptimization.LocalStochasticSearch","text":"LocalStochasticSearch{T}(b::Real, iters::Int) where {T<:AbstractFloat}\n\nCreate a new LocalStochasticSearch object with the given step size and number of iterations.\n\nArguments\n\nb::Real: The local step standard deviation.\niters::Int: The number of iterations to perform.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#GlobalOptimization.NonlinearSolveLocalSearch","page":"Public API","title":"GlobalOptimization.NonlinearSolveLocalSearch","text":"NonlinearSolveLocalSearch{T,A} <: DerivativeBasedLocalSearch{T}\n\nA local search algorithm that uses the NonlinearSolve.jl package to locally improve the candidate solution. Note that this method only works for NonlinearProblem and NonlinearLeastSquaresProblem types.\n\nAdditionally, this method is not able to guarantee satisfaction of the box constraints or the penalty nonlinear inequality constraint (if any). However, if a new solution violates either of these constraints, the new solution is discarded and the local search is terminated.\n\nFields\n\npercent_decrease_tolerance::T: The tolerance on the percent decrease of the objective   function for performing another local search. I.e., if after a local search involving   iters_per_solve iterations, the objective function value is reduced by more than   percent_decrease_tolerance percent, then another local search is performed.\nalg::A: The NonlinearSolve.jl algorithm to use.\nabs_tol::Float64: The absolute tolerance for the solver. Default is 1e-8.\nmax_solve_iters::Int: The maximum number of iterations to perform in each local search.   Default is 5.\nmax_solve_time::Float64: The maximum time per solve in seconds. If a solve does not   finish in this time, the solve process is terminated. Default is 0.1.\ncache::LocalSearchSolutionCache{T}: The solution cache for storing the solution from   solving with NonlinearSolve.jl.\n\n\n\n\n\n","category":"type"},{"location":"lib/public/#GlobalOptimization.NonlinearSolveLocalSearch-Union{Tuple{A}, Tuple{T}} where {T, A}","page":"Public API","title":"GlobalOptimization.NonlinearSolveLocalSearch","text":"NonlinearSolveLocalSearch{T,A}(\n    alg::A;\n    iters_per_solve::Int=5,\n    time_per_solve::Float64=0.1,\n    percent_decrease_tol::Number=50.0,\n    abs_tol::Float64=1e-8,\n)\n\nCreate a new NonlinearSolveLocalSearch object with the given parameters.\n\nArguments\n\nalg::A: The NonlinearSolve.jl algorithm to use. For example,   NonlinearSolve.NewtonRaphson() of NonlinearSolve.TrustRegion().\niters_per_solve::Int: The number of iterations to perform in each local search.\ntime_per_solve::Float64: The maximum time per solve in seconds. If a solve does not   finish in this time, the solve process is terminated.\npercent_decrease_tol::Number: The tolerance on the percent decrease of the objective   function for performing another local search. I.e., if after a local search involving   iters_per_solve iterations, the objective function value is reduced by more than   percent_decrease_tol percent, then another local search is performed.\nabs_tol::Float64: The absolute tolerance for the solver. Default is 1e-8.\n\n\n\n\n\n","category":"method"},{"location":"lib/public/#Index","page":"Public API","title":"Index","text":"","category":"section"},{"location":"lib/public/","page":"Public API","title":"Public API","text":"Pages = [\"public.md\"]","category":"page"},{"location":"#GlobalOptimization","page":"Home","title":"GlobalOptimization","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Currently, GlobalOptimization provides Particle Swarm Optimization (PSO) and several variants of Differential Evolution (DE) as the only global optimization algorithms supported. Monotonic Basin Hopping (MBH) is in the works.","category":"page"},{"location":"#Simple-PSO-Example","page":"Home","title":"Simple PSO Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Let's use PSO to find the minimum to the non-convex Ackley function given by","category":"page"},{"location":"","page":"Home","title":"Home","text":"J(mathbfx) = -a expleft(-bsqrtfrac1dsum_i=1^d x_i^2right) - expleft(frac1dsum_i=1^d cos (cx_i)right) + a + exp(1)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where a = 20, b = 02, c = 2pi, and d is the length of the decision vector mathbfx, subject to the constraint that -32768 leq x_i leq 32768 hspace1mm forall hspace1mm x_i hspace1mm in hspace1mm mathbfx.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To begin, we'll first define an Ackley function in Julia as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function ackley(x)\n    a = 20\n    b = 0.2\n    c = 2*\n    d = length(x)\n\n    sum1 = 0.0\n    sum2 = 0.0\n    for val in x\n        sum1 += val^2\n        sum2 += cos(c*val)\n    end\n    return -a*exp(-b*sqrt(sum1/d)) - exp(sum2/d) + a + exp(1)\nend\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Next, we'll define the OptimizationProblem by providing its constructor our new ackley function and bounds that define the search space. Then, we'll instantiate a StaticPSO (an implementation of the PSO algorithm that does not use parallel computing to evaluate the cost function) to perform the optimization!","category":"page"},{"location":"","page":"Home","title":"Home","text":"using GlobalOptimization\n\nN   = 10 # The number of decision variables\nLB  = [-32.768 for _ in 1:10] # The lower bounds\nUB  = [ 32.768 for _ in 1:10] # The upper bounds\n\n# Construct the optimization problem\nop  = OptimizationProblem(ackley, LB, UB)\n\n# Instantiate PSO instance\npso = PSO(op)\n\n# Perform optimization with pso\nres = optimize!(pso)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, we can get the final optimal decision vector with","category":"page"},{"location":"","page":"Home","title":"Home","text":"best_candidate = res.xbest","category":"page"},{"location":"","page":"Home","title":"Home","text":"and the fitness of the final optimal decision vector with","category":"page"},{"location":"","page":"Home","title":"Home","text":"best_candidate_fitness = res.fbest","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
